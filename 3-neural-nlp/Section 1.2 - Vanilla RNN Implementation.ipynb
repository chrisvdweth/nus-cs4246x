{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6eb3c13-6af2-4394-a7d5-33740cfb70b4",
   "metadata": {},
   "source": [
    "<img src='data/images/section-notebook-header.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f19ac5-0883-4848-ac6c-3313d1b2c528",
   "metadata": {},
   "source": [
    "# Vanilla RNN Implementation\n",
    "\n",
    "The Recurrent Neural Network (RNN) architecture is a type of neural network specifically designed to process sequential data. Unlike feedforward neural networks, which process input data independently, RNNs have connections that allow information to be passed from previous steps to the current step, enabling them to capture temporal dependencies and patterns in sequences. Recall the slide from the lecture:\n",
    "\n",
    "<img src='data/images/lecture-slide-01.png' width=\"90%\" />\n",
    "\n",
    "The key feature of the RNN architecture is the presence of recurrent connections that create loops in the network, allowing information to persist and flow through time. This looping mechanism allows RNNs to maintain an internal state or memory, which can capture context and information from previous steps in the sequence. This makes RNNs particularly useful for Natural Language Processing (NLP) tasks for several reasons:\n",
    "\n",
    "* **Sequence Modeling:** RNNs excel at modeling sequential data, making them well-suited for tasks that involve understanding and generating sequences of text. They can effectively process and capture dependencies in sequences, such as word order, sentence structure, and context, which are crucial in NLP tasks like language modeling, machine translation, and sentiment analysis.\n",
    "\n",
    "* **Variable-Length Input:** NLP often deals with variable-length input, such as sentences or documents of different lengths. RNNs can handle variable-length sequences by iteratively processing each element in the sequence, regardless of its length. This flexibility makes RNNs highly adaptable to NLP tasks where input length varies, allowing them to process text at the word, character, or sentence level.\n",
    "\n",
    "* **Contextual Information:** RNNs have the ability to maintain a hidden state that captures context and information from previous steps. This contextual information is valuable in NLP tasks that rely on understanding and generating language, as it allows the model to consider the entire sequence and make informed predictions. For example, in machine translation, the hidden state can encode the context of the source sentence, helping the model generate accurate translations.\n",
    "\n",
    "* **Long-Term Dependencies:** RNNs can theoretically capture long-term dependencies in sequences by propagating information through the recurrent connections. This capability is important for tasks that involve understanding relationships between distant words or phrases. However, traditional RNNs can struggle with capturing long-term dependencies due to the vanishing or exploding gradient problem. To address this, variants such as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) were introduced, which have improved memory and gating mechanisms to alleviate these issues.\n",
    "\n",
    "Overall, the recurrent neural network architecture, including its variants like LSTM and GRU, has proven to be highly effective in NLP tasks. Their ability to model sequential data, handle variable-length input, capture context, and capture long-term dependencies makes them a powerful choice for tasks involving language understanding, generation, and sequence-to-sequence mapping.\n",
    "\n",
    "In this notebook, we built a simple RNN-based classifier. The input of the model is a last name and the prediction is the nationality of the person of that name. What makes this model so easy is that we do not consider sequences for words but sequences of characters -- and compared to words, there's only a small limited number of characters. And most conveniently, there's no need for word embeddings; we represent each character as a one-hot vector. This model direct adopts a [PyTorch tutorial](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html); while the dataset is part of the notebook, it is also available [here](https://download.pytorch.org/tutorial/data.zip).\n",
    "\n",
    "Compared to the PyTorch tutorial, we do not use an existing RNN layer provided by Pytorch such as [`nn.RNN`](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html), [`nn.GRU`](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html), or [`LSTM`](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html). Instead, we implement our own RNN network using the most basic architecture like we saw in the lecture; see below:\n",
    "\n",
    "<img src='data/images/lecture-slide-02.png' width=\"50%\" />\n",
    "\n",
    "Of course, we still use components such as linear layers and activation functions provided by PyTorch. The goal is to implement the recurrent nature *\"by hand\"* to get a better understanding about the intuition behind RNNs. Hence the name Vanilla RNN: the most basic recurrent architecture, including a very simple dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237f9b5a",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949044d8-6ba7-4134-825a-bc17655986a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64822fed-25e8-49cb-a176-b4e4ffee0b82",
   "metadata": {},
   "source": [
    "We utilize PyTorch as our deep learning framework of choice by importing the `torch` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff7844b-913d-4e28-ba3e-8280fd38aff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda98ef8-1828-481b-855a-3d6a2eb23e6a",
   "metadata": {},
   "source": [
    "The implementation of our model can be found in the file `src/rnn.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac81f5f-0cca-49d5-8f09-e96d3535e4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rnn import VanillaRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3d60c1",
   "metadata": {},
   "source": [
    "### Checking/Setting the Computation Device\n",
    "\n",
    "PyTorch allows to train neural networks on supported GPUs to significantly speed up the training process. If you have a support GPU, feel free to utilize it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f6b72b-4175-488e-ad4a-9febd5335d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Use this line below to enforce the use of the CPU (in case you don't have a supported GPU)\n",
    "# With this small dataset and simple model you won't see a difference anyway\n",
    "#use_cuda = False\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "print(\"Available device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147753ef-0c7a-42c4-9153-558587989c45",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d5aa78-e406-4d4a-bd6b-9bccd85574de",
   "metadata": {},
   "source": [
    "## Load & Prepare Data\n",
    "\n",
    "The corpus of last names can be found in `data/names` or can be downloaded [here](https://download.pytorch.org/tutorial/data.zip). This folder contains a set of files where the file name indicates the nationality of the list of names contained in the respective files. For example, the file `German.txt` contains all the names labeled as German.\n",
    "\n",
    "### Read Datafiles\n",
    "\n",
    "First, let's read all the files, extract the names and store them in a list `names`. The list `nationalities` keeps track of the nationality of that name derived from the file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206835de-d20e-423a-a8ac-702772e26cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = set()\n",
    "        \n",
    "names, nationalities = [], []\n",
    "    \n",
    "with os.scandir('data/corpora/names/') as it:\n",
    "    for file_name in it:\n",
    "        nationality = file_name.name.split('.')[0].lower()\n",
    "        count = 0\n",
    "        with open(file_name) as file:\n",
    "            for line in file:\n",
    "                names.append(line.strip().lower())\n",
    "                nationalities.append(nationality)\n",
    "                \n",
    "print(\"Number of data samples: {}\".format(len(names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d390d992",
   "metadata": {},
   "source": [
    "### Generate Pandas Dataframe\n",
    "\n",
    "Based on these two lists, we create a Pandas Dataframe which makes its further use as a training dataset for our classifier a bit easier.\n",
    "\n",
    "**Side note:** Compared to the notebooks about the MLP architecture, we do not use an utility classes such as [`Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) and [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader). In this notebook, we do not train the model using mini batches but use one data sample (i.e., a name + label) at a time. The reason is that all the sequences in a batch need to have the same length. Of course, names -- like text text data in general -- can be of different length. This is a general problem and there are various approaches to mitigate them. After all, we want to use mini batches of larger sizes for performance reasons. However, the dataset here is small so performance is not critical. In later notebooks, we see and discuss how to handle sequences of different lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937ab5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with two columns\n",
    "df = pd.DataFrame(columns=['name', 'nationality'])\n",
    "df['name'] = names\n",
    "df['nationality'] = nationalities\n",
    "\n",
    "# Convert string value of nationality to a integer value of range 0..(#nationalities-1)\n",
    "df.nationality, nationalities_mapping = df.nationality.factorize()\n",
    "\n",
    "print('Number of different nationalities/labels: {}'.format(len(nationalities_mapping)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf939ec1-f552-47a7-a56d-251d8451deab",
   "metadata": {},
   "source": [
    "To see if the whole notebook is working just fine, you can first consider only a random sample of the overall dataset. If everything is working as expected, feel free to come back to this cell and edit it to use the full dataset. It is not important here, since the focus is not on building a state-of-the art model but to better understand the RNN architecture.\n",
    "\n",
    "As usual, we also split our dataset into a training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b3a06e-aa86-4b61-9c74-421113a3eca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's generally always good to shuffle the dataset\n",
    "# For testing, you can also first use smaller samples\n",
    "#df = df.sample(frac=1.0).reset_index(drop=True)\n",
    "df = df.sample(frac=0.1).reset_index(drop=True)\n",
    "\n",
    "# Split the dataset into training and test data\n",
    "df_train, df_test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Let's see how the training data looks like.\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41619201-cfb5-4810-a1da-5617b57a2f19",
   "metadata": {},
   "source": [
    "We can also quickly check the sizes of the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cf8498-3b1b-42f6-a655-4fd31e563207",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of training data: {}'.format(df_train.shape))\n",
    "print('Shape of test data: {}'.format(df_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01f5814-5690-4256-8eaf-fcff280b8199",
   "metadata": {},
   "source": [
    "Lastly, from an evaluation perspective, it is also useful to know if our dataset is reasonably balanced not. For this we can use the information we have to create a bar plot that shows the number of names for each nationality. The code cell below accomplishes this. Without going into the details, Pandas makes this calculation of the numbers per nationality very easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84316881-76f6-441f-9b33-520513dde3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.groupby('nationality')['name'].count().tolist()\n",
    "x = list(range(len(y)))\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(x, y, width=0.9)\n",
    "plt.xticks(x, nationalities_mapping, rotation=45)\n",
    "plt.ylabel(\"number of names\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cd9ad9-f27c-4536-b9e7-21b23639bc2f",
   "metadata": {},
   "source": [
    "We can see that our dataset is highly unbalanced with some nationalities over-represented, particularly Russian and English names. At the very least, this tells us that accuracy would not be a suitable metric to use in our evaluation. We therefore use the f1 score; see below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd128d0",
   "metadata": {},
   "source": [
    "### Auxiliary Preparation Steps\n",
    "\n",
    "We first need to identify the vocabulary including its size. Here, the vocabulary is simply the number of unique characters across all names. The size of the vocabulary, of course, determines the size of the input of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2058ec-849e-473d-9202-f3dc4f5a69f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(''.join(names)))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print('Vocabulary:')\n",
    "print(vocab)\n",
    "print()\n",
    "print('Size of vocabulary: {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5182d770",
   "metadata": {},
   "source": [
    "From the output above you can see that we have some odd characters in our vocabulary, e.g., `/` or other special symbols. In practice, we would need to check if indeed all characters are valid and potentially conduct data cleaning steps to fix that. However, here we simply assume that all characters are indeed valid and part of our dataset -- which might indeed be true!\n",
    "\n",
    "We already know that Neural Networks don't work on characters or strings. We therefore need to vectorize them, i.e, to convert each name into a 2d tensor (or matrix). Each name is a sequence of characters, and we represent each character as a one-hot encoded vector. Again, this is perfectly valid approach since\n",
    "\n",
    "* The vocabulary is small, so the one-hot vectors are also small\n",
    "\n",
    "* It is reasonable to assume to treat characters as nominal data without any notion of similarity between them (in contrast to words, where we would like to represent similar words using similar vectors.)\n",
    "\n",
    "The code cell below defines the method `name_to_tensor()` to convert a names in to the corresponding 2d tensor where the first dimensions reflects the length of the name in terms of the number of characters, and the second dimensions reflects the size of the one-hot vector determined by the size of the vocabulary (cf., line `tensor = torch.zeros(len(name), vocab_size)`). By iterating over each character in the given name, we set the corresponding position in the one-hot vector to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d8c365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a line into a tensor of shape (sequence_lenght, vocab_size)\n",
    "def name_to_tensor(name):\n",
    "    # Create tensor of the right shape with all elements being 0\n",
    "    tensor = torch.zeros(len(name), vocab_size)\n",
    "    # Create one-hot vector for each character by setting the corresponding element to 1\n",
    "    for li, letter in enumerate(name):\n",
    "        tensor[li][vocab.index(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "print(name_to_tensor('jones').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174a016c",
   "metadata": {},
   "source": [
    "As you can see, the tensor for \"jones\" has a shape of `(5, 58)` as \"jones\" has 5 letters where each letter is represented by a one-hot encoded vector of the size of the vocabulary (here: 58)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ae0b70-6a10-400b-b3ae-09882973d104",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48edebed",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "\n",
    "Now that the data is ready, we can build and train our model. For our classifier, we use the most Vanilla RNN as covered in the lecture. You can check the file `src/rnn.py` for the implementation of class `VanillaRNN`. We have only 1 additional parameter to specify, which is the dimension of the hidden state, i.e., `hidden_size`. Since this is a rather simple task, we would probably get away with a smaller hidden state. Feel free to lower `hidden_size` to see when it affects the quality of the resulting classifier.\n",
    "\n",
    "The output size is the number of different nationalities in the dataset, which we can directly infer from the size of the dictionary `nationalities_mapping` (which is 18 here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a5c5e8-501c-4889-aee2-3cc254c3ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "output_size = len(nationalities_mapping)\n",
    "\n",
    "model = VanillaRNN(vocab_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# We can also print the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec01a7d3",
   "metadata": {},
   "source": [
    "The following 2 methods predict the nationality for an individual name (method `predict`) and evaluate the model over the test data (method `evaluate`) by calculating its accuracy -- f1 score might be more suitably here, but the dataset is fairly balanced and this is just a toy example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9902fce1-0566-44a0-9904-9108815da411",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8060c7-2774-4fa5-9344-489831092cff",
   "metadata": {},
   "source": [
    "## Training & Evaluating the Model\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "Similar to the MLP notebook, we first implement a series of auxiliary methods. The first methods handle the evaluation of the model. The code cell below implements the method `evaluate()` to, well, evaluate our model. Apart from the model itself, the method also receives the pandas `DataFrame` as input parameter. This allows us later to use both `dfr_train` and `df_test` to evaluate the training and test loss using the same method.\n",
    "\n",
    "Note that we use a separate method `predict()` to actually input the vectorized name into the model and get the prediction. This makes the code a bit cleaner since -- compared to a simple MLP -- \"running\" the model actually requires looping over each character (more specifically, its one-hot vector). Of course, we could have moved the code implemented by the method `predict()` also directly in the `forward()` method of class `VanillaRNN`. But actually seeing here the loop that reflects the recurrent nature of the RNN is more instructive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225f7f70-5545-4636-bb11-017e9b43f9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, name_tensor):    \n",
    "    with torch.no_grad():\n",
    "        # Initialize the first hidden state h0\n",
    "        hidden = model.init_hidden(1).to(device)\n",
    "        # Iterate over all characters in the name and given the current character and the last hidden state to the model\n",
    "        for i in range(name_tensor.size()[0]):\n",
    "            output, hidden = model(name_tensor[i], hidden)\n",
    "        # The predicted class is the index in the output vector with the largest value (log-prob)\n",
    "        _, prediction = torch.max(output, dim=1)\n",
    "    # Return the class as a simple integer value\n",
    "    return prediction.cpu().detach().numpy()[0]\n",
    "\n",
    "\n",
    "def evaluate(model, df):\n",
    "    # Set model to \"eval\" mode (not needed here, but a good practice)\n",
    "    model.eval()\n",
    "    \n",
    "    y_pred = []\n",
    "    y_true = df['nationality'].tolist()\n",
    "    num_samples = df.shape[0]\n",
    "    \n",
    "    # Iterate over all samples in the test data\n",
    "    with tqdm(total=num_samples) as progress_bar:\n",
    "        for _, row in df.iterrows():\n",
    "            # Prepare the data sample\n",
    "            name, nationality = row['name'], row['nationality']\n",
    "            # Vectorize name and move tensor to device\n",
    "            name_tensor = name_to_tensor(name).to(device)\n",
    "            # Use model to prediction class\n",
    "            prediction = predict(model, name_tensor)\n",
    "            # Add prediction to list\n",
    "            y_pred.append(prediction)\n",
    "            # Update progress bar\n",
    "            progress_bar.update(1)\n",
    "            \n",
    "    # Set model to \"train\" mode (not needed here, but a good practice)\n",
    "    model.train()              \n",
    "            \n",
    "    # Return f1 score (here: micro)\n",
    "    return metrics.f1_score(y_true, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46ddb2d",
   "metadata": {},
   "source": [
    "Let's evaluate the model without any training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f30abf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e9b635",
   "metadata": {},
   "source": [
    "As expected, the accuracy is very low. As a sanity check, recall that we have 18 different nationalities (i.e., class labels). This means that a model that's just randomly guessing -- and assuming the class labels are well balanced -- would be correct with a probability $1/18 = 0.0556$. Of course, we already know that our dataset is highly imbalanced, so the f1 score for the untrained model can deviate very much from $1/18$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7560e727",
   "metadata": {},
   "source": [
    "### Training (and evaluation after each epoch)\n",
    "\n",
    "The code cell below implements the method to train our model using the basic loop structure we have already seen in the MLP notebook, and which represents the most basic structure doing the following steps for each batch. Note that our batch size is 1, i.e., we use each individual sample (i.e., name + label) to update the trainable parameters. The main reason is that our inputs (i.e., the names) are generally of different length, and all samples within a batch need to be the same length. There are 2 ways around this:\n",
    "\n",
    "* Pad all sequences in a batch to the length of the longest sequence in the batch\n",
    "\n",
    "* Generate batches in such a way that you insert only sequences of equal length\n",
    "\n",
    "Both are standard techniques but just out of the scope of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4e2981-cd36-4316-bc50-b98c9b7208f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, df_train, df_test, optimizer, criterion, num_epochs):\n",
    "\n",
    "    losses, f1_train, f1_test = [], [], []    \n",
    "    \n",
    "    # Run all epochs\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        # Shuffle training data to yield new batches (good practice)\n",
    "        df = df_train.sample(frac=1).reset_index(drop=True)\n",
    "        # Initilize loss for whole epoch\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        with tqdm(total=len(df_train)) as progress_bar:\n",
    "            for _, row in df_train.iterrows():\n",
    "                # Prepare the data sample\n",
    "                name, nationality = row['name'], row['nationality']        \n",
    "\n",
    "                # Convert name and nationality to a tensor and move it the the GPU (if available)\n",
    "                name = name_to_tensor(name).to(device)\n",
    "                nationality = torch.Tensor([nationality]).long().to(device)\n",
    "\n",
    "                # Initialize the first hidden state h0\n",
    "                hidden = model.init_hidden(1).to(device)\n",
    "\n",
    "                # Iterate over all characters in the name and given the current character and the last hidden state to the model\n",
    "                for i in range(name.size()[0]):\n",
    "                    output, hidden = model(name[i], hidden)\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = criterion(output, nationality)\n",
    "\n",
    "                # Let PyTorch do its magic to calculate the gradients and update all trainable parameters\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Keep track of overall epoch loss\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                # Update progress bar\n",
    "                progress_bar.update(1)\n",
    "\n",
    "        # Keep track of all epoch losses\n",
    "        losses.append(epoch_loss)\n",
    "\n",
    "        # Compute f1 score for both TRAINING and TEST data\n",
    "        f1_tr = evaluate(model, df_train)\n",
    "        f1_te = evaluate(model, df_test)\n",
    "        f1_train.append(f1_tr)\n",
    "        f1_test.append(f1_te)\n",
    "        \n",
    "        print(\"Loss:\\t{:.3f}, f1 train: {:.3f}, f1 test: {:.3f} (epoch {})\".format(epoch_loss, f1_tr, f1_te, epoch))\n",
    "        \n",
    "    # Return all losses and f1 scores (all = for each epoch)\n",
    "    return losses, f1_train, f1_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3403e62d-698f-42cb-be9e-fec6a10d80bd",
   "metadata": {},
   "source": [
    "The last steps before training our model are as usual to first define the loss function (i.e. criterion) as well as the optimizer. Since our model returns log probabilities, we have to use the Negative Log Likelihood Loss ([`nn.NLLLoss()`](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html)). We also use the [`Adam`](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6bda3b-f254-4608-83bc-63ebf5bfc658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model and movie to device\n",
    "model = VanillaRNN(vocab_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Define optimizer (you can try, but the basic (Stochastic) Gradient Descent is actually not great)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca32b3f4",
   "metadata": {},
   "source": [
    "Now we have all the parameters to call the `train()` in the code cell below. Note that you can run the code cell below multiple times to continue the training for further 10 epochs. Each epoch will print 3 progress bars:\n",
    "\n",
    "* training over training set\n",
    "\n",
    "* evaluating over training set\n",
    "\n",
    "* evaluating over test set\n",
    "\n",
    "After each epoch, a print statement will show the current loss as well as the latest f1 scores for the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afab6f62-3414-4924-bdf9-ce9ce95a5785",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "losses, f1_train, f1_test = train(model, df_train, df_test, optimizer, criterion, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807db09f-d7ae-42d0-98a0-fb328fe19c8c",
   "metadata": {},
   "source": [
    "### Plotting the Results\n",
    "\n",
    "Since the method `train()` returns the losses and f1 scores for each epoch, we can use this data to visualize how the loss and the f1 scores change over time, i.e., after each epoch. The code cell below creates the corresponding plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b272b1-4d71-48c5-936a-ecbbf673a32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(1, len(losses)+1))\n",
    "\n",
    "# Convert losses to numpy array\n",
    "losses = np.asarray(losses)\n",
    "# Normalize losses so they match the scale in the plot (we are only interested in the trend of the losses!)\n",
    "losses = losses/np.max(losses)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(x, losses, lw=3)\n",
    "plt.plot(x, f1_train, lw=3)\n",
    "plt.plot(x, f1_test, lw=3)\n",
    "\n",
    "font_axes = {'family':'serif','color':'black','size':16}\n",
    "\n",
    "plt.gca().set_xticks(x)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.xlabel(\"Epoch\", fontdict=font_axes)\n",
    "plt.ylabel(\"F1 Score\", fontdict=font_axes)\n",
    "plt.legend(['Loss', 'F1 (train)', 'F1 (test)'], loc='lower left', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f795f04",
   "metadata": {},
   "source": [
    "### Making Predictions\n",
    "\n",
    "In practice, we also want to use our trained model to predict the nationality for a given name. In the code cell below, pick your name of choice and let the model predict the nationality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d59a1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'chris'\n",
    "\n",
    "# Convert name to a tensor and move it to the GPU (if available)\n",
    "name_tensor = name_to_tensor(name).to(device)\n",
    "        \n",
    "# Use model to prediction class\n",
    "prediction = predict(model, name_tensor)\n",
    "\n",
    "# Convert class label to nationality\n",
    "print(nationalities_mapping[prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0c584d-fe53-4831-9415-9acd5992b7d7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf998121-1ce0-439c-bf4c-06a98baaee76",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Basic recurrent neural networks (RNNs) are a type of neural network architecture designed to process sequential data by capturing dependencies and patterns over time. They have a looping mechanism that allows information to persist and flow through the network, making them well-suited for tasks involving sequences, such as Natural Language Processing (NLP) and time series analysis.\n",
    "\n",
    "In a basic RNN, each step of the sequence is processed one at a time. At each step, the RNN takes an input, which could be a word, a character, or a feature vector, and combines it with the hidden state from the previous step. This combination is passed through an activation function, typically a hyperbolic tangent or sigmoid function, to produce an output and update the hidden state. The updated hidden state is then used in the next step, allowing the network to capture information and dependencies from earlier steps.\n",
    "\n",
    "RNNs have been successfully applied to various NLP tasks. They can perform tasks such as language modeling, where the network learns to predict the next word in a sequence given the previous words. RNNs are also used in machine translation, sentiment analysis, named entity recognition, and many other NLP applications. By leveraging their sequential processing capabilities and capturing context and dependencies over time, basic RNNs and their variants have become essential tools in NLP, enabling more sophisticated language understanding, generation, and sequence-to-sequence mapping.\n",
    "\n",
    "The topic of RNNs is of course much wider than covered in this notebook. For example, one limitation of basic RNNs is the vanishing or exploding gradient problem. During backpropagation, the gradients can either become too small, causing the network to have difficulty learning long-term dependencies, or become too large, leading to instability during training. To address this, more advanced variants like Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) were introduced. These variants incorporate gating mechanisms to control the flow of information and better capture long-term dependencies, making them widely used in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d121c7-6c70-494a-b18c-1a2e82e78d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs5246",
   "language": "python",
   "name": "cs5246"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
