{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83524cb6-a19c-4a76-beb4-15efc4cfd45b",
   "metadata": {},
   "source": [
    "<img src='data/images/section-notebook-header.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39787fe-d965-49e4-b966-ad04df8dea08",
   "metadata": {},
   "source": [
    "# Word2Vec: CBOW\n",
    "\n",
    "CBOW (Continuous Bag-of-Words) is a model for training word embeddings in the Word2Vec framework. CBOW aims to predict a target word based on its context words, making it a \"bag-of-words\" approach. In CBOW, the model architecture consists of a hidden layer that represents the word embeddings and an output layer that predicts the target word. The input to the model is a set of context words, and the output is the target word. The image below is taken from the lecture slides showing the basic setup and intuition behind CBOW.\n",
    "\n",
    "<img src='data/images/lecture-slide-08.png' width='80%' />\n",
    "\n",
    "This example assumes a window size of 2. This means that the context to consider for a given center word are the 2 words before and the 2 words after the center words. In the image above, these 4 context words are *\"watching\"*, *\"funny\"*, *\"on\"*, and *\"netflix\"* -- keep in mind that the order does not matter. On the right, the image shows some example words, with the color indicating which word is intuitively the most likely center word (green = high probability; red = low probability). Of course, the actual most likely word will depend on the training data; here it is only about the basic intuition behind CBOW.\n",
    "\n",
    "In this notebook, we will train a CBOW model from scratch. Since we already prepared the data in the accompanying notebook, there's actually not much more to do. We implement and train this model using PyTorch. The model should train with or without a GPU, although having a GPU significantly speeds up the process. However, here we don't care too much about accuracy but the basic idea behind CBOW."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce9c36a",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0145b53-9325-40e2-be8b-ee1db8fd1b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64262e92-19de-4142-a957-0e6b85681f5c",
   "metadata": {},
   "source": [
    "We utilize some utility methods from PyTorch, so we need to import the `torch` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc5d3b6-7fed-46c0-a883-b136afab5c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1439c977-c4b8-42e2-8c7b-16feeea95058",
   "metadata": {},
   "source": [
    "We also need the PyTorch implementation of the CBOW model. While the code is very short, having the implementation in separate files makes it easier to re-use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd12009-5282-459c-88d1-0d5dc62b768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.word2vec import CBOW\n",
    "from src.utils import tsne_plot_similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4fdc6d",
   "metadata": {},
   "source": [
    "### Checking/Setting the Computation Device\n",
    "\n",
    "PyTorch allows to train neural networks on supported GPUs to significantly speed up the training process. If you have a support GPU, feel free to utilize it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a00219",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Use this line below to enforce the use of the CPU \n",
    "#use_cuda = False\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "print(\"Available device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d2e277-6f7c-411d-968f-922859fb6678",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75810c25-f8dc-451b-9774-42409c8b02f1",
   "metadata": {},
   "source": [
    "## Load all the Data\n",
    "\n",
    "### Load Vocabulary\n",
    "\n",
    "In the Data Preprocessing notebook we created the vocabulary to map from words to their indices, and vice versa. We naturally need this vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2cf803-36b8-4ec1-bf57-56a41282deac",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = torch.load('data/corpora/imdb-reviews/vectorized-word2vec/imdb-word2vec-20000.vocab')\n",
    "\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "print('Size of vocabulary:\\t{}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b12871",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "\n",
    "Of course, we need the training data. Recall, that each data sample is an array of word indices, not the words themselves. Depending on your size $m$ for the context (cf. Data Preprocessing notebook), a data sample contains $(2m + 1)$ indices, where the first $2m$ indices represent the context words and the last index represents the center word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c0bd4b-19d5-4f2e-8b5d-5256ca3dd412",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('data/corpora/imdb-reviews/vectorized-word2vec/imdb-dataset-cbow.npy')\n",
    "\n",
    "num_samples, num_indices = data.shape\n",
    "\n",
    "print('Number of samples: {}'.format(num_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b72b08b",
   "metadata": {},
   "source": [
    "### Split Dataset into Inputs & Targets\n",
    "\n",
    "The input features `X` are the contexts (i.e., the first $2m$ entries), and the targets are the last entry in each data sample array. We also directly convert the Numpy arrays into PyTorch tensors to serve as input for the CBOW model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ff097d-7872-462d-b86d-6228af3d5856",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.Tensor(data[:,0:-1]).long()\n",
    "y = torch.Tensor(data[:,-1]).long()\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9082382e",
   "metadata": {},
   "source": [
    "### Create `Dataset` and `DataLoader`\n",
    "\n",
    "PyTorch comes with different `Dataset` classes and a `DataLoader` class that make working with batches of different sizes very easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b50ca3-f367-47a9-9239-815493234e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f4d6af",
   "metadata": {},
   "source": [
    "## Create and Train CBOW Model\n",
    "\n",
    "### Create Model\n",
    "\n",
    "CBOW belongs to the family of shallow neural network models and is used to represent words as dense vectors in a continuous vector space. CBOW aims to predict a target word based on the context words surrounding it. During training, CBOW constructs a hidden layer that represents the aggregated information from the context words. This hidden layer acts as a continuous vector representation of the context. The figure below is taken from the lecture slides to visualize the basic shallow architecture of CBOW by means of an example input and output\n",
    "\n",
    "<img src='data/images/lecture-slide-09.png' width='80%' />\n",
    "\n",
    "The word embeddings are learned by updating the weights of the neural network using backpropagation and gradient descent. The code for the CBOW model can be found in `src/cbow.py`. Have a look how simple the model looks. It directly implements the model visualized in the image above -- with some very minor tweaks to improve the training. As size for the word embeddings we go with 300 by default -- feel free to change it -- as it is the common embedding size of pretrained Word2Vec models you can download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f2e88b-012a-45eb-8e3f-03c31313f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 300\n",
    "\n",
    "# Create model\n",
    "model = CBOW(vocab_size, embed_dim)\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Move th model to GPU, if available (by default it \"stays\" on the CPU)\n",
    "model.to(device)\n",
    "# Print model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30809fc7",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "\n",
    "The code cell below shows the most basic structure for training a model. The outer loop determines how many epochs we want to train. An epoch describes the processing of all data samples in the dataset. For each epoch, we then loop over the dataset in the form of batches (this is where the `DataLoader` comes so handy). Instead of (Mini-Batch) Gradient Descent, we use the more sophisticated `Adam` optimizer, but feel free to change it to Gradient Descent which PyTorch also provides. The loss function -- often called \"criterion\" in Pytorch lingo -- is the Cross-Entropy Loss. Note that the model has **no Softmax layer**, as this is handled by the `CrossEntropyLoss` class of Pytorch. There is nothing special about it, but it does save 1 or 2 lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ec8639-f368-4187-b0b2-d515faba3c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for idx, (contexts, y) in enumerate(tqdm(dataloader)):\n",
    "        # Move current batch to GPU, if available\n",
    "        contexts, y = contexts.to(device), y.to(device)\n",
    "        \n",
    "        # Calculate output of the model\n",
    "        logits = model(contexts)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(logits, y)\n",
    "        \n",
    "        # Reset the gradients from previous iteration\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Calculate new Gradients using backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update all trainable parameters (i.e., the theta values of the model)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Keep track of the overall loss of the epoch\n",
    "        epoch_loss += loss.item()\n",
    "            \n",
    "    print('[Epoch {}] Loss: {}'.format((epoch+1), epoch_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158b69dd",
   "metadata": {},
   "source": [
    "### Save/Load Model\n",
    "\n",
    "As retraining the model all the time can be tedious, we can save and load our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9012808-20f3-465d-a6e3-43a86d5d5b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 'save'\n",
    "#action = 'load'\n",
    "#action = 'none'\n",
    "\n",
    "if action == 'save':\n",
    "    torch.save(model.state_dict(), 'data/models/word2vec/model-cbow.pt')\n",
    "elif action == 'load':\n",
    "    model = CBOW(vocab_size, embed_dim)\n",
    "    model.to(device)\n",
    "    model.load_state_dict(torch.load('data/models/word2vec/model-cbow.pt'))\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd249764-d391-4388-81c7-1c5f6939fa02",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b9aaf6-46e3-4991-9015-ce942bd4b5a6",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "The following code is purely to visualize the results. Of course, depending on how much of the training data you used and how long you have trained your model, the resulting plots might differ greatly from the ones in the lecture slides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7de136",
   "metadata": {},
   "source": [
    "### Auxiliary Method\n",
    "\n",
    "The method `get_most_similar()` below returns for a given word the k-most similar words w.r.t. the word embeddings. Note that we only use matrix `U` for the word embeddings, and completely ignore matrix `V`, just to keep it simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c57481-85d2-4b35-8c05-1246bb4e227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar(word, k=5):\n",
    "    # Get the index for the input word\n",
    "    idx = vocabulary.lookup_indices([word])[0]\n",
    "    # Get the word vector of the input word\n",
    "    reference = model.U.weight[idx]\n",
    "    # Calculate all pairwise similarites between the input word vector and all other word vectors\n",
    "    dist = F.cosine_similarity(model.U.weight, reference)\n",
    "    # Sort the distances and return the top-k word vectors that are most similar to the input word vector\n",
    "    # Note that the top-k contains the input word vector itself, which is fine here\n",
    "    index_sorted = torch.argsort(dist, descending=True)\n",
    "    indices = index_sorted[:k]\n",
    "    # Convert the top-k nearest word vectors into their corresponding words\n",
    "    return [ vocabulary.lookup_token(n.item()) for n in indices ]    \n",
    "    \n",
    "#Example\n",
    "get_most_similar('music')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d332b85d",
   "metadata": {},
   "source": [
    "### Visualization of Results\n",
    "\n",
    "We start by creating a list of seed words. For each seed word, we will get the top-k nearest words and later show them together into a 2d plot (see below). Feel free to change the list of seed words. Just note that each seed word and its resulting cluster will be assigned its unique color. So the more seed words you use, the less distinctive will be some of the colors in the final plot. You might also want to ensure that the seed words themselves are not semantically very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647170e5-8877-4e45-a356-88a28c267904",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_words = ['movie', 'actor', 'scene', 'music', 'dvd', 'story', 'horror', 'funny', 'laugh', 'love', 'director']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3485ac43",
   "metadata": {},
   "source": [
    "#### Create Word Embedding Clusters\n",
    "\n",
    "Here, a cluster is simply the seed word and all its top-k nearest words. This helps us later to plot each cluster in a different color later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d72ece-7e03-42d4-95d8-626622c2c92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {}\n",
    "\n",
    "embedding_clusters = []\n",
    "word_clusters = []\n",
    "\n",
    "for word in seed_words:\n",
    "    embeddings = []\n",
    "    words = []\n",
    "    for neighbor in get_most_similar(word):\n",
    "        words.append(neighbor)\n",
    "        embeddings.append(model.U.weight[vocabulary.lookup_indices([neighbor])[0]].detach().cpu().numpy())\n",
    "    embedding_clusters.append(embeddings)\n",
    "    word_clusters.append(words)\n",
    "    \n",
    "embedding_clusters = np.array(embedding_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe22048",
   "metadata": {},
   "source": [
    "#### Dimensionality Reduction\n",
    "\n",
    "Our word embeddings are of size 300 (by default). This makes plotting them a bit tricky :). We therefore use a dimensionality reduction technique called T-SNE to map the word embeddings from the 300d space to a 2d space. A deeper discussion of T-SNE is beyond our scope here, but feel free to explore yourself how T-SNE works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4050b64f-a4bb-4ce3-8a1b-7b3917e98792",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n, m, k = embedding_clusters.shape\n",
    "\n",
    "tsne_model_en_2d = TSNE(perplexity=15, n_components=2, n_iter=3500, random_state=32)\n",
    "embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4bd0b6",
   "metadata": {},
   "source": [
    "#### Plot Results\n",
    "\n",
    "Lastly, the method `tsne_plot_similar_words()` implemented in the file `src.utils` plots our cluster of word embeddings that are now all in the 2d space. Again, the results very much depend on how much training data you used and how long you trained the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ba6aa5-bcdc-4b95-ba4d-e8b63cf09ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot_similar_words('', seed_words, embeddings_en_2d, word_clusters, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86c6785-d07e-4f31-9b09-a89edcf9cf55",
   "metadata": {},
   "source": [
    "Assuming you have trained over the complete dataset for at least 10 epochs, the plot above should intuitive results where words with embeddings of the same color are indeed semantically related (e.g., the region/cluster containing the words *\"music\"*, *\"tune\"*, *\"soundtrack\"*, etc.). In general, the longer the training, the better the results, but even a few epochs should suffice here to get meaningful word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab233a4c-f72b-4671-948b-e3bde6618ec7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c24298-7678-4f87-a2de-561a1d2f217d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "CBOW (Continuous Bag-of-Words) is a popular word embedding technique used in NLP. It belongs to the family of shallow neural network models and is used to represent words as dense vectors in a continuous vector space. CBOW aims to predict a target word based on the context words surrounding it. In CBOW, the training process involves creating a sliding window over a text corpus. The window size determines the number of context words considered. The model takes the context words as input and predicts the target word. This prediction is based on the learned word embeddings and the weights of the neural network. The objective of CBOW is to maximize the probability of predicting the correct target word given the context words.\n",
    "\n",
    "During training, CBOW constructs a hidden layer that represents the aggregated information from the context words. This hidden layer acts as a continuous vector representation of the context. The word embeddings are learned by updating the weights of the neural network using backpropagation and gradient descent. CBOW has several advantages. It is computationally efficient compared to other word embedding techniques like Skip-gram, which considers each word as a target word and predicts the context words. CBOW is also known to work well for frequent words and is faster to train due to its simpler architecture. However, it may struggle with rare words or words with multiple meanings, as it does not capture the individual characteristics of each word.\n",
    "\n",
    "In summary, CBOW is a shallow neural network model used for creating word embeddings. It predicts a target word based on the context words, learns dense vector representations for words, and aims to maximize the probability of correct predictions. CBOW is computationally efficient, works well for frequent words, but may not capture the nuances of rare or polysemous words. And after completing this notebook, you now know how to train word embeddings using CBOW yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbcb5a2-3a3d-4bce-8c90-5e4c272e62fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs5246",
   "language": "python",
   "name": "cs5246"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
