{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93eaf864-8281-40e4-bb6e-35429823b601",
   "metadata": {},
   "source": [
    "<img src='data/images/section-notebook-header.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d19d0a9-13f7-408c-82f8-ae7dbef28125",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron (MLP)\n",
    "\n",
    "A Multi-Layer Perceptron (MLP) is a type of artificial neural network that is widely used for various tasks, including classification, regression, and pattern recognition. An MLP consists of multiple layers of interconnected artificial neurons organized into an input layer, one or more hidden layers, and an output layer. Each neuron in a layer is connected to every neuron in the subsequent layer, forming a fully connected network. An MLP is a feedforward neural network, which means that the data is transmitted from the input layer to the output layer in the forward direction.\n",
    "\n",
    "The neurons in an MLP are often modeled as perceptrons, which are basic computational units that take input values, apply weights to them, and pass the weighted sum through an activation function. The activation function introduces non-linearity into the network, allowing it to model complex relationships between inputs and outputs. For example, if the activation function is the Sigmoid function, the perceptron is identical to a Logistic Regression unit.\n",
    "\n",
    "During training, an MLP learns the optimal weights for its connections by adjusting them based on the error between the predicted outputs and the desired outputs. This process, known as backpropagation, uses gradient descent optimization to iteratively update the weights and minimize the error. MLPs are considered shallow neural networks compared to deep neural networks, which have many more hidden layers. However, MLPs can still capture complex patterns and are effective for a wide range of tasks. They have been used successfully in various domains, including image recognition, natural language processing, and financial modeling.\n",
    "\n",
    "**Important:** This notebook does not serve as a proper introductory tutorial into PyTorch, it only provides a minimal example for implementing a basic MLP text classifier, glossing over many details. If you are totally new to PyTorch, it's highly recommended to check out the tutorials on the [official PyTorch page](https://pytorch.org/tutorials/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4767575d-c9d2-495f-88c8-5e9a0c5947d4",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0bab8a-49d2-46a4-b17f-79dcb5da5cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877d8348-19e2-4be2-b4c7-024df3cd45ce",
   "metadata": {},
   "source": [
    "We utilize PyTorch as our deep learning framework of choice by importing the `torch` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc40e1d9-b2a2-4fcf-a066-893e0b5fd4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58591d4-ab6a-4e78-a83d-e9da562c5c44",
   "metadata": {},
   "source": [
    "As usual, we rely on spaCy to perform basic text preprocessing and cleaning steps, mainly tokenization and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5224b57-90ea-4ce7-b893-143c7185f360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# We use spaCy for preprocessing, but we only need the tokenizer and lemmatizer\n",
    "# (for a large real-world dataset that would help with the performance)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['ner', 'parser'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a599743-2bdd-4621-b359-fa690cf59e5b",
   "metadata": {},
   "source": [
    "### Checking/Setting the Computation Device\n",
    "\n",
    "PyTorch allows to train neural networks on supported GPUs to significantly speed up the training process. If you have a support GPU, feel free to utilize it. However, for this notebook it's certainly not needed as our dataset is very small and our network model is very simple. In fact, the training might be faster on the CPU here since initializing memory on the GPU and moving the data to the GPU involves some overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abac52a-5539-4b7b-9681-6910031304c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Use this line below to enforce the use of the CPU (in case you don't have a supported GPU)\n",
    "# With this small dataset and simple model you won't see a difference anyway\n",
    "use_cuda = False\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "print(\"Available device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc21c9f-64bb-44ce-80f2-5936919fdd90",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95605cb-d78c-4176-aff3-89f787d4c559",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "For this notebook, we use a simple dataset for sentiment classification. This dataset consists of 10,662 sentences, where 50% of the sentences are positive and 50% of the sentences are negative, split across 2 text files. Since PyTorch assumes that all class labels are integers between `0` and `C-1` -- where `C` is the number of classes -- we assume that negative sentences are labeled with `0` and positive sentences are labeled with `1`. The dictionary `label_map` reflects this idea.\n",
    "\n",
    "### Loading Sentence/Label Pairs from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde43613-7446-4e5a-b4f0-755c83cde028",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [\n",
    "    \"data/corpora/sentence-polarity/sentence-polarity.pos\",\n",
    "    \"data/corpora/sentence-polarity/sentence-polarity.neg\"\n",
    "]\n",
    "\n",
    "label_map = {'neg': 0, 'pos': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b5b311-5330-4038-943d-66d7d9d50ad9",
   "metadata": {},
   "source": [
    "We generate our dataset by reading both text files containing the sentences. For each text file, we derive the class label from the file extension (\"pos\" or \"neg\"). When iterating through both files, we use spaCy to preprocess each sentence. More specifically, we\n",
    "\n",
    "* Tokenize each sentence\n",
    "\n",
    "* Lemmatize each token, and\n",
    "\n",
    "* Filter out any non-words (e.g., numbers and punctuation marks) by check each token if `is_alpha==True`\n",
    "\n",
    "Feel free to change the preprocessing steps to see if and how it might affect the results. For example, you can consider explicitly removing stopwords by additionally checking `is_stop==False` for each token. In the code cell below, we omit this step for two main reason:\n",
    "\n",
    "* Since we use TF-IDF weights for the term-document matrix -- see below -- the weights for stopwords are expected to me very small\n",
    "\n",
    "* Given the task of sentiment analysis, stopwords such as *\"not\"*, *\"n't\"*, *\"never\"*, etc. might be very useful, particularly if we consider features beyond unigrams (e.g., bigrams, trigrams, etc.)\n",
    "\n",
    "But again, those are initial assumptions that may or may not hold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6f4d0d-8d27-435d-9bfc-a904fe34653e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, labels = [], []\n",
    "\n",
    "with tqdm(total=10662) as progress_bar:\n",
    "    \n",
    "    # Loop over all file names\n",
    "    for file_name in file_names:\n",
    "\n",
    "        # Get sentiment label from file name extensions\n",
    "        label = label_map[file_name.split(\".\")[-1]]\n",
    "\n",
    "        # Loop over each sentence (1 sentence per line)\n",
    "        with open(file_name) as file:\n",
    "            for line in file:\n",
    "                sentence = line.strip()\n",
    "                # Perform spaCy magic\n",
    "                doc = nlp(sentence)\n",
    "                sentence = ' '.join([ t.lemma_.lower() for t in doc if t.is_alpha == True ])\n",
    "                sentences.append(sentence)\n",
    "                # Add currently label to labels list\n",
    "                labels.append(label)\n",
    "                # Update progress bar\n",
    "                progress_bar.update(1)\n",
    "                \n",
    "                \n",
    "print(\"Total number of sentences: {}\".format(len(sentences)))\n",
    "print()\n",
    "print(sentences[0])  # Print an example sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b728c8fc-19f5-46b8-b4ae-c5020a54c0d4",
   "metadata": {},
   "source": [
    "### Create Training & Test Set\n",
    "\n",
    "To evaluate any classifier, we need to split our dataset into a training and a test set. With the method `train_test_split()` this is very easy to do; this method also shuffles the dataset by default, which is important for this example, since the dataset file is ordered with all positive sentences coming first. In the example below, we set the size of the test set to 20%.\n",
    "\n",
    "**Note:** The variable `sentences` containing all 10,662 from the input files lists first all 5,331 positive sentences and then all 5,331 negative sentences. Since we want to avoid that the test set but also training batches contain only sentences of the same label, it is strongly recommended to use shuffling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c8e763-c7f7-44db-ab32-e799eab2ae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split sentences and labels into training and test set with a test set size of 20%\n",
    "sentences_train, sentences_test, labels_train, labels_test = train_test_split(sentences, labels, test_size=0.2, random_state=0)\n",
    "\n",
    "# We can directly convert the numerical class labels from lists to numpy arrays\n",
    "y_train = np.asarray(labels_train, dtype=np.int8)\n",
    "y_test = np.asarray(labels_test, dtype=np.int8)\n",
    "\n",
    "print(\"Size of training set: {}\".format(len(sentences_train)))\n",
    "print(\"Size of test set: {}\".format(len(sentences_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5afb3b-55fb-4b5d-84e0-95f95d299f5f",
   "metadata": {},
   "source": [
    "### TF-IDF Feature Extraction\n",
    "\n",
    "To serve as valid input for our network, we need to convert our sentences into document vectors. For this, we use as always a scikit-learn vectorizer. In the code cell below, you can try the TF-IDF vectorizer or the Count vectorizer. The final results will be more or less the same. Still, feel free to play with the choice of the vectorizer as well as with the different input parameters (e.g., `ngram_range` or `max_features`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf1edf0-b6c7-46d1-b55e-6e98ce04b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Term-Document Matrix for different n-gram sizes\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1), max_features=10000)\n",
    "#vectorizer = CountVectorizer(ngram_range=(1, 1), max_features=10000)\n",
    "\n",
    "# Vectorize both training and test set\n",
    "X_train = vectorizer.fit_transform(sentences_train)\n",
    "X_test = vectorizer.transform(sentences_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e95375-0b9f-4af2-9408-eaac626bea26",
   "metadata": {},
   "source": [
    "### Creating Tensors\n",
    "\n",
    "Both `X_train` and `X_test` are now our matrices containing the document vectors of our training and test set. However, right now, `X_train` and `X_test` are sparse matrices, i.e., representations that only store the non-zero values. To further use this data with PyTorch, we have to perform 2 additional steps:\n",
    "\n",
    "* Convert the sparse representation to a dense (i.e., full/normal) representation using `.todense()`; the output will be NumPy arrays\n",
    "\n",
    "* Convert NumPy arrays to tensors. `Tensor` is the data object used by PyTorch; they look, feel, and handle basically the same as numpy arrays.\n",
    "\n",
    "**Side note:** While PyTorch tensors \"look\" like NumPy arrays, under the hood, they additionally support all mechanisms and methods for deep learning tasks, offer native GPU acceleration and computational graph capabilities, and have a dedicated ecosystem for deep learning. NumPy is a general-purpose numerical computing library with a broader range of applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfc34ea-1b43-4509-869b-6321c4de8dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default Tensor stores float values\n",
    "X_train = torch.Tensor(X_train.todense())\n",
    "X_test = torch.Tensor(X_test.todense())\n",
    "\n",
    "# Our labels are integers, hence we use LongTensor\n",
    "# (that's required, otherwise we would get an error later)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1329bccd-540b-4528-ac3c-1e50bfa08435",
   "metadata": {},
   "source": [
    "### Creating PyTorch Datasets\n",
    "\n",
    "Training a neural network is usually not done computing the gradient w.r.t. the whole dataset as most of the time the dataset is way too large to fit into memory. It might also slow down training since the gradients w.r.t. the whole dataset can be very small (although this could be addressed by increasing the learning rate). In practice, the training is basically always done using batches, i.e., much smaller subsets of the data. While we can take `X_train` and `X_test` and implement our own loops to create batches, PyTorch comes with a series of convenient utility classes to simplify things.\n",
    "\n",
    "The first utility class we use is the [`Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) class; more specifically, since `Dataset` is in abstract class, we use the [`TensorDataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset) class to wrap our tensors and make each sample retrievable by indexing the tensors along the first dimension. This will be used by the data loaders below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94153c94-821c-4118-b458-f3ff683f1a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = TensorDataset(X_train, y_train)\n",
    "dataset_test = TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db00cd0-1bec-4b2f-8675-997c6623f676",
   "metadata": {},
   "source": [
    "### Creating Data Loaders\n",
    "\n",
    "The [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) class takes a `Dataset` object as input to handle to split the dataset into batches. As such, a data loader also has `batch_size` as an input parameter. In the following, we use a batch size of 64, although you can easily go higher since we are dealing with only single sentences which are unlikely to take up much memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7ba17a-8c1a-4a6f-b32e-8fc393c7d8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88334763-e55e-490b-a416-e831cff94642",
   "metadata": {},
   "source": [
    "We can use the data loaders to loop over all batches and use them for training and testing our model. The code cell below shows the general idea. The for loop would iterate all the batches as to include all the samples in the dataset. We therefore use a `break` to only look at the first batch and then stop the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbc07d8-01d4-40cb-875d-2b6921263946",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X_batch, y_batch in loader_train:\n",
    "    print(\"Shape of the input:  \", X_batch.shape)\n",
    "    print(\"shape of the output: \", y_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ce34b2-499c-4712-a4d0-2f9092652823",
   "metadata": {},
   "source": [
    "Appreciate how much additional code we would have written if we had implemented this batching on our own. Well, it actually wouldn't be that much, but using these utility classes makes our code much cleaner and less prone to errors. These utility classes also allow to train models in a distributed setting; but this is not important at the moment. In short, always try to make use of existing utility classes and methods as much as possible! Only implement your own solutions if you really require special functionalities not supported by available classes and methods!\n",
    "\n",
    "With respect to preparing our data, we are now ready to build and train a neural network model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb06d21-409f-4154-94d5-671e9600f2f0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d460f9e6-7c78-4bbb-b315-560d3b855ef5",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "\n",
    "In this notebook, we replicate the exact model architecture we used as an example on the lecture slides -- see the image below: We have 3 hidden layers and 2 outputs. Note that we could implement the model with just 1 output since we are trying to solve binary classification tasks. However, using 1 output and the Binary Cross Entropy Loss or using 2 outputs and using the Multiclass Cross Entropy Loss is completely equivalent. Hence, we simply implement to model exactly like visualized below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38347238-6de4-470f-8353-63520507baff",
   "metadata": {},
   "source": [
    "<img src=\"data/images/mlp-example-network.png\" width=\"90%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd5c7a3-3413-4cca-b452-b5c03bf9d363",
   "metadata": {},
   "source": [
    "Of course, our input layer doesn't have just 3 features but 10,000 (assuming that's the value of `max_features` of the vectorizer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fc5a57-cca2-426f-9d42-6ba61a05194e",
   "metadata": {},
   "source": [
    "### Basic Implementation\n",
    "\n",
    "The code cell below shows the most straightforward implementation of our network architecture. To make the model a bit flexible, we have the `vocab_size` (vocabulary size) as an input parameter. This means, any time we change the `max_features` parameter of the vectorizer, we define our model using this value. In this implementation, we use [`nn.ReLU`](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) (Rectified Linear Unit) as the activation function for each unit, which became often of the activation of choice. Still, fill free to try out other activation functions (e.g., [`nn.Tanh`](https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html#torch.nn.Tanh)). Of course, you can also mix it up across the different hidden layers.\n",
    "\n",
    "For the activation function for the output layer we use [`nn.LogSoftmax`](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax) -- instead of [`nn.Softmax`](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#torch.nn.Softmax) -- which applies $\\log(Softmax(x))$ to its input. This is very common in practice as it makes computation typically numerical stable as Softmax probabilities can be very small, particularly in case of many class labels. While we have only 2 classes here, and Softmax will probably do just fine, using the LogSoftmax is just a good practice. Of course, the outputs are no longer probabilities but log probabilities. This will affect the choice of the loss functions; see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9896599-2cac-445f-a407-77ef48702d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet1(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        # Define 1st fully connected (i.e., linear) hidden layer\n",
    "        self.fc1 = nn.Linear(self.vocab_size, 4)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # Define 2st fully connected (i.e., linear) hidden layer\n",
    "        self.fc2 = nn.Linear(4, 3)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        # Define 3st fully connected (i.e., linear) hidden layer\n",
    "        self.fc3 = nn.Linear(3, 3)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # Define output layer (which is also a linear layer)\n",
    "        self.out = nn.Linear(3, 2)        \n",
    "        # Define log softmax layer\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = self.fc1(X)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.out(out)\n",
    "        log_probs = self.log_softmax(out)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791b6a4d-2c1e-4acc-81e4-57766a05b06b",
   "metadata": {},
   "source": [
    "To \"visualize\" the network, we can create and print an instance of the class `SimpleNet1`. The output should reflect the model architecture shown in the image above.\n",
    "\n",
    "The command `.to(device)` \"moves\" the instance to the selected instance (e.g., the CPU or GPU). In general, both the model and the data need to reside on the same instance. So if the model will be on the GPU, we also need to move the data later to the GPU. Using consistently `.to(device)` on the model and the data (see below) ensures that there will be no mismatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c479be4-8253-4e39-ab04-cd4bad733583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model and move to device\n",
    "classifier = SimpleNet1(X_train.shape[1]).to(device)\n",
    "\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fefc765-1aae-4a33-819b-b8aacb71ade5",
   "metadata": {},
   "source": [
    "### Modified Implementation\n",
    "\n",
    "In the implementation above, we used the most basic approach by having 2 main parts:\n",
    "\n",
    "* Defining all the layers in the `__init__()` method (i.e., the constructor)\n",
    "\n",
    "* *\"Push\"* the data through all layers in the `forward()` method\n",
    "\n",
    "While this is perfectly fine and for more complex models essentially required, here we can use some additional utility classes to simplify our code. Note how we push our data step-by-step (i.e., sequentially) through each layer. This means we can create a model component using [`nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) that contains all our layers -- here, the model component is basically the complete model. Thus, in the `forward()` method, we only need to give the component the data, and the component will automatically push the data through all the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec227eb-dccc-4ac3-a0f3-52adf980834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet2(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.vocab_size, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3, 2),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        log_probs = self.net(X)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1660c6ed-535f-43e3-a6d0-43c04ff7b3dd",
   "metadata": {},
   "source": [
    "This is a neat way to save lines of code and make the implementation more readable and maintainable. Of course, if we create and print an instance of this class, the output should match the one frome `SimpleNet1` with respect to the core layers. Since we use `nn.Sequential` here, the output is not exactly the same, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a433594f-213e-4754-951f-e660d6949282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model and move to device\n",
    "classifier = SimpleNet2(X_train.shape[1]).to(device)\n",
    "\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9412c9c0-01d3-476f-bb79-9b353218ca44",
   "metadata": {},
   "source": [
    "There are several other ways to write the same model using different code structures. For example, PyTorch allows to define submodels and then combine them for the final/complete model. This is particularly useful for very large models that also might re-use different submodels -- you will later see this when we cover the Transformer model. For our very simple MLP model here, the definition of submodels would be just overkill and make the code actually unnecessarily complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccedd111-80c7-48ed-85fa-dec417203ed9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcdfc95-3717-444e-a5fe-1e8b353aa2b0",
   "metadata": {},
   "source": [
    "## Training & Evaluating the Model\n",
    "\n",
    "With the data prepared and the model architecture defined, we can now train and evaluate a model to build our sentiment classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382ea92d-8c40-414f-98bb-d9ceb36ffc47",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "The code cell below implements the method `evaluate()` to, well, evaluate our model. Apart from the model itself, the method also receives the data loader as input parameter. This allows us later to use both `loader_train` and `loader_test` to evaluate the training and test loss using the same method.\n",
    "\n",
    "The method is very generic and is not specific to the dataset. It simply loops over all batches of the data loader, computes the log probabilities, uses these log probabilities to derive the predicted class labels, and compares the predictions with the ground truth to return the f1 score. This means, this method could be used \"as is\" or easily be adopted for all kinds of classifications tasks (incl. tasks with more than 2 classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b952ff-ce1a-4bf5-a838-43a69246431c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "\n",
    "    # Set model to \"eval\" mode (not needed here, but a good practice)\n",
    "    model.eval()\n",
    "\n",
    "    # Collect predictions and ground truth for all samples across all batches\n",
    "    y_pred, y_test = [], []\n",
    "\n",
    "    with tqdm(total=len(loader)) as pbar:\n",
    "        \n",
    "        # Loop over each batch in the data loader\n",
    "        for X_batch, y_batch in loader:\n",
    "\n",
    "            # Move data to device\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # Push batch through network to get log probabilities for each sample in batch\n",
    "            log_probs = model(X_batch)                \n",
    "            \n",
    "            # The predicted labels are the index of the higest log probability (for each sample)\n",
    "            y_batch_pred = torch.argmax(log_probs, dim=1)\n",
    "\n",
    "            # Add predictions and ground truth for current batch\n",
    "            y_test += list(y_batch.detach().cpu().numpy())\n",
    "            y_pred += list(y_batch_pred.detach().cpu().numpy())\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Set model to \"train\" mode (not needed here, but a good practice)\n",
    "    model.train()            \n",
    "            \n",
    "    # Return the f1 score as the output result\n",
    "    return metrics.f1_score(y_test, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5466f6-7e3a-498c-9caf-d9748c3eea72",
   "metadata": {},
   "source": [
    "For a quick test, let's evaluate the newly created model. Of course, we didn't train our model, but it will still make predictions based on the initial weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e5d753-ee64-4489-84fc-14d68146f57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate(classifier, loader_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff0777b-ce55-473a-ba70-0d2f42dc5d9b",
   "metadata": {},
   "source": [
    "**Side note:** When looking at the method above, you might have noticed that we never explicitly call the `forward()` method of our model. Instead, we have the line `log_probs = model(X_batch)`. Note that this is exactly the same as calling `log_probs = model.forward(X_batch)`. Python allows to define a method `__call__` for a class which makes an instance of a model behave like a function. In PyTorch, the `__call__` method simply calls the `forward()` methods. This allows us to drop the `forward` name, and is de-facto standard in PyTorch code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2441710d-8833-4975-b5df-47942ab1f3f1",
   "metadata": {},
   "source": [
    "### Training (and evaluation after each epoch)\n",
    "\n",
    "Similar to the method `evaluate()` we also implement a method `train()` to wrap all the required steps training. This has the advantage that we can simply call `train()` multiple times to proceed with the training. Apart from the model, this method has the following input parameters:\n",
    "\n",
    "* `loader_train` and `loader_test`: this allows us to compute the f1 score over the training data an the test data after each epoch; we can later visualize the changes in the f1 scores\n",
    "\n",
    "* `optimizer`: the optimizer specifies how the computed gradients are used to updates the weights; in the lecture, we only covered the basic Stochastic Gradient Descent, but there are much more efficient alternatives available\n",
    "\n",
    "* `criterion`: this is the loss function; \"criterion\" is just very common terminology in the PyTorch documentation and tutorials\n",
    "\n",
    "* `num_epochs`: the number of epochs -- i.e., the number of times we want train over all samples in our dataset\n",
    "\n",
    "The heart of the method is the snippet described as PyTorch Magic. It consists of the following 3 lines of code\n",
    "\n",
    "* `optimizer.zero_grad()`: After each training step for a batch, set the gradients back to zero for the next batch\n",
    "\n",
    "* `loss.backward()`: Calculate all gradients using backpropagation\n",
    "\n",
    "* `optimizer.step()`: Update all weights using the gradients and the method of the specific optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a63a2d1-ddca-453f-9290-f6fc0d93233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader_train, loader_test, optimizer, criterion, num_epochs):\n",
    "\n",
    "    losses, f1_train, f1_test = [], [], []\n",
    "    \n",
    "    # Set model to \"train\" mode (not needed here, but a good practice)\n",
    "    model.train()\n",
    "\n",
    "    # Run all epochs\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "\n",
    "        # Initialize epoch loss (cummulative loss fo all batchs)\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        with tqdm(total=len(loader_train)) as pbar:\n",
    "\n",
    "            # Loop over each batch in the data loader\n",
    "            for X_batch, y_batch in loader_train:\n",
    "\n",
    "                # Move data to device\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "                # Push batch through network to get log probabilities for each sample in batch\n",
    "                log_probs = classifier(X_batch)                \n",
    "\n",
    "                # Calculate loss\n",
    "                loss = criterion(log_probs, y_batch)\n",
    "\n",
    "                ### PyTorch Magic! ###\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Keep track of overall epoch loss\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                pbar.update(1)\n",
    "        \n",
    "        # Keep track of all epoch losses\n",
    "        losses.append(epoch_loss)\n",
    "        \n",
    "        # Compute f1 score for both TRAINING and TEST data\n",
    "        f1_tr = evaluate(model, loader_train)\n",
    "        f1_te = evaluate(model, loader_test)\n",
    "        f1_train.append(f1_tr)\n",
    "        f1_test.append(f1_te)\n",
    "\n",
    "        print(\"Loss:\\t{:.3f}, f1 train: {:.3f}, f1 test: {:.3f} (epoch {})\".format(epoch_loss, f1_tr, f1_te, epoch))\n",
    "     \n",
    "    # Return all losses and f1 scores (all = for each epoch)\n",
    "    return losses, f1_train, f1_test        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0924b60-0244-475f-a14a-7ffea9b4500d",
   "metadata": {},
   "source": [
    "Before we can actually train the model, we need to instantiate the `criterion` (i.e., the loss function) and the `optimizer`. Since out model returns log probabilities, we need to use the [`nn.NLLLoss()`](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html), i.e., the Negative Log Likelihood Loss. This is basically the same as the Multiclass Cross Entropy Loss but using log probabilities instead of probabilities.\n",
    "\n",
    "For the optimizer, we pick the widely used [`Adam`](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) optimizer. It stands for \"Adaptive Moment Estimation\" and it maintains an adaptive learning rate for each parameter in the neural network. It computes individual learning rates based on estimates of the first and second moments of the gradients. This adaptive learning rate enables the optimizer to converge quickly and effectively navigate complex optimization landscapes. A deeper technical discussion about Adam is way beyond our scope here. In essence, however, it performs the same task as Gradient Descent: minimizing the loss by adjusting the learnable parameters based on their gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b230ef76-9592-4e32-b5e4-5cb97aa9241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model and movie to device\n",
    "classifier = SimpleNet2(X_train.shape[1]).to(device)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Define optimizer (you can try, but the basic (Stochastic) Gradient Descent is actually not great)\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
    "# Feel free to try SGD to see how poorly it performs compared to Adam\n",
    "#optimizer = torch.optim.SGD(classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f56b411-f4d2-4664-83d7-2ce38b873a44",
   "metadata": {},
   "source": [
    "Now we finally have everything in place to train the model. For this, we now only need to call the `train()` in the code cell below. Note that you can run the code cell below multiple times to continue the training for further 10 epochs. Each epoch will print 3 progress bars:\n",
    "\n",
    "* training over training set\n",
    "\n",
    "* evaluating over training set\n",
    "\n",
    "* evaluating over test set\n",
    "\n",
    "After each epoch, a print statement will show the current loss as well as the latest f1 scores for the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117a3b09-4bcf-4006-bb8c-78d843682706",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "losses, f1_train, f1_test = train(classifier, loader_train, loader_test, optimizer, criterion, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a2f01d-0c96-4a7e-82eb-141ec71d9336",
   "metadata": {},
   "source": [
    "### Plotting the Results\n",
    "\n",
    "Since the method `train()` returns the losses and f1 scores for each epoch, we can use this data to visualize how the loss and the f1 scores change over time, i.e., after each epoch. The code cell below creates the corresponding plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40086674-fe58-4708-967b-b337e3a72df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(1, len(losses)+1))\n",
    "\n",
    "# Convert losses to numpy array\n",
    "losses = np.asarray(losses)\n",
    "# Normalize losses so they match the scale in the plot (we are only interested in the trend of the losses!)\n",
    "losses = losses/np.max(losses)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(x, losses, lw=3)\n",
    "plt.plot(x, f1_train, lw=3)\n",
    "plt.plot(x, f1_test, lw=3)\n",
    "\n",
    "font_axes = {'family':'serif','color':'black','size':16}\n",
    "\n",
    "plt.gca().set_xticks(x)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.xlabel(\"Epoch\", fontdict=font_axes)\n",
    "plt.ylabel(\"F1 Score\", fontdict=font_axes)\n",
    "plt.legend(['Loss', 'F1 (train)', 'F1 (test)'], loc='lower left', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6011403-3aeb-486e-bd86-69a87417047e",
   "metadata": {},
   "source": [
    "From the plot, we can observe several things\n",
    "\n",
    "* The loss goes down! This essentially means that our model is learning. This can be very useful as an incorrectly implemented model may not throw an error but not train properly (i.e., the loss not going down). So it's a good sanity check when implementing and using a new model.\n",
    "\n",
    "* The f1 score over the training data reaches 1.0 (at least after more epoch). This means that the model learns to correctly predict the label for all training samples. Of course, this is not what we are interested in but (a) again tells us that the model is generally learning, and (b) this might give us insights that our model is overfitting.\n",
    "\n",
    "* The f1 score over the test data almost plateaus after Epochs 3-4 and even tend to decrease a bit later. This indicates that the model starts to overfit after Epoch 6\n",
    "\n",
    "**Important:** This plot showing the trends for the loss and f1 scores (or other metrics) can look very different depending on the dataset, the network architecture, and the hyperparameters. While in this simple example all trends seem to smoothly converge, this is not the case in general. For example, the f1 score for the test data might see a much steeper drop after peaking, which would even more clearly indicate the model is starting to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706b9078-fe5e-4701-a872-fe1609bc4324",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28199202-477e-4260-83af-d83a6fa251ed",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we built our first text classifier using PyTorch. In more detail, we trained and evaluated a Multi-Layer Perceptron to build a classifier for sentiment analysis. Building and training more complex architecture might require more code, but most of the essential steps have been covered here. For example, we already made use of important utility classes such as `Dataset` and `DataLoader` that help us prepare our datasets for the training using batches. Also method 'train()' shows the general blueprint of the train loop in PyTorch: loop over each batch, compute model output, compute loss and perform the \"PyTorch Magic\" (i.e.: compute gradients using backpropagation and all update trainable parameters).\n",
    "\n",
    "Appreciate how PyTorch hides all the nitty-gritty details of the training process, most importantly the computation of all gradients and the updates of the weights. While implementing backpropagation from scratch is not too difficult for simple MLP architectures, for more advanced architectures covered later this would become quite demanding. MLPs for simple text classification can provide decent results, especially for tasks with moderate-sized datasets and relatively simple classification requirements. However, for more complex text classification tasks, recurrent neural networks (RNNs), convolutional neural networks (CNNs), or transformer-based models like BERT may be more suitable and can achieve state-of-the-art performance.\n",
    "\n",
    "**Side note:** The most obvious limitation of using an MLP architecture is that it does not support sequential data but treats text documents as a \"Bag of Words\" (here with TF-IDF weights). We only preserve some information about the word order if we consider bigrams, trigrams, and so on when vectorizing our documents. To really utilize the sequential nature of text data we have to rely on more sophisticated architectures such as RNNs and Transformers. CNNs are also rather limited in this regard as they also only consider word order on a more local level but not fully across complete sentences or documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e929ff-d1af-43f6-ae45-376f9ae3e369",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs5246",
   "language": "python",
   "name": "cs5246"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
