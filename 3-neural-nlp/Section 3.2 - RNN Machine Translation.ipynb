{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4f5859c-9a95-4f65-8fcd-87e1d757df82",
   "metadata": {},
   "source": [
    "<img src='data/images/section-notebook-header.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e3a3d0-633c-4e3e-956b-a6c01b28aa16",
   "metadata": {},
   "source": [
    "# Machine Translation with RNN Encoder-Decoder\n",
    "\n",
    "Recurrent Neural Networks (RNNs) have been widely used in machine translation tasks, particularly for sequence-to-sequence modeling. Here's how RNNs can be applied to machine translation:\n",
    "\n",
    "* **Encoder-Decoder Architecture:** RNNs are often used in a specific architecture called the encoder-decoder model. The encoder takes the source language sentence as input and processes it sequentially, word by word, capturing the contextual information and representing it as a fixed-length vector, often referred to as the \"context\" or \"thought\" vector. The encoder typically consists of recurrent layers, such as LSTM or GRU, which maintain a hidden state that carries information from previous words to the next.\n",
    "\n",
    "* **Attention Mechanism:** In machine translation, long sentences or phrases can be challenging to translate accurately, as the entire sentence needs to be encoded into a fixed-length vector. To address this, attention mechanisms were introduced. Attention allows the decoder to focus on different parts of the source sentence while generating the translation. It dynamically determines which words in the source sentence are most relevant at each decoding step.\n",
    "\n",
    "* **Decoding and Generation:** Once the encoder has processed the source sentence and produced the context vector, the decoder RNN takes over. It generates the translated sentence word by word, conditioned on the context vector and the previously generated words. At each decoding step, the decoder RNN takes the context vector, the previously generated word, and the hidden state as input, and predicts the probability distribution over the target vocabulary. The word with the highest probability is selected as the output, and the process continues until an end-of-sentence token is generated.\n",
    "\n",
    "* **Training and Optimization:** RNNs in machine translation are trained using parallel corpora, which consist of pairs of source and target language sentences. During training, the model is optimized to minimize the difference between the predicted translation and the target translation. This is typically done using maximum likelihood estimation or variants of it, such as teacher forcing. Optimization techniques like backpropagation through time (BPTT) are used to update the parameters of the encoder and decoder RNNs.\n",
    "\n",
    "* **Beam Search:** During inference, when generating translations for unseen sentences, beam search is often employed. Instead of greedily selecting the word with the highest probability at each decoding step, beam search explores multiple hypotheses simultaneously. It maintains a beam of the most likely partial translations and expands them based on the predicted probabilities. This allows the model to consider a wider range of possibilities and generate more accurate translations.\n",
    "\n",
    "RNN-based machine translation models have demonstrated impressive performance and have been the dominant approach for a long time. However, they can still face challenges in capturing long-range dependencies and dealing with out-of-vocabulary words. More recent approaches, such as transformer-based models, have shown significant improvements in machine translation tasks. Nonetheless, RNNs remain foundational in understanding the evolution of machine translation models.\n",
    "\n",
    "The image below, taken from the lecture slides, shows encoder-decoder architecture proposed by [Sutskever et al. in 2014](https://proceedings.neurips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf). In this architecture, both the encoder and decoder are RNNs, with the last hidden state of the encoder serving as the initial hidden state of the decoder.\n",
    "\n",
    "<img src='data/images/rnn-encoder-decoder-mt.png' />\n",
    "\n",
    "In this notebook, we will build and train a simple machine learning model from scratch, based on this RNN encoder-decoder architecture. The implementation presented will also include the concept of **attention**. Attention is a mechanism used in recurrent neural networks (RNNs) and other sequence-to-sequence models to focus on relevant parts of the input sequence when generating an output. It allows the model to selectively attend to different parts of the input sequence based on their importance or relevance to the current step of the decoding process.\n",
    "\n",
    "In traditional RNN models, such as the vanilla RNN or LSTM, the hidden state at each time step summarizes all the information from the previous steps. However, this fixed-length representation can be limiting, especially when processing long sequences or when certain parts of the input sequence are more important than others. Attention mechanisms address this limitation by dynamically computing a weighted sum of the input sequence, emphasizing certain parts that are more relevant to the current decoding step. It enables the model to attend to different parts of the input sequence with different weights, allowing it to focus on the most relevant information.\n",
    "\n",
    "In an RNN-based encoder-decoder architecture, the attention mechanism computes relevance scores by measuring the similarity between the current hidden state of the decoder and each hidden state of the encoder. There are various ways to calculate these scores, such as dot product, cosine similarity, or using a learned compatibility function. The scores are usually transformed into attention weights through a softmax function to ensure they sum up to 1. Finally, the attention weights are used to compute a weighted sum of the hidden states of the encoder, resulting in a context vector. This context vector is then combined with the current decoding step's hidden state to generate the output or update the hidden state for the next step. The images below, taken from the lecture slides, visualize the computations involved in the attention mechanisms\n",
    "\n",
    "<img src='data/images/rnn-encoder-decoder-attention-01.png' width='80%' />\n",
    "<img src='data/images/rnn-encoder-decoder-attention-02.png' width='80%' />\n",
    "<img src='data/images/rnn-encoder-decoder-attention-03.png' width='80%' />\n",
    "<img src='data/images/rnn-encoder-decoder-attention-04.png' width='80%' />\n",
    "\n",
    "By incorporating attention mechanisms, RNN models can selectively attend to different parts of the input sequence, giving them the ability to effectively process long sequences, focus on important information, and improve the quality of their predictions or translations. The implementation of our machine translation model will be flexible enough to specify if attention should be used or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a279043-60ad-4be7-aa98-9f28b5f2d5c6",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23427ee-2b9a-43b9-9218-bf0ce3f533b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760c0617-226d-4383-9666-fd63bbbadbb6",
   "metadata": {},
   "source": [
    "We utilize some utility methods from PyTorch, so we need to import the `torch` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf18e836-cd82-45a4-964e-e6ad1d1eeb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0d3101-3553-4310-b8cb-c02f9592055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rnn import Encoder, Decoder, RnnAttentionSeq2Seq\n",
    "from src.sampler import BaseDataset, EqualLengthsBatchSampler\n",
    "from src.utils import Dict2Class, get_line_count, plot_attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5571d77-959f-44c5-a0d0-903577c790f5",
   "metadata": {},
   "source": [
    "### Checking/Setting the Computation Device\n",
    "\n",
    "PyTorch allows to train neural networks on supported GPUs to significantly speed up the training process. If you have a support GPU, feel free to utilize it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bd7e2c-e9c3-4f5f-8e97-c7e2d535f4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Use this line below to enforce the use of the CPU \n",
    "#use_cuda = False\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "print(\"Available device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1bb7b2-058b-4880-865f-3beec9ad2ccd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d06c4a0-1c8a-4174-8413-bd6bce20fb2d",
   "metadata": {},
   "source": [
    "## Building & Testing the Model using a Toy Dataset\n",
    "\n",
    "The implementation of our machine translation model includes 3 classes -- all can be found in the file `src/rnn.py`\n",
    "\n",
    "* The `Encoder` class implements the RNN-based encoder that takes the sentence of the source language and encodes this sentence using the recurrent layer.\n",
    "\n",
    "* The `Decoder` class implement the RNN-based decoder that takes the last hidden state of the decoder -- and optionally all hidden states of the encoder in the case attentions is used -- the learn and generate the output sentences of the target language\n",
    "\n",
    "* The `RnnAttentionSeq2Seq` class simply wraps the encoder and decoder into a single class for easier use.\n",
    "\n",
    "Take some time to look and comprehend the code. Similar to previous implementations of the model, the code looks a bit verbose since both the encoder and decoder are quite configurable (see below). Appreciate that the encoder looks like the model for the RNN sentiment classifier and the decoder looks like the model for the RNN language model.\n",
    "\n",
    "### Create Toy dataset\n",
    "\n",
    "Before we train our model on the Tatoeba dataset we have prepared in the previous notebook, let's use some simple toy dataset to go through the steps of the encoder and decoder and the combined model. In the sample below, we manually create a batch of 3 pairs of input and target sentences. The indices have no association to any words/tokens as we don't really want to train anything here. Note that all input sequences have a length of 4 and all output sequences have a length 5. Later, when using the real data, we again have to address the challenge of working with sequences of variable lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824eb291-edd0-4a83-b92e-f0dd1fd9b653",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample = torch.tensor([[4, 5, 6, 7], [4, 5, 6, 7], [4, 5, 6, 7]]).to(device)\n",
    "Y_sample = torch.tensor([[5, 6, 7, 8, 9], [5, 6, 7, 8, 9], [5, 6, 7, 8, 9]]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c974acbc-2063-417b-914b-ff9566cb7a90",
   "metadata": {},
   "source": [
    "### Create Model\n",
    "\n",
    "#### Specify Configuration\n",
    "\n",
    "As previously mentioned, our model is implemented in a way to make it quite configurable. In principle, we could define the parameters for the encoder and decoder independently, but here we go with a single set of parameter values and name the parameters appropriately so it is clear if the parameter is relevant for the encoder or decoder. For the toy dataset, we just pick some simple values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8311e581-ec30-4ffd-a006-258ca302f15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"device\": device,                   # as the decoder also generates sentence it mus be able to move the data to the correct device\n",
    "    \"vocab_size_encoder\": 20,           # the size of the source vocabulary determines the input size of the encoder embedding\n",
    "    \"vocab_size_decoder\": 20,           # the size of the target vocabulary determines the input size of the decoder embedding\n",
    "    \"embed_size\": 50,                   # size of the word embeddings (here the same for encoder and decoder; but not mandatory)\n",
    "    \"rnn_cell\": \"GRU\",                  # in practice GRU or LSTM will always outperform RNN\n",
    "    \"rnn_hidden_size\": 128,             # size of the hidden state\n",
    "    \"rnn_num_layers\": 2,                # 1 or 2 layers are most common; more rarely sees any benefit\n",
    "    \"rnn_dropout\": 0.2,                 # only relevant if rnn_num_layers > 1\n",
    "    \"rnn_encoder_bidirectional\": True,  # The encoder can be bidirectional; the decoder can not\n",
    "    \"linear_hidden_sizes\": [1024],      # list of sizes of subsequent hidden layers; can be [] (empty); only relevant for the decoder\n",
    "    \"linear_dropout\": 0.2,              # if hidden linear layers are used, we can also include Dropout; only relevant for the decoder\n",
    "    \"attention\": \"DOT\",                 # Specify if attention should be used; only \"DOT\" supported; None if no attention\n",
    "    \"teacher_forcing_prob\": 0.5,        # Probability of using Teacher Forcing during training by the decoder\n",
    "    \"special_token_unk\": 1,             # Index of special token <UNK>\n",
    "    \"special_token_sos\": 2,             # Index of special token <SOS>\n",
    "    \"special_token_eos\": 3,             # Index of special token <EOS>\n",
    "    \"clip\": 0.5                         # Clipping value to limit gradients to prevent exploding gradients\n",
    "}\n",
    "\n",
    "params = Dict2Class(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fb6e69-a813-4b0a-b3e2-26df149450c3",
   "metadata": {},
   "source": [
    "In the list above there might be 2 parameters that haven't been covered in the lectures and you therefore might not be familiar with:\n",
    "\n",
    "* **Teacher Forcing:** Teacher Forcing is a technique commonly used when training RNN-based decoders in sequence-to-sequence models. It involves feeding the model with the ground truth (true) output sequence during training instead of using its own predictions from the previous time step. By using Teacher Forcing, the model is exposed to the ground truth output sequence during training, which helps in stabilizing the learning process and accelerating convergence. It provides more accurate and reliable gradients for updating the model's parameters.\n",
    "\n",
    "* **Gradient Clipping:** Gradient clipping is a technique used to mitigate the issue of exploding gradients during training. It involves rescaling the gradients when their norm exceeds a predefined threshold. The purpose of gradient clipping is to ensure more stable and reliable training by preventing large gradient values that can lead to unstable optimization or training difficulties. By applying gradient clipping, the overall effect is that the magnitude of the gradients is kept under control, preventing them from becoming too large. This helps stabilize the optimization process, enabling more reliable and efficient training of deep learning models. It allows the model to make more consistent and meaningful updates to its parameters, leading to better convergence and improved performance.\n",
    "\n",
    "Both Teacher Forcing and Gradient Clipping are not fundamentally required for training an RNN-based machine translation model, so a more detailed discussion is beyond our scope. However, in practice, those techniques -- and many other tricks and tweaks -- are used to achieve a more effective training and better results. So at least these two common techniques are implemented here.\n",
    "\n",
    "#### Encoder\n",
    "\n",
    "Let's create an encoder instance with the parameters defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e5e212-38e0-4ead-8db1-6164dcf28289",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(params).to(device)\n",
    "\n",
    "print(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e82674c-b402-42c6-93bd-39f300944e57",
   "metadata": {},
   "source": [
    "We can now give the encoder the input sequences of our toy batch and check how the output looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ede6310-5994-4dcd-9229-55a59b87cc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs, encoder_hidden = encoder(X_sample)\n",
    "\n",
    "print('Shape of encoder_outputs:', encoder_outputs.shape)\n",
    "print('Shape of encoder_hidden:', encoder_hidden.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da99cf62-302c-4a7e-9aa0-460d96152a83",
   "metadata": {},
   "source": [
    "The shapes of both outputs can be described as follows:\n",
    "\n",
    "* The shape of `encoder_outputs` is `(batch_size=3, seq_len=4, rnn_hidden_size=256)` representing the 3 sequences in the batch, with each sequences being of length 4, and each item in the sequences represented by a 256-dim vector. Recall that defining the encoder and decoder with 2 layers -- `rnn_num_layers=2`; `encoder_outputs` contains only the outputs after the last layer.\n",
    "\n",
    "* The shape of `encoder_hidden` is `(num_layers=2, batch_size=3, rnn_hidden_size=256)` representing the 2 recurrent layers, the 3 sequences in the batch, and each item in the sequences represented by a 256-dim vector.\n",
    "\n",
    "`encoder_hidden` will later be used to initialize the hidden state of the decoder; `encoder_outputs` will be required for the attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a95a2e0-c0df-41f0-9e88-ac517b6b2dc2",
   "metadata": {},
   "source": [
    "#### Decoder\n",
    "\n",
    "Now let's also create a decoder instance. Note that we also have to specify the criterion (i.e., the loss function) as the calculation of the loss has been moved into the implementation of the decoder. This makes its use when training the model much more convenient. In fact, the loss is the only output of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e99b23-c7f3-4dd2-a402-2080adb86ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "decoder = Decoder(params, criterion).to(device)\n",
    "\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dc4f54-545a-4e97-a81f-67d9d083d9f0",
   "metadata": {},
   "source": [
    "When training the decoder, it gets as input the batch of target sequences, the last hidden state of the decoder (to initialize the hidden state of the decoder), as well as all hidden states of the decoder for the attention mechanism. The output is the loss based on comparing the predicted target sequence and the true target sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fd1763-5b4b-4b53-a6d6-fb143253b648",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = decoder(Y_sample, encoder_hidden, encoder_outputs)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c5ba42-6ab6-4e53-8477-8cefaf0b0987",
   "metadata": {},
   "source": [
    "#### Training Encoder & Decoder Separately\n",
    "\n",
    "By combining the code snippets above, we can now easily write a loop to train our encoder-decoder architecture on the toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91f16de-c0a9-43c4-b9f6-c72ddd34fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizers for encoder and decoder\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.0005)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.0005)\n",
    "\n",
    "for _ in range(20):\n",
    "    # Push input sequences through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(X_sample)\n",
    "    # Set initial hidden stated of decoder as the last hidden state of the encoder\n",
    "    decoder_hidden = encoder_hidden\n",
    "    # Push target sequences and encoder output through the decoder to get the final loss for that batch\n",
    "    loss = decoder(Y_sample, decoder_hidden, encoder_outputs)\n",
    "    # Perform PyTorch magic!\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(encoder.parameters(), 0.5)\n",
    "    torch.nn.utils.clip_grad_norm_(decoder.parameters(), 0.5)\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    # Print the loss\n",
    "    print(loss.item() / len(X_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5971570f-80ba-422b-a852-96e5ebc7a4fc",
   "metadata": {},
   "source": [
    "You should see the loss going down after each iteration.\n",
    "\n",
    "#### Training the Combined Model\n",
    "\n",
    "Since we always train the encoder and decoder together, we can wrap the `Encoder` and `Decoder` class into a single class, here called `RnnAttentionSeq2Seq`. Again this is not mandatory but a good practice as it makes its later training and use more convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7871a732-6b8f-4158-bc0b-c325b352dfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = RnnAttentionSeq2Seq(params, nn.CrossEntropyLoss()).to(device)\n",
    "# Define optimizers for encoder and decoder\n",
    "encoder_optimizer = optim.Adam(model.encoder.parameters(), lr=0.0005)\n",
    "decoder_optimizer = optim.Adam(model.decoder.parameters(), lr=0.0005)\n",
    "# Print model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f80608-6c32-4d5c-bbe6-420e072e61f1",
   "metadata": {},
   "source": [
    "Let's also write an auxiliary method to train a single batch. Again, this just keeps our code cleaner and easier to maintain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07746fe7-10d0-49a2-ad7b-be46383fd642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(model, encoder_optimizer, decoder_optimizer, X, Y):\n",
    "    batch_size, num_steps = X.shape\n",
    "\n",
    "    loss = model(X, Y)\n",
    "    \n",
    "    # Backpropagation\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.encoder.parameters(), model.encoder.params.clip)\n",
    "    torch.nn.utils.clip_grad_norm_(model.decoder.parameters(), model.decoder.params.clip)\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / (num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25348597-f575-4a73-aa4e-89af10836244",
   "metadata": {},
   "source": [
    "Let's train our model again for 20 epochs by calling the method `train_batch()` in each iteration. Of course, our toy dataset has only a single batch, see we always call the method with the same input and target sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee32b98c-dd5d-44ac-8108-f883dfb248c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(20):\n",
    "    loss = train_batch(model, encoder_optimizer, decoder_optimizer, X_sample, Y_sample)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d57918-c683-472e-8526-7624eaae184d",
   "metadata": {},
   "source": [
    "### Auxiliary Method with Proper Batch Handling\n",
    "\n",
    "With the toy dataset, we didn't really care about the sequence values. However, if we want to properly train a model, we have to properly prepare each batch before calling the method `train_batch()`. This mainly means that we need to append the `<EOS>` token to each of the input and target sequences. We perform these steps in the method `train()` below that performs the complete training of our model. As you have seen in previous notebooks, this method essentially iterates over all batches and calls `train_batch` for each batch. The outer loop reflects how many epochs we want to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfae1c50-0cbe-4c98-9785-9d8eb3a3a3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, encoder_optimizer, decoder_optimizer, num_epochs, verbose=False):\n",
    "    # Set model to \"train\" mode\n",
    "    model.train()\n",
    "    \n",
    "    print(\"Total Training Time (total number of epochs: {})\".format(num_epochs))\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "\n",
    "        # Initialize epoch loss (cummulative loss fo all batchs)\n",
    "        epoch_loss = 0.0\n",
    "    \n",
    "        with tqdm(total=len(loader)) as progress_bar:\n",
    "        \n",
    "            for X_batch, Y_batch in loader:\n",
    "                batch_size, seq_len = X_batch.shape[0], X_batch.shape[1]\n",
    "\n",
    "                # Add EOS token to all sequences in that batch\n",
    "                eos = torch.LongTensor([model.encoder.params.special_token_eos]*batch_size)\n",
    "                X_batch = torch.cat((X_batch, eos.reshape(-1, 1)), axis=1)\n",
    "                Y_batch = torch.cat((Y_batch, eos.reshape(-1, 1)), axis=1)                \n",
    "                \n",
    "                # Move the batch to the correct device\n",
    "                X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
    "\n",
    "                # Train batch and get batch loss\n",
    "                batch_loss = train_batch(model, encoder_optimizer, decoder_optimizer, X_batch, Y_batch)\n",
    "\n",
    "                # Update epoch loss given als batch loss\n",
    "                epoch_loss += batch_loss\n",
    "\n",
    "                # Update progress bar\n",
    "                progress_bar.update(batch_size)\n",
    "                \n",
    "        if verbose is True:\n",
    "            print(\"Loss:\\t{:.3f} (epoch {})\".format(epoch_loss, epoch))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82c9c4e-f500-4b2c-ad37-60ad176e2d94",
   "metadata": {},
   "source": [
    "### Create `Dataset`, `Sampler` & `DataLoader`\n",
    "\n",
    "We already know that we can only give batches to an RNN that have sequences of the same lengths. Here, having 2 RNNs -- the encoder and the decoder -- we now additionally have to ensure that all sequence pairs in a batch have the same lengths. For example, for our toy batch all input sequences `X_sample` have the length 4 and all target sequences have `Y_sample` the lengths 5. This is a valid input for our encoder-decoder network.\n",
    "\n",
    "Of course, real-world sentences can have very different lengths and we need to address this. While concepts such as padding are a common alternative, here we again follow the approach of organizing our batches in a smart way so that the length requirements are always met -- that is, only sentence pairs with the same lengths are put into the same batch. The custom class `EqualLengthsBatchSampler` we already used earlier can also handle pairs of sequences.\n",
    "\n",
    "The code cell below shows the main steps to create a `DataLoader` you should already be familiar with. Once we iterate over the data loader, we will only get batches where all sample pairs have matching sequence lengths. Of course, this might mean that some batches won't be full in the case there are less than `batch_size` sentence pairs with matching sequence length. But this is less and less likely to occur for an increasing amount of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5d7568-47f7-45af-b1a3-62d352a018c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample = torch.tensor([[4, 5, 6, 7], [4, 5, 6, 7], [4, 5, 6, 7]])\n",
    "Y_sample = torch.tensor([[5, 6, 7, 8, 9], [5, 6, 7, 8, 9], [5, 6, 7, 8, 9]])\n",
    "\n",
    "batch_size = 32\n",
    "dataset_sample = BaseDataset(X_sample, Y_sample)\n",
    "sampler_sample = EqualLengthsBatchSampler(batch_size, X_sample, Y_sample)\n",
    "loader_sample = DataLoader(dataset_sample, batch_sampler=sampler_sample, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf41d244-729c-4da7-9da8-40c391b76cc8",
   "metadata": {},
   "source": [
    "### Complete Training Example\n",
    "\n",
    "Now we have everything in place to properly train our RNN-based encoder-decoder network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a0ac40-e163-404d-9646-edb8a8a17a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = RnnAttentionSeq2Seq(params, nn.CrossEntropyLoss()).to(device)\n",
    "# Define optimizers for encoder and decoder\n",
    "encoder_optimizer = optim.Adam(model.encoder.parameters(), lr=0.0005)\n",
    "decoder_optimizer = optim.Adam(model.decoder.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac62f9e2-3473-4ba9-9964-f97156487f28",
   "metadata": {},
   "source": [
    "Well, let's train our model again for 20 epochs over the toy dataset. The resulting losses should roughly match the ones from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbb250f-4625-421c-ab2f-b79c600efa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "train(model, loader_sample, encoder_optimizer, decoder_optimizer, num_epochs, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eed849-8591-4b34-a8e4-7c5245a03145",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3421954c-a41d-447e-a43e-4b74dec20275",
   "metadata": {},
   "source": [
    "## Building & Testing the Model using Real-World Dataset\n",
    "\n",
    "Using the toy dataset made it easier to better understand the RNN-based encoder-decoder architecture used in this notebook. We also implemented several auxiliary methods to handle the actual training of the model. This means that we are now ready to actually train a model using the Tatoeba dataset we prepared in the previous notebook.\n",
    "\n",
    "### Load & Prepare Dataset\n",
    "\n",
    "#### Load Vocabularies\n",
    "\n",
    "In the Data Preparation notebook, we already preprocessed and vectorized our corpus of sentence pairs, and saved the resulting vocabularies and dataset into files. We essentially only need to load these generated files. Let's start with the two vocabularies. Recall, the `vocab_deu` and `vocab_eng` are `vocab` objects from the `torchtext` package, allowing us to map words/tokens to their unique integer identifiers and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991e04db-e67a-446a-86fb-986a0167c909",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_deu = torch.load('data/corpora/tatoeba/tatoeba-deu-20000.vocab')\n",
    "vocab_eng = torch.load('data/corpora/tatoeba/tatoeba-eng-20000.vocab')\n",
    "\n",
    "vocab_size_eng = len(vocab_eng)\n",
    "vocab_size_deu = len(vocab_deu)\n",
    "\n",
    "print(\"Size of ENG vocabulary:\\t{}\".format(vocab_size_eng))\n",
    "print(\"Size of DEU vocabulary:\\t{}\".format(vocab_size_deu))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4aef40-7a35-4a55-8197-1e5085f1755b",
   "metadata": {},
   "source": [
    "#### Load Vectorized Sequence Pairs\n",
    "\n",
    "Now we can load the vectorized sentence pairs. Each line contains 2 tab-separated sequences: one for the input sequence (German sentence), one for the target sequence (English sentence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef6987c-3c27-43dd-a277-f1ae8550be67",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "\n",
    "num_samples = get_line_count('data/corpora/tatoeba/tatoeba-deu-eng-vectorized.txt')\n",
    "\n",
    "with open('data/corpora/tatoeba/tatoeba-deu-eng-vectorized.txt') as file:\n",
    "    with tqdm(total=num_samples) as progress_bar:\n",
    "        for line in file:\n",
    "            deu, eng = line.split('\\t')\n",
    "            # Convert name to a sequence of integers\n",
    "            seq_deu = [ int(index) for index in deu.split() ]\n",
    "            seq_eng = [ int(index) for index in eng.split() ]\n",
    "            # Add (sequence,label) pair to list of samples\n",
    "            samples.append((seq_deu, seq_eng))\n",
    "            # Update progress bar\n",
    "            progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33ba6e3-26e9-49de-93bf-2dc9b5ac0ded",
   "metadata": {},
   "source": [
    "#### Convert Sequence Pairs to List of Input & Target Tensors\n",
    "\n",
    "For the training, we need to split the list `samples` of sentence pairs into the inputs `X` and target `Y`. During this step, we also convert the individual sequences into tensors. Note that `X` and `Y` won't be tensors themselves as the sequences have different lengths. In the last line, we set `samples` to `None`. Since we don't need this list any longer, we can free some memory by setting the variable to `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8e0878-15dc-4a82-8bb9-f2eeb1a0193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_samples = 99999999999\n",
    "\n",
    "X = [ torch.LongTensor(deu) for (deu, _) in samples[:max_num_samples] ]\n",
    "Y = [ torch.LongTensor(eng) for (_, eng) in samples[:max_num_samples] ]\n",
    "\n",
    "# Free up some memory\n",
    "samples = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8be4c9-212f-40d0-b7bd-a6cb842b42b0",
   "metadata": {},
   "source": [
    "#### Split Data into Training & Test Set\n",
    "\n",
    "Using the familiar method `train_test_split()`, we split our input and target sequences into training and test sets using a 90:10 split -- that is, 90% of the data will be used for training. Note that it does not really matter here since we do not really evaluate our model and a proper quantitative manner; see a discussion at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ded1ed9-77d5-478a-88ae-b263934fe5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, shuffle=True, random_state=0)\n",
    "\n",
    "print(\"Number of training samples:\\t{}\".format(len(X_train)))\n",
    "print(\"Number of test samples:\\t\\t{}\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a7bb16-a3d8-44a2-977c-8947690cd399",
   "metadata": {},
   "source": [
    "#### Create `Dataset`, `Sampler` & `DataLoader` for Source and Target Language\n",
    "\n",
    "Like above, we need to create our data loaders to ensure that each batch will have only sentence pairs of matching lengths. We define the loader for the test set with a batch size of 1 since we only look at individual test sentences when \"manually\" assessing the quality of the translations done by our model. We set the batch size for the training loader to 512, but you can play with different values if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ac8b2f-ee86-4419-9072-6992dd8fae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "dataset_train = BaseDataset(X_train, Y_train)\n",
    "sampler_train = EqualLengthsBatchSampler(batch_size, X_train, Y_train)\n",
    "loader_train = DataLoader(dataset_train, batch_sampler=sampler_train, shuffle=False, drop_last=False)\n",
    "\n",
    "dataset_test = BaseDataset(X_test, Y_test)\n",
    "sampler_test = EqualLengthsBatchSampler(1, X_test, Y_test)\n",
    "loader_test = DataLoader(dataset_test, batch_sampler=sampler_test, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1375d9-f499-4486-945f-0247e5b2de71",
   "metadata": {},
   "source": [
    "### Training a Machine Translation Model\n",
    "\n",
    "#### Create Model\n",
    "\n",
    "We first need to create the model. In contrast to the example above, here many parameter values are determined by the vocabularies. But in principle, the steps are exactly the same as seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a62d779-2516-4f97-972f-e3f99b9dd1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"device\": device,                            # as the decoder also generates sentence it mus be able to move the data to the correct device\n",
    "    \"vocab_size_encoder\": vocab_size_deu,        # the size of the source vocabulary determines the input size of the encoder embedding\n",
    "    \"vocab_size_decoder\": vocab_size_eng,        # the size of the target vocabulary determines the input size of the decoder embedding\n",
    "    \"embed_size\": 300,                           # size of the word embeddings (here the same for encoder and decoder; but not mandatory)\n",
    "    \"rnn_cell\": \"LSTM\",                          # in practice GRU or LSTM will always outperform RNN\n",
    "    \"rnn_hidden_size\": 512,                      # size of the hidden state\n",
    "    \"rnn_num_layers\": 2,                         # 1 or 2 layers are most common; more rarely sees any benefit\n",
    "    \"rnn_dropout\": 0.2,                          # only relevant if rnn_num_layers > 1\n",
    "    \"rnn_encoder_bidirectional\": True,           # The encoder can be bidirectional; the decoder can not\n",
    "    \"linear_hidden_sizes\": [1024, 2048],         # list of sizes of subsequent hidden layers; can be [] (empty); only relevant for the decoder\n",
    "    \"linear_dropout\": 0.2,                       # if hidden linear layers are used, we can also include Dropout; only relevant for the decoder\n",
    "    \"attention\": \"DOT\",                          # Specify if attention should be used; only \"DOT\" supported; None if no attention\n",
    "    \"teacher_forcing_prob\": 0.5,                 # Probability of using Teacher Forcing during training by the decoder\n",
    "    \"special_token_unk\": vocab_eng['<UNK>'],     # Index of special token <UNK>\n",
    "    \"special_token_sos\": vocab_eng['<SOS>'],     # Index of special token <SOS>\n",
    "    \"special_token_eos\": vocab_eng['<EOS>'],     # Index of special token <EOS>\n",
    "    \"clip\": 1.0                                  # Clipping value to limit gradients to prevent exploding gradients\n",
    "}\n",
    "\n",
    "params = Dict2Class(params)\n",
    "# Create model (incl. the definition of the loss function)\n",
    "model = RnnAttentionSeq2Seq(params, nn.CrossEntropyLoss()).to(device)\n",
    "# Define optimizers (for encoder and decoder)\n",
    "encoder_optimizer = optim.Adam(model.encoder.parameters(), lr=0.0005)\n",
    "decoder_optimizer = optim.Adam(model.decoder.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa6079-a25b-419a-934b-5551b791c92c",
   "metadata": {},
   "source": [
    "#### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be02094c-a2f7-4220-b51a-8d790a98fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 0\n",
    "\n",
    "train(model, loader_train, encoder_optimizer, decoder_optimizer, num_epochs, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5df313-901f-4822-b536-1398db603f2a",
   "metadata": {},
   "source": [
    "#### Save/Load Model\n",
    "\n",
    "As retraining the model all the time can be tedious, we can save and load our model. If you trained the model for the first time, only saving is of course possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a630e0bf-7bfd-4374-9c6c-69c54109c03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#action = \"save\"\n",
    "action = \"load\"\n",
    "#action = \"none\"\n",
    "\n",
    "if action == \"save\":\n",
    "    torch.save(model.state_dict(), 'data/models/translation/tatoeba-deu-eng-rnn.pt')\n",
    "elif action == 'load':\n",
    "    model = RnnAttentionSeq2Seq(params, nn.CrossEntropyLoss()).to(device)\n",
    "    model.load_state_dict(torch.load('data/models/translation/tatoeba-deu-eng-rnn.pt'))\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30283eec-9647-4628-b7d6-cf37d7f07256",
   "metadata": {},
   "source": [
    "### Testing the Model\n",
    "\n",
    "#### Auxiliary Method\n",
    "\n",
    "We first define a method `translate()` that takes an input sequence, and uses the model to return the translated sequence. Note that it calls the method `generate()` of the `Decoder` class. If you have a look at the code, you will notice that it is very similar to the `generate` method of the `RnnLanguageModel` we have seen before. After all, the decoder acts like a language model for the target language (here: English), so the similarities between the 2 methods is no accident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cee6f22-c32f-45e4-bc70-67dcdc1a041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, inputs, max_len=100):\n",
    "    # Encode input sequence/sentence\n",
    "    encoder_outputs, encoder_hidden = model.encoder(inputs)\n",
    "    # Translate input but generating/predicting the output sequence/sentence\n",
    "    decoded_indices, attention_weights = model.decoder.generate(encoder_hidden, encoder_outputs, max_len=max_len)\n",
    "    # Return the translation + the attention weights\n",
    "    return decoded_indices, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4516aa-1c00-4d80-8132-f36eaa5e72aa",
   "metadata": {},
   "source": [
    "#### Translating a Test Sentence\n",
    "\n",
    "Let's look at a single sentence from the test set. For this we perform 1 iteration over the test loader; recall that each test batch contains only a single sentence pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7469fe-e7d1-4e4b-af8f-9921cc30f259",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (inputs, targets) in enumerate(loader_test):\n",
    "    # The input is the first sequence\n",
    "    inputs = inputs[0:1].to(device)\n",
    "    # Decode input sequence of indices to sequences of word/tokens\n",
    "    src_labels = vocab_deu.lookup_tokens(inputs[0].cpu().numpy().tolist())\n",
    "       \n",
    "    # Translate input sequence into predicted target sequence\n",
    "    decoded_indices, attention_weights = translate(model, inputs)\n",
    "    \n",
    "    # Decode target sequence of indices to sequences of word/tokens\n",
    "    tgt_labels = vocab_eng.lookup_tokens(decoded_indices)\n",
    "    \n",
    "    # Print input and translation\n",
    "    print(' '.join(src_labels))\n",
    "    print()\n",
    "    print(' '.join(tgt_labels))\n",
    "    \n",
    "    # Break the loop; we only want to check a single batch with a single sentence\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82819b7d-ef0b-4261-9b02-dbdb6e2467d9",
   "metadata": {},
   "source": [
    "**Important:** The quality of the translation can vary widely depending on the parameter settings and how long you have trained the model. Keep in mind that any model that generates text is generally much more difficult to train than, say, a classification model. Well, strictly speaking, the decoder also just performs a classification but the class labels are represented by the whole vocabulary, which in our example is over 20k tokens/word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc7e9f0-02c8-498b-8554-06f5fc666986",
   "metadata": {},
   "source": [
    "#### Visualizing the Attention Weights\n",
    "\n",
    "Since the decoder also returns the attention weights, we also visualize them. We recommend to first run the code cell above until you get an example translation that is reasonably good; this is more likely to be the case for short sentences. Once you have a good example, you can run the code cell below to visualize the attention weights. For this we provide the utility method `plot_attention_weights()` in the file `src/utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23c7212-16f5-4c72-8d79-322782453ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = attention_weights.detach().cpu().numpy()\n",
    "\n",
    "print(src_labels)\n",
    "print(tgt_labels)\n",
    "\n",
    "plot_attention_weights(weights, src_labels, tgt_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc83dd3-d04e-499b-bb1f-419261ccc32a",
   "metadata": {},
   "source": [
    "In the plot above, the x axis represents the tokens/words of the input sequence (i.e., the German sentence), and the y axis represents the tokens/words of the target sequence (i.e., the German sentence). A high attention weight means that the prediction of the specific token/word in the target sequence was mostly influenced by that specific token/word in the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4378c27e-1b44-4f9c-a2f9-b9ddb38e0f1e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a8ba20-75d2-4013-9784-f22c40b40253",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "This notebook went through all the basic steps required to build an RNN-based machine translation model. Apart from using only a small dataset for training, we also we skipped any proper evaluation of our model, which is a very non-trivial task in itself. Machine translation models are evaluated by comparing their generated translations with reference translations, typically done on a separate set of data called the evaluation or test set. The evaluation process measures the quality of the translations and assesses how well the model performs in terms of accuracy, fluency, and overall translation adequacy. Common metrics used for evaluating machine translation models include:\n",
    "\n",
    "* **BLEU (Bilingual Evaluation Understudy):** BLEU is a widely used metric for machine translation evaluation. It measures the overlap between the model's generated translations and the reference translations based on n-grams (contiguous sequences of words). BLEU computes a precision score by comparing n-gram matches and penalizes overly long translations. Higher BLEU scores indicate better translation quality.\n",
    "\n",
    "* **METEOR (Metric for Evaluation of Translation with Explicit ORdering):** METEOR is another popular metric that considers both precision and recall by matching unigrams (individual words) between the generated and reference translations. It also incorporates additional features such as stemming, synonym matching, and word order information. METEOR provides a score that correlates with human judgment of translation quality.\n",
    "\n",
    "* **TER (Translation Edit Rate):** TER calculates the minimum number of edit operations (such as insertions, deletions, substitutions, and shifts) required to transform the generated translation into the reference translation. Lower TER scores indicate better translation quality.\n",
    "\n",
    "* **ROUGE (Recall-Oriented Understudy for Gisting Evaluation):** ROUGE is primarily used for evaluating text summarization tasks, but it can also be adapted for machine translation evaluation. It measures the overlap of n-grams and word sequences between the generated and reference translations. ROUGE scores are commonly used in research papers to compare machine translation models.\n",
    "\n",
    "* **NIST (NIST BLEU):** NIST is an extension of the BLEU metric that incorporates features like document-level statistics and different n-gram weighting schemes. It aims to align the metric with human evaluation and correlation.\n",
    "\n",
    "* **Human Evaluation:* While automatic metrics provide a quick and objective assessment, human evaluation is crucial to gauge translation quality accurately. Human evaluators assess the translations based on fluency, adequacy, grammaticality, and overall meaning. They can also provide insights into other aspects of translation, such as domain-specific accuracy or stylistic appropriateness.\n",
    "\n",
    "It's important to note that automatic metrics like BLEU, METEOR, TER, and ROUGE have limitations and may not perfectly align with human judgments. They are used as rough indicators and for comparing different models or approaches. Human evaluation remains an essential component to capture the nuances and quality aspects that automatic metrics may not fully capture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9326516-7fc2-4f86-9e90-a6935a73c5b5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a1b58c-fe86-4f9f-b832-4e81405b2e84",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Machine translation can be effectively addressed using recurrent neural networks (RNNs) in natural language processing (NLP). RNNs are a class of neural networks designed to handle sequential data, making them well-suited for tasks like language translation. The key idea is to use an encoder-decoder architecture, where the input sentence is encoded into a fixed-length representation and then decoded into the target language. In this setup, the RNN encoder processes the source sentence word by word, capturing the contextual information and producing a meaningful representation known as the context vector. The context vector contains a condensed representation of the input sentence's semantic and syntactic information.\n",
    "\n",
    "The RNN decoder takes the context vector as its initial input and generates the output translation one word at a time. At each time step, the decoder RNN uses the previously generated word and its own internal state to predict the next word in the target language. The process continues until the decoder generates an end-of-sentence token or reaches a predefined length. During training, the model is optimized to minimize the discrepancy between the generated translations and the reference translations. This is done by using techniques like teacher forcing, where the model is fed with the true output tokens during training to guide its learning.\n",
    "\n",
    "The power of RNNs lies in their ability to capture the sequential dependencies and handle variable-length input and output sequences. The recurrent connections within the RNN enable the model to retain and utilize information from previous time steps. This makes RNN-based machine translation models capable of handling both short and long sentences while preserving the syntactic and semantic coherence in the translations. Overall, using recurrent neural networks for machine translation in NLP provides a flexible and effective framework to handle the complexities of language translation, allowing for improved translation quality and better preservation of the meaning and structure across different languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dc1c62-a55a-4979-9906-3103439983ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs5246",
   "language": "python",
   "name": "cs5246"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
