{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1faabd45-d04f-4257-8b1e-64a425a44365",
   "metadata": {},
   "source": [
    "<img src='data/images/section-notebook-header.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6cf7a1-f088-4136-93db-9c8db095468f",
   "metadata": {},
   "source": [
    "# Word2Vec: Skip-gram\n",
    "\n",
    "Skip-gram is a model for training word embeddings in the Word2Vec framework. Unlike the CBOW model, which predicts a target word based on the context words, the Skip-gram model predicts context words given a target word. In the Skip-gram model, the training process involves sliding a window over a text corpus just like CBOW. However, instead of predicting the target word, the model predicts the surrounding context words. The objective is to maximize the probability of predicting the correct context words given the target word. The image below is taken from the lecture slides showing the basic setup and intuition behind Skip-gram.\n",
    "\n",
    "<img src='data/images/lecture-slide-10.png' width='80%' />\n",
    "\n",
    "This example assumes a window size of 2. This means that given a center word (here: *\"movies\"*), we want to predict 4 context words, 2 before and 2 after the center word. On the right, the image shows some example words, with the color indicating which words are intuitively the most likely context words (green = high probability; red = low probability). Of course, the actual most likely words will depend on the training data; here it is only about the basic intuition behind Skip-gram.\n",
    "\n",
    "In this notebook, we will train a Skip-gram model from scratch. Since we already prepared the data in the accompanying notebook, there's actually not much more to do. We implement and train this model using PyTorch. The model should train with or without a GPU, although having a GPU significantly speeds up the process. However, here we don't care too much about accuracy but the basic idea behind Skip-gram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7d3515",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d88581-6d75-4591-8c33-49c234ab4d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd40661f-ccc2-4a04-8a9e-9352568c9bb1",
   "metadata": {},
   "source": [
    "We utilize some utility methods from PyTorch, so we need to import the `torch` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f76a1bf-1fad-4a0f-92af-4281f8aa3b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ce6f4a-80af-4a41-9ece-b5ca25a014b1",
   "metadata": {},
   "source": [
    "We also need the PyTorch implementation of the Skip-gram model. While the code is very short, having the implementation in separate files makes it easier to re-use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fa5a74-bf39-41d3-b79a-cc72af1e5da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.word2vec import Skipgram\n",
    "from src.utils import tsne_plot_similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db320f4",
   "metadata": {},
   "source": [
    "### Checking/Setting the Computation Device\n",
    "\n",
    "PyTorch allows to train neural networks on supported GPUs to significantly speed up the training process. If you have a support GPU, feel free to utilize it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fc6a95-5ac0-4f95-a006-0df66791cf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Use this line below to enforce the use of the CPU \n",
    "#use_cuda = False\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "print(\"Available device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af44e86-9f73-4177-b5e4-18f8e4f4b8c0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803abe88-272f-46c7-8a3f-2cfe65c3d603",
   "metadata": {},
   "source": [
    "## Load all the Data\n",
    "\n",
    "### Load Vocabulary\n",
    "\n",
    "In the Data Preprocessing notebook we created the vocabulary to map from words to their indices, and vice versa. We naturally need this vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07ce4b6-b138-4d64-97eb-8130ecb9901d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = torch.load('data/corpora/imdb-reviews/vectorized-word2vec/imdb-word2vec-20000.vocab')\n",
    "\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "print('Size of vocabulary:\\t{}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40a6080",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "\n",
    "Of course, we need the training data. Depending on your size $m$ for the context (cf. Data Preprocessing notebook), there are $2m$ (context_word, center_word)-pairs for each center word and associated contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b18888-adf1-42e5-95c2-11a7789834d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('data/corpora/imdb-reviews/vectorized-word2vec/imdb-dataset-skipgram.npy')\n",
    "\n",
    "num_samples, num_indices = data.shape\n",
    "\n",
    "print('Number of samples: {}'.format(num_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b82350",
   "metadata": {},
   "source": [
    "### Split Dataset into Inputs & Targets\n",
    "\n",
    "The input features `X` are the context word indices, and the targets are the center word indices. We also directly convert the Numpy arrays into PyTorch tensors to serve as input for the Skip-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d1a831-8d92-48f0-a9c0-281bb3c82cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.Tensor(data[:,0]).long()\n",
    "y = torch.Tensor(data[:,-1]).long()\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9cafd4",
   "metadata": {},
   "source": [
    "### Create `Dataset` and `DataLoader`\n",
    "\n",
    "PyTorch comes with different `Dataset` classes and a `DataLoader` class that make working with batches of different sizes very easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1365b5f-f50e-4e7e-95b6-2e20d0aabb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cf1129",
   "metadata": {},
   "source": [
    "## Create and Train Skip-gram Model\n",
    "\n",
    "### Create Model\n",
    "\n",
    "Like CBOW, the Skip-gram model uses a shallow neural network with a single hidden layer. The input layer represents the target word, and the output layer represents the context words. The hidden layer acts as a continuous vector representation of the target word and is known as the word embedding layer. The word embeddings are learned by updating the weights of the neural network using backpropagation and gradient descent. The figure below is taken from the lecture slides to visualize the basic shallow architecture of CBOW by means of an example input and output.\n",
    "\n",
    "<img src='data/images/lecture-slide-11.png' width='80%' />\n",
    "\n",
    "\n",
    "In general, the Skip-gram model can be computationally more expensive compared to CBOW since it needs to predict multiple context words for each target word. It also requires a larger amount of training data to achieve robust word embeddings.\n",
    "\n",
    "The code for the Skip-gram model can be found in `src/skipgram.py`. Have a look how simple the model looks. It directly implements the model they way we introduced in the lectures -- with some very minor tweaks to improve the training. As size for the word embeddings, we go with 300 by default -- feel free to change it -- as it is the common embedding size of pretrained Word2Vec models you can download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0829a48a-c7f4-475b-96cb-61f373e466c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 300\n",
    "\n",
    "# Create model\n",
    "model = Skipgram(vocab_size, embed_dim)\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Move th model to GPU, if available (by default it \"stays\" on the CPU)\n",
    "model.to(device)\n",
    "# Print model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f268e43",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "\n",
    "The code cell below shows the most basic structure for training a model -- basically identical to the one for CBOW. The outer loop determines how many epochs we want to train. An epoch describes the processing of all data samples in the dataset. For each epoch, we then loop over the dataset in the form of batches (this is where the `DataLoader` comes in handy). Instead of (Mini-Batch) Gradient Descent, we use the more sophisticated `Adam` optimizer, but feel free to change it to Gradient Descent which PyTorch also provides. The loss function -- often called \"criterion\" in Pytorch lingo -- is the Cross-Entropy Loss. Note that the model has **no Softmax layer**, as this is handled by the `CrossEntropyLoss` class of Pytorch. There is nothing special about it, but it does save 1 or 2 lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2052c879-c912-4e48-9090-6252ff59caf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for idx, (x, y) in enumerate(tqdm(dataloader)):\n",
    "        # Move current batch to GPU, if available\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # Calculate output of the model\n",
    "        logits = model(x)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(logits, y)\n",
    "        \n",
    "        # Reset the gradients from previous iteration\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Calculate new Gradients using backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        #nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        \n",
    "        # Update all trainable parameters (i.e., the theta values of the model)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Keep track of the overall loss of the epoch\n",
    "        epoch_loss += loss.item()\n",
    "            \n",
    "    print('[Epoch {}] Loss: {}'.format((epoch+1), epoch_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5129856",
   "metadata": {},
   "source": [
    "### Save/Load Model\n",
    "\n",
    "As retraining the model all the time can be tedious, we can save and load our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3076e7-1ae9-40dd-91d4-68f366a82a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 'save'\n",
    "action = 'load'\n",
    "#action = 'none'\n",
    "\n",
    "if action == 'save':\n",
    "    torch.save(model.state_dict(), 'data/models/word2vec/model-skipgram.pt')\n",
    "elif action == 'load':\n",
    "    model = Skipgram(vocab_size, embed_dim)\n",
    "    model.to(device)\n",
    "    model.load_state_dict(torch.load('data/models/word2vec/model-skipgram.pt'))\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a79ec1-2684-4907-8077-a1015959d658",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee2bcde-ecf8-46e2-800d-04d209e11702",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "The following code is purely to visualize the results. Of course, depending on how much of the training data you used and how long you have trained your model, the resulting plots might differ greatly from the ones in the lecture slides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8253ee",
   "metadata": {},
   "source": [
    "### Auxiliary Method\n",
    "\n",
    "The method `get_most_similar()` below returns for a given word the k-most similar words w.r.t. the word embeddings. Note that we only use matrix `U` for the word embeddings, and completely ignore matrix `V`, just to keep it simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822a3009-5ff8-4cef-9c79-761ef00da63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar(word, k=5):\n",
    "    # Get the index for the input word\n",
    "    idx = vocabulary.lookup_indices([word])[0]\n",
    "    # Get the word vector of the input word\n",
    "    reference = model.U.weight[idx]\n",
    "    # Calculate all pairwise similarites between the input word vector and all other word vectors\n",
    "    dist = F.cosine_similarity(model.U.weight, reference)\n",
    "    # Sort the distances and return the top-k word vectors that are most similar to the input word vector\n",
    "    # Note that the top-k contains the input word vector itself, which is fine here\n",
    "    index_sorted = torch.argsort(dist, descending=True)\n",
    "    indices = index_sorted[:k]\n",
    "    # Convert the top-k nearest word vectors into their corresponding words\n",
    "    return [ vocabulary.lookup_token(n.item()) for n in indices ]    \n",
    "    \n",
    "#Example\n",
    "get_most_similar('dvd')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ef4f2e",
   "metadata": {},
   "source": [
    "### Visualization of Results\n",
    "\n",
    "We start by creating a list of seed words. For each seed word, we will get the top-k nearest words and later show them together into a 2d plot (see below). Feel free to change the list of seed words. Just note that each seed word and its resulting cluster will be assigned its unique color. So the more seed words you use, the less distinctive will be some of the colors in the final plot. You might also want to ensure that the seed words themselves are not semantically very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4dfd25-022b-4f38-8760-43ae49824661",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_words = ['movie', 'actor', 'scene', 'music', 'dvd', 'story', 'horror', 'funny', 'laugh']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08db2ef3",
   "metadata": {},
   "source": [
    "#### Create Word Embedding Clusters\n",
    "\n",
    "Here, a cluster is simply the seed word and all its top-k nearest words. This helps us later to plot each cluster in a different color later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf78c8b-081c-4677-93ed-06cb157a9cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {}\n",
    "\n",
    "embedding_clusters = []\n",
    "word_clusters = []\n",
    "\n",
    "for word in seed_words:\n",
    "    embeddings = []\n",
    "    words = []\n",
    "    for neighbor in get_most_similar(word):\n",
    "        words.append(neighbor)\n",
    "        embeddings.append(model.U.weight[vocabulary.lookup_indices([neighbor])[0]].detach().cpu().numpy())\n",
    "    embedding_clusters.append(embeddings)\n",
    "    word_clusters.append(words)\n",
    "    \n",
    "embedding_clusters = np.array(embedding_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308b0b96",
   "metadata": {},
   "source": [
    "#### Dimensionality Reduction\n",
    "\n",
    "Our word embeddings are of size 300 (by default). This makes plotting them a bit tricky :). We therefore use a dimensionality reduction technique called T-SNE to map the word embeddings from the 300d space to a 2d space. A deeper discussion of T-SNE is beyond our scope here, but feel free to explore yourself how T-SNE works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409e7f6e-94f2-426b-af90-fea678df3d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, m, k = embedding_clusters.shape\n",
    "\n",
    "tsne_model_en_2d = TSNE(perplexity=15, n_components=2, n_iter=3500, random_state=32)\n",
    "embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)\n",
    "\n",
    "#pca_model_en_2d = PCA(n_components=2).fit(embedding_clusters.reshape(n * m, k))\n",
    "#embeddings_en_2d = np.array(pca_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efea270c",
   "metadata": {},
   "source": [
    "#### Plot Results\n",
    "\n",
    "Lastly, the method `tsne_plot_similar_words()` implemented in the file `src.utils` plots our cluster of word embeddings that are now all in the 2d space. Again, the results very much depend on how much training data you used and how long you trained the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de23a6fa-aa29-4add-adf7-3cc7234c457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot_similar_words('', seed_words, embeddings_en_2d, word_clusters, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0d50d9-e483-4c8b-bc77-99548f82ac05",
   "metadata": {},
   "source": [
    "Assuming you have trained over the complete dataset for at least 10 epochs, the plot above should intuitive results where words with embeddings of the same color are indeed semantically related (e.g., the region/cluster containing the words *\"music\"*, *\"tune\"*, *\"soundtrack\"*, etc.). In general, the longer the training, the better the results, but even a few epochs should suffice here to get meaningful word embeddings.\n",
    "\n",
    "However, having used the same corpus for training the CBOW and Skip-gram model, and assuming the same number of epochs during training, the CBOW result tends to look a bit more intuitive -- although it's difficult to quantify. This is due to the comment stated at the beginning saying that Skip-gram generally requires more data and/or longer training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54661f66-39c2-410f-837e-c7b283b0d7a8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142939e5-2f95-4795-9b11-bbaa2b5f6977",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "The Skip-gram model is a popular word embedding technique used in NLP. It aims to learn continuous vector representations of words by predicting the context words given a target word. In this approach, a sliding window is used to create training instances from a text corpus, where the target word is the input and the context words are the output. The Skip-gram model utilizes a shallow neural network with a single hidden layer. The input layer represents the target word, and the output layer represents the context words. The hidden layer serves as the word embedding layer, capturing the semantic relationships between words. By learning the co-occurrence patterns of words, the Skip-gram model can identify words with similar meanings or those that frequently appear together.\n",
    "\n",
    "One advantage of the Skip-gram model is its ability to handle rare words effectively. Since each word in the corpus is treated as a target word during training, even infrequent words can have meaningful word embeddings. Additionally, the Skip-gram model can capture fine-grained nuances and semantic relationships, making it suitable for tasks such as word similarity and analogy detection. However, the Skip-gram model can be computationally expensive and requires a large amount of training data to achieve robust word embeddings. Training the model involves predicting multiple context words for each target word, which can increase the complexity. Despite these challenges, the Skip-gram model remains widely used and has demonstrated success in various NLP applications, including language modeling, information retrieval, and machine translation.\n",
    "\n",
    "In summary, the Skip-gram model is a word embedding technique in NLP that predicts context words given a target word. It captures semantic relationships, handles rare words well, and enables fine-grained analysis of word meanings. While it can be computationally expensive and requires abundant training data, the Skip-gram model has proven valuable in numerous NLP tasks, showcasing its effectiveness and versatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13d3b33-6a43-4c25-bede-bf792093970c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs5246]",
   "language": "python",
   "name": "conda-env-cs5246-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
