{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6072022-e8a4-4b03-a542-b99bb26d7ed3",
   "metadata": {},
   "source": [
    "<img src='data/images/section-notebook-header.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2d52db-ac20-4da9-b9ae-8d99348b2152",
   "metadata": {},
   "source": [
    "# Text Summarization\n",
    "\n",
    "## Overview\n",
    "\n",
    "Text summarization refers to the task of condensing a given text into a shorter version while retaining its most important information. It is a process of distilling the key points, main ideas, and relevant details from a text document, whether it's an article, news story, research paper, or any other form of textual content. The goal of text summarization is to generate a concise and coherent summary that captures the essence of the original text, enabling users to get a quick overview or understanding of the content without having to read the entire document. Summarization can be extractive or abstractive in nature:\n",
    "\n",
    "* **Extractive Summarization:** In this approach, the summary is created by selecting and combining the most important sentences or phrases from the original text. The selected sentences are usually representative of the main ideas and are directly taken from the source document. Extractive summarization methods rely on statistical techniques, machine learning algorithms, or graph-based algorithms to identify the salient sentences for inclusion in the summary.\n",
    "\n",
    "* **Abstractive Summarization:** This approach involves generating a summary that may contain words, phrases, and even sentences that were not present in the original text. Abstractive summarization methods attempt to understand the meaning of the text and generate a summary using natural language generation techniques. They rely on deep learning models, such as recurrent neural networks (RNNs), transformers, or other sequence-to-sequence models, to generate coherent and contextually relevant summaries.\n",
    "\n",
    "Text summarization has various applications, including:\n",
    "\n",
    "* News summarization: Generating brief summaries of news articles or headlines.\n",
    "* Document summarization: Condensing lengthy documents or research papers for efficient reading or reviewing.\n",
    "* Email summarization: Providing concise overviews of long email threads.\n",
    "* Social media summarization: Extracting important information from social media posts or conversations.\n",
    "* Automatic summarization of legal documents: Summarizing legal contracts, court rulings, and other legal texts.\n",
    "* Summarization for content recommendation: Generating summaries for recommendation systems to provide previews or snippets of articles, blogs, or videos.\n",
    "\n",
    "Text summarization is a challenging task in NLP, requiring the model to understand the nuances of language, extract relevant information, and generate coherent and concise summaries. It is an active area of research with ongoing efforts to improve the quality and effectiveness of summarization algorithms.\n",
    "\n",
    "## (Most) Basic Algorithm for Text Summarization\n",
    "\n",
    "Since abstractive summarization requires the generation of new text, it has become the exclusive domain of Deep Learning methods. However, these models are difficult to train and require huge datasets and a lot of computing resources. Therefore, in practice, most users have to rely on pretrained models. Given the syllabus and focus of this course, this notebook focuses on very basic approaches. The goal is not to generate state-of-the-art summaries but to build intuition and experience about core NLP tasks. The figure below is taken from the lecture slides and shows a basic architecture for text summarization:\n",
    "\n",
    "<img src='data/images/ts-basic-architecture.png' width='90%' />\n",
    "\n",
    "As it can been seen in the previous figure, architecture is arguably basic since it makes the following simplifying assumptions:\n",
    "\n",
    "* **Extractive:** Summaries are generated as a subset of selected sentences; this includes that not new text is being generated\n",
    "\n",
    "* **Generic:** The summary is solely based on the input document and not dependent on any user inputs such as a query or a prompt.\n",
    "\n",
    "* **Single Document:** In input for the summarization is a single document about a single topic; in contrast to multiple documents about either the same or different topic.\n",
    "\n",
    "With our focus on simplicity here, the only real challenge we have to solve here is to select the best sentences we want to choose to form the final summary. There are, of course, many approaches to find the best sentences, essentially differing in the way these methods quantify the importance of a sentence. In this notebook, we make use of the topic we have already covered in the previous notebook about \"Keyword Extraction\". In a nutshell, the basic intuition is that a sentence is important if it has a high score with respect to keyword scores the sentence contains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1303561a-3cca-43b9-abd8-adb22949b891",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Import Required Packages\n",
    "\n",
    "Similar to the notebook about \"Keyword Extraction\" our context are news articles, so the `newspaper` package comes in handy. As a more or less arbitrary choice, we deploy Yake! to handle to keyword extraction and scoring step, naturally requiring the `yake` package again as well. We also use `spacy` for some basic data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92827c60-fed4-48a5-bc63-13ade9184c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import yake\n",
    "import re\n",
    "\n",
    "from newspaper import Article\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8f6456-a7ac-4f2e-939e-e38e543be4aa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d620c4d3-8fc2-4247-8bde-03f542811071",
   "metadata": {},
   "source": [
    "## Prepare Input Document\n",
    "\n",
    "### Fetch News Article\n",
    "\n",
    "The code cell below performs the same steps to fetch an online news article using the [`newspaper`](https://newspaper.readthedocs.io/en/latest/) package like in the \"Keyword Extraction\" notebook. So we skip any details here. At the end, the variable `text` will contain the raw text of the news article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f79b7d6-aea8-40f7-91a3-e76625aa9feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.channelnewsasia.com/singapore/built-order-bto-waterway-sunrise-ii-project-delays-exceed-one-year-complete-housing-development-board-hdb-compensation-reimbursement-3324526\"\n",
    "#url = \"https://www.channelnewsasia.com/singapore/focus-metaverse-another-fading-tech-fad-or-our-future-online-existence-3320981\"\n",
    "#url = \"https://www.channelnewsasia.com/singapore/tiong-bahru-road-tree-collapse-bus-service-car-crushed-walkway-shelter-3322491\"\n",
    "\n",
    "article = Article(url)\n",
    "article.download()\n",
    "article.parse()\n",
    "\n",
    "text = article.text\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce358ce-fe48-4e10-b1b0-ec6ec3a0343f",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "As the very first step, we split our input news article into a list of sentences. This is important since first we want to perform some preprocessing before the keyword extraction step, but want to add the original and not the processed sentences to our final summary. Thus, we use `sentences` to hold on to all original sentences until the very end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06d13f3-00a4-4260-b843-def5328eab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "sentences = [ str(s).strip() for s in doc.sents ]\n",
    "\n",
    "num_sentences = len(sentences)\n",
    "\n",
    "print('Number of sentences: {}'.format(num_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54be5a94-87a7-4b78-8140-beaa3949e852",
   "metadata": {},
   "source": [
    "As usual, we define ourselves a basic auxiliary method to preprocess all sentences. This makes it easy to go back and change the method to see how different preprocessing steps might affect the summarization results. Below, the method `preprocess()` basically just performs case-folding and some very simple cleaning steps. Since we use Yake! we actually do not want to perform steps like stopword removal. However, feel free to modify this method by applying additional preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307133e4-88fb-4d9d-98f5-2c761562a026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentences):\n",
    "    sentences_processed = []\n",
    "    # Iterate over each sentence and preprocess it\n",
    "    for s in sentences:\n",
    "        # Remove line breaks\n",
    "        processed = re.sub('\\n', ' ', s)\n",
    "        # Use spaCy for tokenzing (seems to be convenient)\n",
    "        processed = ' '.join([ t.text.lower() for t in nlp(processed) ])\n",
    "        # Remove duplicate whitespaces\n",
    "        processed = re.sub('\\s+', ' ', processed)\n",
    "        sentences_processed.append(processed)\n",
    "    return sentences_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a08917-eea0-44f2-b390-c69f4419127d",
   "metadata": {},
   "source": [
    "Now `sentences_processed` is a list containing all the processed sentences, which we can now combine to a new document which we then can use as the input for Yake!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74be374-bb76-462b-9451-c57a1c9a063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_processed = preprocess(sentences)\n",
    "\n",
    "text_processed = ' '.join(sentences_processed)\n",
    "\n",
    "print(text_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5073d0f-9763-454b-bb60-6bfb6adbb87d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeb08d5-5011-4378-88d5-3894b1164a5b",
   "metadata": {},
   "source": [
    "## News Article Summarization\n",
    "\n",
    "We can organize our simple approach for summarizing news articles into 4 basic steps.\n",
    "\n",
    "* **Keyword Extraction:** Use Yake! for keyword extraction where each keyword is also assigned a score (the lower, the more important)\n",
    "\n",
    "* **Sentence Scoring:** Use the scores of keywords to assign scores to each sentence\n",
    "\n",
    "* **Sentence Selection:** Use sentence scores to identify the sentence to be included in the summary\n",
    "\n",
    "* **Summary Generation:** Create the summary from selected sentences.\n",
    "\n",
    "In the following, we go through each of the 4 steps in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd304e10-5cae-4882-8194-f5cebeeb6d72",
   "metadata": {},
   "source": [
    "### Keyword Extraction\n",
    "\n",
    "We already saw in the \"Keyword Extraction\" notebook how to use Yake! to extract the most important keywords together with their scores from a document. In the code cell below perform the same steps, by generally applying the default parameters. But again, you can also modify the parameters to see if and how it might affect the final summary. An interesting parameter is `top` which allows us to specify the maximum number of keywords extracted. It is easy to see that if we set this parameter very low, maybe even to 1, that we only pick sentences that indeed include this single top keyword; all other sentences will have a score of 0 and won't be included in the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8668e7-3484-44d5-83c8-997796c1672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"en\"\n",
    "max_ngram_size = 3\n",
    "deduplication_threshold = 0.9\n",
    "deduplication_algo = 'seqm'\n",
    "window_size = 1\n",
    "num_of_keywords = 20 # <-- interesting parameter for us\n",
    "\n",
    "yake_keyword_extractor = yake.KeywordExtractor(lan=language, \n",
    "                                               n=max_ngram_size, \n",
    "                                               dedupLim=deduplication_threshold, \n",
    "                                               dedupFunc=deduplication_algo, \n",
    "                                               windowsSize=window_size,\n",
    "                                               top=num_of_keywords,\n",
    "                                               features=None)\n",
    "\n",
    "yake_keywords = yake_keyword_extractor.extract_keywords(text_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bd0d34-b30a-409a-af63-6748c46a761d",
   "metadata": {},
   "source": [
    "With the `yake.KeywordExtractor` we can extract and display the top keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f86d64-e4e5-4a80-bc64-53c288e7f3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "yake_keywords_sorted = sorted(yake_keywords, key=lambda tup: tup[1], reverse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a0fd3a-a7ec-4e58-a202-e98b329bca25",
   "metadata": {},
   "source": [
    "Let's first have a quick look at the top keywords so we can later check if those keywords are indeed occurring in our summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dba4bb6-2527-4af2-ae09-8569db983d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for keyword, score in yake_keywords_sorted:\n",
    "    print(\"{:.6f}:\\t{}\".format(score, keyword))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d18f1d4-ebc9-4f0d-a3e0-9248076febaf",
   "metadata": {},
   "source": [
    "### Sentence Scoring\n",
    "\n",
    "Now that we have the top keywords together with their scores, we can use this information to score all sentences. One thing we have to be careful about, though: Recall that with Yake!, the lower the score the more important the keyword. However, simply summing all keyword scores for each sentence, and then using the sentences with the lowest score would be wrong. The problem is that sentences without any keyword would get a score of 0.0 which would be misleading. There are various way to solve this, but here we propose a very straightforward approach\n",
    "\n",
    "* Calculate the reciprocal of the original keyword scores so that the higher the value the more important the keyword\n",
    "\n",
    "* Apply a logarithm on the score so that a single keyword cannot dominate the calculation of the score of a sentence\n",
    "\n",
    "* Normalize the score of all sentences based on the length; otherwise, longer sentences will always tend to be more important.\n",
    "\n",
    "**Important:** In many ways, this approach is somewhat arbitrary and only chosen based on some intuition. Maybe having longer sentences in our summary is a good thing, in which case we shouldn't normalize the scores. Similarly, maybe applying a log normalization will smoothing the keyword scores too much, and instead we want to maximize the effects of the very top keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f94d0f-5267-4522-9547-ab55c9fd4aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary to keep track of sentence scores\n",
    "sentence_scores = {}\n",
    "\n",
    "for sid, sent in enumerate(sentences_processed):\n",
    "    # Initialize sentence score\n",
    "    sent_score = 0.0\n",
    "    # Iterate over each top keyword\n",
    "    for keyword, score in yake_keywords_sorted:\n",
    "        # Just a failsafe in the case the keyword has score of 0 (should never happen, though)\n",
    "        if score == 0:\n",
    "            continue\n",
    "        # Iterate over all keyword and check if they occur in the sentence\n",
    "        if keyword in sent:\n",
    "            # Update sentence score using \"some\" formula\n",
    "            sent_score += np.log(1 / score)\n",
    "\n",
    "    # Compute length of sentence (length = number or words/tokens)\n",
    "    sent_len = len(sent.split())\n",
    "\n",
    "    # Normalize score w.r.t. sentence length\n",
    "    sentence_scores[sid] = sent_score / sent_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0192b8-9d63-43ce-9db6-5b51ee9f1c6b",
   "metadata": {},
   "source": [
    "Let's check out the scores we just have calculated for each sentence. In the code cell below, we first order all sentences (more specifically, their indices) with respect to their scores in a descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0a7f75-61ed-4056-8dae-d62b1f0cf28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores_sorted = sorted(sentence_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for sid, score in sentence_scores_sorted:\n",
    "    print('Sentence ID: {}\\t (score: {})'.format(sid, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf2f29d-53b3-4e78-85ac-68ac1211d90b",
   "metadata": {},
   "source": [
    "Now the core part of our summarization algorithm is already done: Each sentence has some kind of importance score that we can now utilize to find the sentence we want to put into the final summary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07e592d-58ec-472c-b96a-f81565f871b4",
   "metadata": {},
   "source": [
    "### Sentence Selection\n",
    "\n",
    "Since we now know how important each sentence is, we now mainly have to decide how big our summary should be. A simply but common way to specify the length of the summary is terms of its relative length w.r.t. the original input document. Here, we quantify using `summary_size` which represents the percentage of the number of sentences in the input document. This means, with `summary_size`, we specify that the summary should be around 25% of the length of the input (in terms of the number of sentences). Note that we always round up (`np.ceil`).\n",
    "\n",
    "In the code cell below, we not only pick the sentence indices with the largest scores, but also extract only those indices -- as we no longer need the score -- and finally sort the indices. We do this to preserve the original sentence order. This means that the most important sentence might not be the first sentence. However, it is only important that the most important sentences are there, and it is arguably preferable to preserve the original sentence order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafd0cfa-4369-4f75-b9f0-6fb23a38c7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_size = 1/4\n",
    "\n",
    "# Sort sentence indices based on sentence score (for Yake!: ascendengin since the smaller the better)\n",
    "top_sentences = sentence_scores_sorted[:int(np.ceil((summary_size*num_sentences)))]\n",
    "print(top_sentences)\n",
    "\n",
    "# Extract only the sentence indices; no need for the score any more\n",
    "top_sentences = [ t[0] for t in top_sentences ]\n",
    "print(top_sentences)\n",
    "\n",
    "# Sort indices to preserve order \n",
    "top_sentences = sorted(top_sentences)\n",
    "print(top_sentences)\n",
    "\n",
    "print()\n",
    "print('Number of sentences in the summary: {}'.format(len(top_sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f084f10a-9ac5-4815-859e-716e8d044f39",
   "metadata": {},
   "source": [
    "### Summary Generation\n",
    "\n",
    "The last step is now to actually generate the summary. The most intuitive approach is to include all the most important sentences (incl. preserving the order). However, we can also include any additional consideration we deem meaningful. For example, one can argue that the summary of a news article should always include the very first sentence as it (a) ensures a \"smooth\" start of the summary and (b) the first sentence of a news article often contains some core information. While our list of important sentences might already include the first sentences, this is not guaranteed.\n",
    "\n",
    "As such, in the code cell below, if the first sentence is not part of the most important sentences, we simply add it (i.e., sentence index 0) to the front.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10af99e7-21de-42f2-a484-0f85f7a87747",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0 not in top_sentences:\n",
    "    top_sentences = [0] + top_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850757a7-4f28-410d-be95-d3e0d338966c",
   "metadata": {},
   "source": [
    "Knowing all the indices of the sentences that should form our summary, we can finally generate it. Note that we take the sentences from `sentences`, i.e., the list containing the original and not the preprocessed sentences to ensure that the sentences in the summary look indeed like the ones in the original news article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9c6355-dcec-4437-8bf6-fd485ae48d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in top_sentences:\n",
    "    print(sentences[idx].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7e2210-fb59-4e4d-b617-7f70f978b9bb",
   "metadata": {},
   "source": [
    "There it is, our summary. For easy comparison, we can also print all original sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bf2a4d-e0cd-4b90-b6be-5ec000edcc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in sentences:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977b79ae-bd64-4dd3-a236-0cb641a31d08",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec717a4-c815-400d-91d5-577e27710811",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "As you have seen throughout the notebook, creating an extractive summary -- at least on a very basic level -- is actually not that difficult. Of course, the approach presented here is unlikely to yield state-of-the-art results since we made many simplifying assumptions and processing steps. Here are just a few consideration when aiming to improve the results\n",
    "\n",
    "* **Choice of Keyword Extraction Algorithm:** We used keyword extraction as a core building block for our summarization algorithm. Not only do many other keyword extraction exist, most (incl. Yake!) feature a series of input parameters that potentially could be tuned to see better summaries.\n",
    "\n",
    "* **Beyond Importance:** We selected the most important sentences for the summary, where the importance of a sentence is a normalized sum of the scores of the keywords contained in the sentence. In practice, this might mean that a summary can contain sentences that can be very similar because they contain the same highly scored keywords. To address this, we could also consider other criteria, for example, \"novelty\" where we might ignore an important sentence when the summary already contains a very similar sentence. Or maybe we want to favor sentences that are particularly short/long, contain many person or place names, or contain numbers. Such choices will often depend on the application context (e.g., we might prefer summaries with many numbers in the case of financial documents).\n",
    "\n",
    "* **Limitations of Extractive Summaries:** Picking non-consecutive sentences from a document -- even when preserving the order -- is unlikely to yield a summary that \"flows\" well. This is particularly true if the summary might contain sentences with many pronouns and we might know to which phrases those pronouns refer to -- or even worse, the summary might in fact mislead us regarding the references of pronouns. In this case, might might want to lower the score of sentences containing (many) pronouns, or perform coreference resolution before creating our summary.\n",
    "\n",
    "In short, it's very easy to point out problems and limitations with the simple summarization approach proposed in this notebook. However, there are also a wide range of meaningful and arguably intuitive extensions for further improvement conceivable. Having such a simple baseline algorithm (a) shows that it can be very easy to get started, but also (b) helps to better understand the involved challenges and potential solutions towards more sophisticated results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752a1747-5c64-4527-87e6-a4e170454eda",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6e0ae1-0b7a-415e-a705-fe157966b3c7",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Extractive summarization based on keyword extraction is a technique that focuses on identifying and selecting the most important sentences or phrases from a text using keywords. In this approach, keywords are extracted from the original text, representing the significant concepts or topics discussed in the document. These keywords serve as the basis for determining the salient sentences that will be included in the summary.\n",
    "\n",
    "The process of keyword extraction involves several steps. Initially, the text is preprocessed by removing stopwords, punctuation, and other irrelevant elements. Then, various algorithms and techniques are applied to identify and rank the keywords. Common approaches include frequency-based methods like TF-IDF (Term Frequency-Inverse Document Frequency) and statistical techniques such as TextRank, RAKE (Rapid Automatic Keyword Extraction), or -- like in this notebook -- Yake! (Yet Another Keyword Extractor).\n",
    "\n",
    "Once the keywords are determined, the next step is to identify the sentences in the document that contain these keywords or are semantically related to them. These sentences are considered to be the most informative and are selected for inclusion in the summary. The final summary is created by combining these selected sentences, preserving their original order or rearranging them to improve coherence.\n",
    "\n",
    "Extractive summarization based on keyword extraction offers a relatively simple and effective way to generate summaries. It relies on the assumption that important information is often associated with the keywords present in the text. However, this approach may have limitations in capturing the overall context and generating summaries that are truly representative of the original content. It may struggle with understanding the relationships between sentences or capturing nuanced information that is not explicitly associated with the extracted keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58448e1-4e54-465f-8ed5-47681c97919a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs5246]",
   "language": "python",
   "name": "conda-env-cs5246-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
