{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "915d0d2c-52e9-41be-a554-6ebf171326d6",
   "metadata": {},
   "source": [
    "<img src='data/images/section-notebook-header.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912a64c0-6d86-4975-b7c7-a12a54866c0d",
   "metadata": {},
   "source": [
    "# The CYK Algorithm\n",
    "\n",
    "The CYK algorithm, also known as the Cocke-Younger-Kasami algorithm, is a dynamic programming algorithm used for parsing sentences in order to determine their syntactic structure. It is specifically designed for parsing based on context-free grammars (CFGs). The CYK algorithm is named after its inventors: John Cocke, Daniel Younger, and Tadao Kasami.\n",
    "\n",
    "The CYK algorithm operates by constructing a parse table based on the input sentence and the CFG grammar rules. The table has multiple cells, each representing a span of the input sentence. The algorithm iteratively fills in the cells of the parse table bottom-up, starting from shorter spans and gradually building up to longer spans.\n",
    "\n",
    "The key idea behind the CYK algorithm is to determine if a span of the sentence can be derived from any CFG rule by considering all possible combinations of smaller subspans. This process is repeated for increasing span lengths until the entire sentence is covered. Here's a simplified step-by-step overview of the CYK algorithm:\n",
    "\n",
    "* **Initialization:** Create a parse table with cells representing individual words of the sentence.\n",
    "\n",
    "* **Lexical Analysis:** Populate the initial cells of the parse table with non-terminal symbols that can generate the corresponding words in the sentence, based on the CFG rules.\n",
    "\n",
    "* **Parsing Loop:** Iterate over the remaining cells of the parse table, considering all possible ways to split a span into two subspans. Check if the non-terminal symbols in the subspans can be combined to generate the current span.\n",
    "\n",
    "* **Backtracking:** Once the parsing loop is complete, examine the top-right cell of the parse table. If it contains the start symbol of the CFG, it indicates that the sentence can be derived from the grammar. By backtracking through the parse table, the algorithm can generate one or more parse trees representing the syntactic structure of the sentence.\n",
    "\n",
    "The CYK algorithm is efficient for parsing sentences based on CFGs, with a time complexity of O(n^3 * |G|), where n is the length of the sentence and |G| is the number of rules in the CFG grammar. It is widely used in computational linguistics and natural language processing for constituency parsing, which involves analyzing the hierarchical structure of sentences. Note that the CYK algorithm is applicable only for parsing with CFGs, and it assumes that the grammar is in Chomsky Normal Form (CNF) or can be converted to CNF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb93092-b1ce-4045-991d-15451853edc7",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dacf741-83c0-4bd8-b6cd-dd92fdddbe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402fcf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from src.utils import load_grammar, create_nltk_tree, reconstruct_parse_trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a25b9e-fc38-4b38-84d3-1df4f490174b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bebe715",
   "metadata": {},
   "source": [
    "## Using Off-the-Shelf Parsers\n",
    "\n",
    "Syntactic parsing is a very fundamental NLP task. Hence, there's a wide range of existing tools that parse sentences, and which you would use in practice. NLTK is quite flexible in the sense that it provides a parser that takes your custom grammar as input and tries to parse sentences using this grammar. Let's load our toy grammar we used in the lecture. Note that this grammar is context-free but not in Chomsky Normal Form (CNF), as the NLTK parser we use is smart enough to handle this.\n",
    "\n",
    "\n",
    "### Load Grammar (from a file into a string)\n",
    "\n",
    "In NLTK, you can write your own grammar as a string and then use the `CFG` class to represent your string as a context-free grammar. In practice, the easiest way to do this is to write all grammar rules into a file and then read that file into a string; see the code cell below. Each grammar rule is indicated by its own line in the file. The left-hand side and right-hand side of a rule is separated by a `->`. We already prepared the grammar we use in the lecture as a text file. So let's first read that file into a string and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96ccadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/grammars/toy-grammar-cfg.txt\",\"r\") as f:\n",
    "    grammar_string = f.read()\n",
    "    \n",
    "print(grammar_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c174c22",
   "metadata": {},
   "source": [
    "### Create Parser (using our toy grammar)\n",
    "\n",
    "The `nltk.parse` package in NLTK provides various parsers for syntactic parsing. These parsers allow you to analyze the grammatical structure of sentences and generate parse trees representing the syntactic relationships between words and phrases. Here are some of the parsers available in the `nltk.parse` package:\n",
    "\n",
    "* **Recursive Descent Parser:** The `RecursiveDescentParser` is a top-down parser that recursively applies production rules from a context-free grammar (CFG) to generate parse trees. It explores all possible production rule applications until it finds a valid parse or exhausts all options.\n",
    "\n",
    "* **Shift-Reduce Parser:** The `ShiftReduceParser` is a bottom-up parser that uses a shift-reduce strategy to construct parse trees. It starts with an empty stack and shifts tokens from the input sentence onto the stack, then reduces the stack by applying CFG rules until a valid parse is found.\n",
    "\n",
    "* **Chart Parser:** The `ChartParser` is based on chart parsing, which is a dynamic programming algorithm for parsing according to a CFG. It incrementally fills in a parse chart, combining constituents based on CFG rules and building larger constituents until the entire sentence is parsed.\n",
    "\n",
    "* **Earley Parser:** The `EarleyParser` is based on Earley's algorithm, a chart parsing algorithm that efficiently handles grammars with arbitrary CFG rules. It uses a chart to represent completed and partial parses and performs predictions, scans, and completions to construct parse trees.\n",
    "\n",
    "* **PCFG Parser:** The `PChartParser` and `ViterbiParser` are probabilistic parsers based on probabilistic context-free grammars (PCFGs). They use statistical probabilities to guide the parsing process, determining the most likely parse tree based on the observed probabilities in the training data.\n",
    "\n",
    "These parsers in the `nltk.parse` package provide different parsing strategies and algorithms, catering to different requirements and linguistic phenomena. You can choose a parser based on the specific parsing needs, the nature of the grammar, and the available training data.\n",
    "\n",
    "In the following, we will make use of the `ChartParser`. To this end, we first need to convert our grammar from a string into an internal representation for NLTK, and then create a `ChartParser` based on this internal representation of our grammar. The code cell below accomplishes both steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d04925",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = nltk.CFG.fromstring(grammar_string)\n",
    "parser = nltk.ChartParser(grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c07383d",
   "metadata": {},
   "source": [
    "### Parse Sentences\n",
    "\n",
    "Everything is not set up to actually parse sentences. Of course, we are very limited w.r.t. to the types of sentences we can parse since our toy grammar is indeed very small, particularly regarding the lexicon rules (i.e., the set of words we consider). So we simply stick to the example sentence from the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd19498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = ['I', 'book', 'the', 'flight', 'through', 'Singapore' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62acca4f",
   "metadata": {},
   "source": [
    "Depending on the sentence, you may get 0, 1 or multiple parse trees; the example we used in the lecture will return 3 parse trees. By default, each parse tree is represented as a formatted string that indicates the nesting of the constituents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a2c9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, tree in enumerate(parser.parse(sent)):\n",
    "    print('=== Parse Tree {}: ==='.format(idx))\n",
    "    print(tree)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227d9a82",
   "metadata": {},
   "source": [
    "For a better visualization, NLTK also provides some basic methods to print parse trees in a more convenient manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfb02cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, tree in enumerate(parser.parse(sent)):\n",
    "    print('=== Parse Tree {}: ==='.format(idx))\n",
    "    create_nltk_tree(tree).pretty_print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2372ef1-8987-40ed-8137-7603afe60b65",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52abc6b",
   "metadata": {},
   "source": [
    "## CYK Parsing from Scratch\n",
    "\n",
    "### Load Grammar\n",
    "\n",
    "Since we want to implement CYK Parsing from scratch, we need our grammar in Chomsky Normal Form (CNF). While this conversion can be implemented in a rather straightforward manner, here we simply assume that we have the CNF of our grammar given in a file. So let's first have a look at it by simply reading the file into a string and printing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcefa665-223d-4c81-b7c7-1d2f71262ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/grammars/toy-grammar-cfg-cnf.txt') as f:\n",
    "    cnf_grammar_string = f.read()\n",
    "    \n",
    "print(cnf_grammar_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079fef02-49a5-42ea-b6a8-e22363ec8717",
   "metadata": {},
   "source": [
    "The non-terminal symbols, which in case of grammars for syntactic parsing are (a) the names of constituents, e.g., noun phrase, verb phrase, prepositional phrase, etc., or (b) part-of-speech tags. Note that \"artificial\" symbols `X1` and `X2` were introduced when mapping the original CFG to a one Chomsky Normal Form (CNF). Check out the lecture for more details how we converted the original CFG or a grammar in CNF.\n",
    "\n",
    "Since we want to implement the CYK algorithm from scratch using a CNF, we cannot use `nltk.CFG` any longer to convert the grammar string into an internal representation. Instead, we provide a auxiliary method `load_grammar()` to read the grammar file to\n",
    "\n",
    "* Extract list `N` containing all non-terminal symbols\n",
    "\n",
    "* Extract list `T` containing all terminal symbols\n",
    "\n",
    "* Extract list `R` containing all rules (incl. the probability for each rule in case of a PCFG)\n",
    "\n",
    "Each rule is represented as a 3-tuple with the left-hand side being at position 0, the right-hand side at position 1, and the probability of the rule at position 2 (if PCFG). Since the right-hand side can of course have multiple symbols, the right hand-side is also represented as a tuple itself. The code cell below uses the method `load_grammar()` to load the grammar from the file, and prints all rules in a similar format as above -- however, note that now each rule is an individual line, instead of using the `|` notation to combine rules with the same left-hand side; also, the rules have no particular order as it's not needed for the algorithm to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81937ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, T, R = load_grammar('data/grammars/toy-grammar-cfg-cnf.txt')\n",
    "\n",
    "# Print all rules (we can ignore the probability part since this is not a PCFG)\n",
    "for lhs, rhs, _ in R:\n",
    "    print(\"{} -> {}\".format(lhs, ' '.join(rhs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccf0694",
   "metadata": {},
   "source": [
    "Since our grammar and particularly the lexicon is very, very simple, we can also easily check out the list of non-terminal and terminal symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a6fd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== Non-terminals: ===')\n",
    "print(N)\n",
    "print()\n",
    "print('=== Terminals: ===')\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e3843c",
   "metadata": {},
   "source": [
    "### Implementing CYK Parsing\n",
    "\n",
    "We introduced the CYK Parsing algorithm in the lecture and saw how it utilizes the idea of Dynamic Programming. Here, the base cases are the parsing of words (terminal symbols) to all possible part-of-speech tags or phrase names (non-terminal symbols), which can directly be derived from the lexicon rules in our grammar. We saw this in full detail in the lecture using a complete walkthrough. We therefore skip a detailed discussion here.\n",
    "\n",
    "Just to highlight one implementation detail: In the walkthrough example on the lecture slides -- see the relevant parse table below -- each cell in the dynamic programming table contained a set of valid non-terminal symbols. For example, the cell for span `[0, 1]` contained 2 non-terminals `NP` and `Pronoun`. This made it very easy to visualize and understand the basic steps of CYK parsing algorithm\n",
    "\n",
    "<img src='data/images/cyk-full-parse-example.png' width='60%' />\n",
    "\n",
    "For the algorithm, however, it's a bit more convenient to use truth values (0 or 1) to tell as if a non-terminal should be placed into a cell. The method therefore uses a nested table that stores `[0, 1, \"NP\"] = 1` and `[0, 1, \"Pronoun\"] = 1`. In other words, our parse table is no longer 2-dimensional like the table shown above but now 3-dimensional. This ensures that the table entries are no longer sets but individual labels. This 3-dimensional table would be tricky to visualize but that simplifies the actual implementation of the CYK algorithm.\n",
    "\n",
    "\n",
    "#### Basic Membership Algorithm\n",
    "\n",
    "The basic membership algorithm of the CYK algorithm is used to determine whether a given sentence can be derived from a given context-free grammar (CFG). The algorithm checks if a sentence belongs to the language defined by the CFG by iteratively building up parse table entries for different spans of the sentence. It's important to note that the basic membership algorithm of the CYK algorithm only provides a \"yes\" or \"no\" answer to the question of whether a sentence belongs to the language defined by the grammar. It does not generate the parse tree or provide additional details about the derivation. For generating parse trees, additional steps and modifications to the basic algorithm are required.\n",
    "\n",
    "The method `cyk_parse_basic()` below implements the basic membership algorithm of the CYK parser. The code of the method is annotated; and together with the lecture slides should make it easy to follow and understand its inner workings. If things are still unclear, we recommend adding additional `print` statements into the code to make it easier to follow the individual steps of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f19d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyk_parse_basic(tokens, rules):\n",
    "    n = len(tokens)\n",
    "    \n",
    "    # Initialize dynamic programming table (it's a 3-dimensional table!)\n",
    "    CYK = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0)))\n",
    "    \n",
    "    # Initialize parse: span of length 1\n",
    "    for s in range(n):\n",
    "        # Find all non-terminals that can generate the terminal\n",
    "        for A, rhs, _ in rules:\n",
    "            if rhs == (tokens[s],):\n",
    "                CYK[s][s+1][A] = 1\n",
    "    \n",
    "    # Handle spans of length 2+ using dynamic programming\n",
    "    for length in range(2, n+1):\n",
    "        for start in range(0, n-length+1):     # Loop over all\n",
    "            end = start + length               # the possible\n",
    "            for split in range(start+1, end):  # binary splits\n",
    "                # Check each production rule (right-hand side must have 2 symbols; ignore lexicon rules with 1 right-hand side symbol)\n",
    "                for A, (B, C), _ in [ r for r in rules if len(r[1]) == 2]:\n",
    "                    # is_valid = 1 if B and C can generate left and right part (recall that each entry in CYK is either 0 or 1!)\n",
    "                    is_valid = CYK[start][split][B] * CYK[split][end][C]\n",
    "                    # The same LHS needs to be able to generate the RHS only once\n",
    "                    CYK[start][end][A] = np.max([ is_valid, CYK[start][end][A] ])\n",
    "\n",
    "    return CYK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cd57ea",
   "metadata": {},
   "source": [
    "We can now use our implementation to check if a sentence is valid w.r.t. to our grammar. Recall that the result of the parse tells us that a sentence is valid if the table contains the start symbol `S` in the cell referring to the complete span of the sentence. In the context of our implementation, this means we have to check if the cell `CYK[0][len(tokens)]['S']` contains a `1` or `0`, where `1` means that the sentence is valid; or otherwise invalid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3039ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['I', 'book', 'the', 'flight', 'through', 'Singapore' ] # Valid\n",
    "#tokens = ['I', 'book', 'the', 'meal', 'through', 'Singapore' ] # Valid despite questionable semantics\n",
    "#tokens = ['I', 'flight', 'the', 'book', 'through', 'Singapore' ] # Invalid\n",
    "\n",
    "# Parse sentence, return complete dynamic programming table\n",
    "CYK = cyk_parse_basic(tokens, R)\n",
    "\n",
    "# Check of the cell for the longest span contains start symbol S (1=yes, 0=no)\n",
    "print(CYK[0][len(tokens)]['S'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5086f352-89a5-4c78-bdb6-e3f01b8bf854",
   "metadata": {},
   "source": [
    "For the list `tokens = ['I', 'book', 'the', 'flight', 'through', 'Singapore' ]` the output should show a `1` as this is a valid sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3f6488-e255-4b09-b4e2-dc73a080b028",
   "metadata": {},
   "source": [
    "#### Get all Parse Trees\n",
    "\n",
    "The basic membership algorithm implemented by method `(cyk_parse_basic)` only tells us if a sentence is valid or not (with respect to a given grammar). This also means that this basic implementation does not return the parse tree itself. Also, we already know -- and have seen above -- that the same sentence can often be derived using different parse trees. Instead of just knowing whether a sentence is valid, often a downstream application might rely on the actual parse tree or multiple parse trees for further processing.\n",
    "\n",
    "We therefore need to extend our basic algorithm to support this. In a nutshell -- in terms of the dynamic programming table used for the CYK algorithm -- we need to remember which 2 table cells we used to fill in a new cell when completing our parse table. After the algorithm terminates -- and the sentence is indeed a valid sentence -- we can \"go back\" to find all possible parse trees. The figure below, taken from the lecture slides, illustrates the idea by using pointer (represented by the errors) to record which 2 cells were used to complete a new cell (here the top-right cell, i.e., the first step when backtracking)\n",
    "\n",
    "<img src='data/images/cyk-parse-with-pointers.png' width='60%' />\n",
    "\n",
    "Fortunately, extending the basic membership algorithm to support this is rather straightforward. In the code cell below, the method `cyk_parse_basic_ptr()` performs the CYK algorithm but also returns the required information to reconstruct all possible parse trees (note that this method does not explicitly return the parse trees themselves). The main addition compared to methods `cyk_parse_basic()` from above is a pointer table `PTR` -- the newly added lines are marked as code comments. The algorithm uses `PTR` to keep track of all pairs of cells that yielded a new valid cell. For example, with respect to the figure above, `PTR` will contain the following entry:\n",
    "\n",
    "* `PTR[0][6]['S'] = ((0, 1, 'NP'), (1, 6, 'VP'))`\n",
    "\n",
    "Have a look at the code method `cyk_parse_basic_ptr()` below and simply acknowledge the minor changes to the original method implementing the basic membership algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3636dfa4-9e97-495c-940e-d57bd940e8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyk_parse_basic_ptr(tokens, rules):\n",
    "    n = len(tokens)\n",
    "    \n",
    "    # Initialize dynamic programming table + backtrace pointers\n",
    "    CYK = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0)))\n",
    "    # Initialize backpointers table\n",
    "    PTR = defaultdict(lambda : defaultdict(lambda : defaultdict(list))) # <-- newly added line\n",
    "    \n",
    "    # Initialize parse: span of length 1\n",
    "    for s in range(n):\n",
    "        # Find all non-terminals that can generate the terminal\n",
    "        for A, rhs, _ in rules:\n",
    "            if rhs == (tokens[s],):\n",
    "                CYK[s][s+1][A] = 1\n",
    "                PTR[s][s+1][A].append(tokens[s])\n",
    "    \n",
    "    # Handle spans of length 2+ using dynamic programming\n",
    "    for length in range(2, n+1):\n",
    "        for start in range(0, n-length+1):     # Loop over all\n",
    "            end = start + length               # the possible\n",
    "            for split in range(start+1, end):  # binary splits\n",
    "                # Check each production rule (ignore lexicon rules)\n",
    "                for A, (B, C), _ in [ r for r in rules if len(r[1]) == 2]:\n",
    "                    # is_valid = 1 if B and C can generate left and right part\n",
    "                    is_valid = CYK[start][split][B] * CYK[split][end][C]\n",
    "                    # The same LHS needs to be able to generate the RHS only once\n",
    "                    CYK[start][end][A] = np.max([ is_valid, CYK[start][end][A] ])\n",
    "                    # If this is a valid split, remember from which cells we came\n",
    "                    if is_valid > 0:                                                      # <-- newly added line\n",
    "                        PTR[start][end][A].append(((start, split, B), (split, end, C)))   # <-- newly added line\n",
    "\n",
    "    return CYK, PTR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5914b0-3fe3-4e5c-a8d6-d86f0f56c8f6",
   "metadata": {},
   "source": [
    "Let's run the method on our example sentences (i.e., token list) and inspect the results. Apart from still checking if the sentence is valid, we can also exemplarily look at some pointer information. In line with the example above, we print the point information for the top-right cell for symbol `S` to see from which 2 cells we came to generate symbol `S`. Of course, the output should match our expectation from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3417deb3-b475-4589-847c-d5cb70b22ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "CYK, PTR = cyk_parse_basic_ptr(tokens, R)\n",
    "\n",
    "print(\"Is the sentence valid (1=yes, 0=no)?\")\n",
    "print(CYK[0][len(tokens)]['S'])\n",
    "print()\n",
    "print(\"Pointer information from top-right cell for S:\")\n",
    "print(PTR[0][len(tokens)]['S'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c041552a-0228-421b-85c0-8461cc78a1fd",
   "metadata": {},
   "source": [
    "The `PTR` variable contains all the information required to reconstruct all possible parse trees. Since this step is not an integral part of the CYK algorithm, we provide a separate auxiliary method `reconstruct_parse_trees` to reconstruct all parse trees starting from any arbitrary cell of the parse table. The execution of `reconstruct_parse_trees()` in the code cell below returns all parse trees for the complete sentence indicated by the input parameters `0, len(tokens)` which represents the top right cell of the parse table. In principle, this cell might contain other symbols, so we also need to specify symbol `S` as input parameter to only consider parse trees representing valid parses for the whole sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587d1b33-5920-4728-b1ee-cd2df679de18",
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = reconstruct_parse_trees(PTR, 0, len(tokens), 'S')\n",
    "\n",
    "# Again, we use some NLTK magic to print the parse trees\n",
    "for t in trees:\n",
    "    create_nltk_tree(t).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5398a2-69ac-453d-9f42-fa4c61f2adca",
   "metadata": {},
   "source": [
    "For the list `tokens = ['I', 'book', 'the', 'flight', 'through', 'Singapore' ]` the output should show 3 parse trees which naturally should also match the lecture slides; see the figure below for the relevant part for the slide. The order of the tree will probably match but it does not have to.\n",
    "\n",
    "<img src='data/images/cyk-example-parse-trees.png' width='40%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee3cca4-e3b7-4bcf-a604-ae8fe41aa0e8",
   "metadata": {},
   "source": [
    "#### Probabilistic CYK Parsing\n",
    "\n",
    "The method `cyk_parse_basic_ptr()` might give us all possible parse trees, however, it does not tell us which is the most likely parse tree. In the output above, the first parse tree is arguably the most likely one since the prepositional phrase (PP) *\"through Singapore\"* should be associated with the Nominal *\"flight\"* and not with a verb phrase. The latter case would be meaningful if the prepositional phrase (PP) would be *\"through an agent\"*.\n",
    "\n",
    "The fundamental idea to find the best parse tree is to associate each parse tree with a probability and pick the parse tree with the highest probability. Probabilistic CYK parsing is an extension of the CYK parsing algorithm that incorporates probabilistic information to handle ambiguity and assign probabilities to different parse trees. In contrast to the previous implementations of the CYK algorithms, each production rule of the CFG is associated with a probability. These probabilities represent the likelihood of a rule being used in a given context. The algorithm considers all possible ways of combining smaller constituents to build larger constituents, and it computes the probabilities of the resulting constituents based on the probabilities of the rules used.\n",
    "\n",
    "The algorithm starts by initializing a matrix where each cell represents a constituent or a sub-constituent of the sentence. It then proceeds to fill in the matrix using dynamic programming techniques. The probabilities of the constituents are computed by combining the probabilities of the rules used to generate the sub-constituents. Once the matrix is filled, the algorithm can determine the most likely parse tree for the sentence by backtracking through the matrix, considering the probabilities of the constituent combinations. This allows for the identification of the most probable parse, given the probabilistic information encoded in the grammar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29299b68-f907-4551-b506-952a2482796d",
   "metadata": {},
   "source": [
    "**Probabilistic Context-Free Grammar (PCFG)**\n",
    "\n",
    "Probabilistic parsing requires a probabilistic context-free grammar (PCFG) where each rule is associated with a probability. In practice these probabilities are derived from an annotated dataset. In this notebook, to keep it simple, we use the example PCFG from the lecture slides which is shown in the figure below; note that this grammar is already in CNF!\n",
    "\n",
    "<img src='data/images/cyk-example-pcfg.png' width='80%' />\n",
    "\n",
    "You can find the rules of this PCFG in the file `toy-grammar-cfg-cnf-probs.txt`. Let's read this file and have a look at the rules; the set of non-terminal and terminal symbols are the same as before. Compared to the previously used CFG, however, now all rules have a probability attached to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0230abb9-69a5-4e96-8f42-e4196605cde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, T, R = load_grammar('data/grammars/toy-grammar-cfg-cnf-probs.txt')\n",
    "\n",
    "# Print all rules\n",
    "for lhs, rhs, prob in R:\n",
    "    print(\"{} -> {} [{}]\".format(lhs, ' '.join(rhs), prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aadf87-4d23-4441-a50c-5180d9edd2b1",
   "metadata": {},
   "source": [
    "**Probabilistic CYK Parsing**\n",
    "\n",
    "With the PCFG, we can now extend the CYK algorithm to find the best parse tree given the available probabilities. The method `cyk_parse_probabilistic_ptr()` below implements the algorithm. The changes, simply speaking, refer to changing the entries of table `CYK` from either `0` or `1` to probabilities values. For example, we change the line `CYK[s][s+1][A] = 1` to `CYK[s][s+1][A] = prob` in the initialization step\n",
    "\n",
    "**Important:** The algorithm below makes one simplifying assumption: We are only interested in the best parse tree -- that is, we no longer care about all possible parse trees and what their probabilities might be. This would require to also keep track of the probabilities in the `PTR` variable, which would also mean that our auxiliary method `reconstruct_parse_trees()` would need to be more complex to accommodate the probability information. In short, the method `cyk_parse_probabilistic_ptr()` will return only a single parse tree (assuming the sentence is valid), which will be the parse tree with the highest probability. This simplification is reflected in the IF clause `if p > CYK[start][end][A]:`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d464a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cyk_parse_probabilistic_ptr(tokens, rules):\n",
    "    n = len(tokens)\n",
    "    \n",
    "    # Initialize dynamic programming table\n",
    "    CYK = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0)))\n",
    "    # Initialize backpointers table\n",
    "    PTR = defaultdict(lambda : defaultdict(lambda : defaultdict(list)))\n",
    "    \n",
    "    # Initialize parse: span of length 1\n",
    "    for s in range(n):\n",
    "        # Find all non-terminals that can generate the terminal\n",
    "        for A, rhs, prob in rules:\n",
    "            if rhs == (tokens[s],):\n",
    "                CYK[s][s+1][A] = prob\n",
    "                PTR[s][s+1][A].append(tokens[s])\n",
    "    \n",
    "    # Handle spans of length 2+ using dynamic programming\n",
    "    for l in range(2, n+1):\n",
    "        for start in range(0, n-l+1):\n",
    "            end = start + l\n",
    "            for split in range(start+1, end):\n",
    "                # Check each production rule (ignore lexicon rules)\n",
    "                for A, (B, C), prob in [ r for r in rules if len(r[1]) == 2]:\n",
    "                    # Calculate probability of reaching the cell with the current rule\n",
    "                    p = CYK[start][split][B] * CYK[split][end][C] * prob\n",
    "                    # If the probability is larger then the current one => update!\n",
    "                    # Important: we only care about the single best parse and no longer all parses\n",
    "                    if p > CYK[start][end][A]:\n",
    "                        CYK[start][end][A] = p\n",
    "                        PTR[start][end][A].append(((start, split, B), (split, end, C)))\n",
    "                        \n",
    "    return CYK, PTR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b7218f-fedf-4213-9cb0-729215a28829",
   "metadata": {},
   "source": [
    "Let's run `cyk_parse_probabilistic_ptr()` on our example sentences (i.e., token list) and inspect the results. Now the entry of the top-right cell for symbol `S` will contain the total probability of the best parse tree. Of course, the probability is very small since we multiply many values less than `1`. In practice, to avoid any arithmetic underflow issues, we naturally would use log probabilities. However, here the focus is on understanding the principles of parsing using the CYK algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d994952f-dc43-4a12-9043-b509c053cb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "CYK, PTR = cyk_parse_probabilistic_ptr(tokens, R)\n",
    "\n",
    "print(\"Probability of best parse:\")\n",
    "print(CYK[0][len(tokens)]['S'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f94fa7-da1c-473f-9db5-d567b51cd84e",
   "metadata": {},
   "source": [
    "**Important:** The image below is taken from the lecture slides showing the total probabilities for 2 possible parse trees for our example sentence. As you can see the probability of the left parse tree (arguably not the correct parse tree) does not match the probability as the output of our method above. The reasons for this is because the probabilities on the slide have been computed based on the PCFG that is **not** in CNF, whereas the method `cyk_parse_probabilistic_ptr()` assumes as PCFG in CNF. Therefore the absolute values for the probabilities are not the same. However, note that in practice we are not interested in the absolute probabilities. We are only interested in which possible parse tree has the highest probability.\n",
    "\n",
    "<img src='data/images/cyk-example-pcfg-probabilities.png' width='80%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48975e1f-85f2-466c-aa7a-93113dd49d45",
   "metadata": {},
   "source": [
    "Lastly, we can again plot the best parse tree using the auxiliary method `reconstruct_parse_trees()`. As mentioned above, the `PTR` variable does not need to carry information about any probabilities since the method `cyk_parse_probabilistic_ptr()` only returns the one best parse tree. In other words, `PTR`  contains only the pointer relevant for this best parse tree and the method `reconstruct_parse_trees()` plots it; see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f6be23-9977-40ca-b36f-7ce775982c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = reconstruct_parse_trees(PTR, 0, len(tokens), 'S')\n",
    "\n",
    "# Again, we use some NLTK magic to print the parse trees\n",
    "for t in trees:\n",
    "    create_nltk_tree(t).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0fd878-1155-41ef-9d91-7e6ea8852fa9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdcfb1d-fe2d-4914-8ff2-262b8b04b99d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The CYK (Cocke-Younger-Kasami) algorithm is a parsing technique used in computational linguistics to determine the possible syntactic structures, or parse trees, of a sentence based on a given context-free grammar (CFG). It is a dynamic programming algorithm that operates in a bottom-up manner, starting from the individual words of the sentence and building up to larger constituents. The CYK algorithm utilizes a parsing table, often represented as a triangular matrix, where each cell corresponds to a constituent or a sub-constituent of the sentence. The algorithm fills in the matrix by considering all possible ways of combining smaller constituents to form larger constituents, based on the production rules of the grammar. By recursively applying these rules, the algorithm can determine the set of possible parse trees for the given sentence.\n",
    "\n",
    "One of the key advantages of the CYK algorithm is that it can handle ambiguity in natural language sentences. Ambiguity arises when a sentence can be parsed in multiple ways, resulting in different syntactic structures and meanings. The CYK algorithm explores all possible parse trees and identifies all valid interpretations of the sentence, allowing for a comprehensive analysis of ambiguous constructions.\n",
    "\n",
    "To further enhance the accuracy and flexibility of the CYK algorithm, the use of a Probabilistic Context-Free Grammar (PCFG) is important. A PCFG extends the traditional CFG by associating probabilities with each production rule. These probabilities represent the likelihood of a rule being used in a particular context or language. Introducing probabilities to the CYK algorithm enables the estimation of the most likely parse tree for a sentence. By considering the probabilities associated with each rule used during the parsing process, the algorithm can rank different parse trees and identify the most probable interpretation. This is particularly valuable in natural language processing tasks such as machine translation, language modeling, and speech recognition, where handling ambiguity and selecting the most likely interpretation are crucial for accurate results.\n",
    "\n",
    "In summary, the CYK algorithm provides a systematic approach for parsing sentences according to a given context-free grammar. By incorporating probabilistic information through a PCFG, the algorithm can handle ambiguity and assign probabilities to different parse trees, leading to more accurate and effective natural language processing applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786a9ead-dcc4-4f51-a2c6-6d810bfabe54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py39]",
   "language": "python",
   "name": "conda-env-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
