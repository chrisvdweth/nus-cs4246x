{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9806fc1-9440-479c-a223-742ed63d4226",
   "metadata": {},
   "source": [
    "<img src='data/images/section-notebook-header.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d54cfe",
   "metadata": {},
   "source": [
    "# POS Tagging with HMMs\n",
    "\n",
    "We have already briefly looked into Part-of-Speech (POS) tagging in \"Natural Language Processing: Foundations\" during the topic of text preprocessing. We saw that the task of lemmatization relies on the information about words POS tag (i.e., word type: noun, verb, adjective, adverb, and so on). However, in this context, we used off-the-shelf POS tagger but ignored how POS tagging actually can be implement. In NLP, POS tagging is one of very important sequence labeling tasks -- that is, where each word/token in a sequences (e.g., a sentence) is assigned with a tag. Other common sequence labeling tasks include:\n",
    "\n",
    "* **Named Entity Recognition (NER):** NER aims to identify and classify named entities in text, such as names of persons, organizations, locations, dates, and more. For instance, given the sentence \"Apple Inc. is planning to open a new store in New York City next month,\" a NER system would label \"Apple Inc.\" as an organization and \"New York City\" as a location.\n",
    "\n",
    "* **Chunking:** Chunking involves identifying and labeling syntactic constituents, also known as chunks, in a sentence. These chunks often correspond to phrases such as noun phrases (NP), verb phrases (VP), or prepositional phrases (PP). For example, in the sentence \"She saw a beautiful sunset,\" a chunker would label \"a beautiful sunset\" as an NP.\n",
    "\n",
    "* **Semantic Role Labeling (SRL):** SRL aims to identify the roles played by different entities in a sentence with respect to a specific predicate. It assigns labels such as \"agent,\" \"patient,\" \"theme,\" and others to indicate the semantic roles. For instance, in the sentence \"John ate an apple,\" the SRL system would label \"John\" as the agent and \"an apple\" as the patient.\n",
    "\n",
    "* **Sentiment Analysis:** Sentiment analysis involves determining the sentiment or opinion expressed in a piece of text. In some cases, sentiment analysis can be treated as a sequence labeling task, where sentiment labels (e.g., positive, negative, neutral) are assigned to each word or sentence. This can be useful in analyzing customer reviews, social media posts, or product descriptions.\n",
    "\n",
    "This family of sequence labeling tasks can be solved using various techniques, ranging from rule-based approaches to statistical and machine learning methods. Rule-based approaches rely on handcrafted linguistic rules and dictionaries, whereas statistical methods use probabilistic models trained on large annotated corpora. More recently, deep learning models, particularly recurrent neural networks (RNNs) and transformer models, have achieved state-of-the-art performance in sequence labeling by effectively capturing the contextual information and long-range dependencies within a sentence.\n",
    "\n",
    "In this notebook, we look at a core technique for sequence labeling based on statistican machine learning: **Hidden Markov Models (HMM)**. We implement our own POS tagger from scratch by training a Hidden Markov Model (HMM) and implementing the Viterbi algorithm for decoding.\n",
    "\n",
    "\n",
    "## Quick Recap: POS Tagging\n",
    "\n",
    "Part-of-speech (POS) tagging is a fundamental task in natural language processing (NLP) that involves assigning a grammatical category or part of speech to each word in a given text. It is a crucial step in many NLP applications, such as machine translation, information retrieval, and sentiment analysis. POS tagging helps in understanding the syntactic structure of a sentence and provides valuable information for subsequent analysis and interpretation.\n",
    "\n",
    "The goal of POS tagging is to label each word in a sentence with its corresponding part of speech, such as noun, verb, adjective, adverb, preposition, conjunction, and so on. These labels capture the lexical and grammatical properties of words and enable a deeper understanding of the text's meaning and structure. POS tagging algorithms utilize linguistic features, contextual clues, and statistical patterns to determine the most likely part of speech for each word, taking into account the surrounding words and the overall context.\n",
    "\n",
    "Accurate POS tagging is essential for many downstream NLP tasks, as it serves as a crucial preprocessing step. By providing a fine-grained analysis of the grammatical structure of text, POS tagging facilitates more sophisticated language understanding and enables the development of advanced NLP applications.\n",
    "\n",
    "## Hidden Markov Models\n",
    "\n",
    "A Hidden Markov Model (HMM) is a statistical model used to describe and analyze sequential data, particularly data with temporal dependencies. It is a type of generative model that consists of a set of states, observed symbols, and transition probabilities between states. The key idea behind an HMM is that the underlying state of the system is hidden or unobserved, while only the emitted symbols or observations are visible. In the context of POS tagging, the states of the systems are the POS tags, and the emitted symbols are the words in our sentence/sequence.\n",
    "\n",
    "Recall the example from slides -- see figure below -- where assumed the existence of 3 states (i.e., POS tags: singular or mass noun (NN), non-3rd person singular present verb (VBP), personal pronoun (PRP) -- using the Penn Treebank tag set). The emitted or observed symbols are the words of the sentence *\"I like NLP\"*. The goal of training and using an HMM is to find the most likely sequences of tags given these sentences, which should be: PRP VBP NN.\n",
    "\n",
    "<img src='data/images/hmm-pos-example.png' width='40%' />\n",
    "\n",
    "In an HMM, the states form a Markov chain, meaning that the probability of transitioning from one state to the next depends only on the current state. The transitions between states are governed by transition probabilities, which represent the likelihood of moving from one state to another. Additionally, each state can emit a symbol from a set of observable symbols, and the emission probabilities determine the likelihood of observing a particular symbol from a given state.\n",
    "\n",
    "The main goal of an HMM is to model the joint probability of the observed sequence of symbols and the underlying sequence of states. This can be useful for various tasks, such as sequence labeling, where the goal is to determine the most likely sequence of hidden states given the observed sequence of symbols. For POS tagging: How likely is a sequence of POS tags for a given sentence.\n",
    "\n",
    "HMMs are particularly useful when dealing with sequential data, where the current state depends on previous states and the observed symbols provide indirect information about the hidden states. Despite their simplicity, HMMs have been widely used and extended to more complex models, such as hidden semi-Markov models (HSMMs) and conditional random fields (CRFs), to capture more intricate dependencies and improve performance in various NLP and pattern recognition tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff23485",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da1528f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import treebank\n",
    "from nltk.corpus import conll2000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2925bec-367e-492e-82dc-924c1ef6e3fe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a40453a",
   "metadata": {},
   "source": [
    "## Toy Example from the Lecture\n",
    "\n",
    "We first look at the toy example that we used to walk through the Viterbi algorithm. In this example, we also considered only 3 states (i.e., POS tags). The image below shows a screenshot of the lecture slides describing the toy example\n",
    "\n",
    "<img src='data/images/hmm-toy-model.png' width='90%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f88c4c9",
   "metadata": {},
   "source": [
    "### \"Training\" the HMM\n",
    "\n",
    "Recall from the lecture, that an HMM is completely defined by the following 3 components:\n",
    "\n",
    "* **Transition matrix $A$:** The transition matrix represents the probabilities of transitioning from one hidden state to another. It is also known as the state transition matrix or the transition probability matrix. The transition matrix for an HMM is typically denoted by $A$ and has dimensions ($N\\times N$), where $N$ is the number of hidden states in the model.\n",
    "\n",
    "* **Emission matrix $B$:** The emission matrix, also known as the emission probability matrix or observation probability matrix, represents the probabilities of emitting observable symbols or observations from each hidden state. The emission matrix captures the relationship between the hidden states and the observed symbols. The emission matrix is typically denoted by $B$ and has dimensions ($N\\times M$), where $N$ is the number of hidden states and $M$ is the number of possible observable symbols.\n",
    "\n",
    "* **Start probabilities $\\pi$:** The start probabilities, also known as initial state probabilities or initial state distribution, represent the probabilities of starting the sequence in each hidden state. The start probabilities describe the likelihood of the HMM's initial state being in a particular hidden state before any observations are made. The start probabilities are typically represented by a vector, denoted as $\\pi$ of length $N$, where $N$ is the number of hidden states in the HMM.\n",
    "\n",
    "In our toy example, all 3 components were given to us, so we didn't need to train anything. We will see how to compute all these values given an annotated dataset when going through our real-world examples below. Here the focus is in simplicity and the algorithm. Right now, let's define $A$, $B$, and $\\pi$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c84b805",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([\n",
    "    [0.0, 0.8, 0.2],\n",
    "    [0.0, 0.5, 0.5],\n",
    "    [0.5, 0.5, 0.0]\n",
    "])\n",
    "\n",
    "# the, fans, love, show\n",
    "B = np.array([\n",
    "    [0.2, 0.00, 0.00, 0.00], # DT\n",
    "    [0.0, 0.05, 0.30, 0.10], # NN\n",
    "    [0.0, 0.25, 0.15, 0.30]  # VB\n",
    "])\n",
    "\n",
    "PI = np.array([0.8, 0.2, 0.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cd26b1",
   "metadata": {},
   "source": [
    "As we can index the values in $A$, $B$ and $\\pi$ only using integer indices, we need a few dictionary to map between \n",
    "* tags (which represent the hidden states) and their respective indices\n",
    "* words (which represent the observed variables) and their respective indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7604521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2index, index2tag = {'DT': 0, 'NN': 1, 'VB': 2}, {0: 'DT', 1: 'NN', 2: 'VB'}\n",
    "word2index, index2word = {'the': 0, 'fans': 1, 'love': 2, 'show': 3}, {0: 'the', 1: 'fans', 2: 'love', 3: 'show'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9de9311",
   "metadata": {},
   "source": [
    "### Viterbi Algorithm\n",
    "\n",
    "The Viterbi algorithm is a dynamic programming algorithm used to find the most likely sequence of hidden states in a Hidden Markov Model (HMM) given a sequence of observations or symbols. It is an efficient algorithm that provides an optimal solution to the decoding problem in HMMs. The goal of the Viterbi algorithm is to find the path of hidden states that maximizes the joint probability of the observed sequence and the corresponding hidden state sequence. This is often referred to as the Viterbi path or the most likely state sequence. For our toy example, this trellis looks as follows (taken from the lecture slides):\n",
    "\n",
    "<img src='data/images/hmm-toy-model-trellis-init.png'  width='90%' />\n",
    "\n",
    "The Viterbi algorithm works by iteratively calculating the most likely path up to each hidden state at each time step. It maintains a dynamic programming table, often called the Viterbi trellis or Viterbi matrix, which stores the partial probabilities and backpointers for each state at each time step.\n",
    "\n",
    "The steps of the Viterbi algorithm are as follows:\n",
    "\n",
    "* **Initialization:** Initialize the first column of the Viterbi trellis with the product of the start probabilities and the emission probabilities for the first observation.\n",
    "\n",
    "* **Recursion:** For each subsequent time step, calculate the maximum partial probability for each state by considering the maximum probability from the previous time step multiplied by the transition probabilities and the emission probabilities for the current observation. Store the maximum probability and the corresponding backpointer in the Viterbi trellis.\n",
    "\n",
    "* **Termination:** Once all time steps have been processed, find the final state with the highest probability in the last column of the Viterbi trellis. This represents the most likely ending state.\n",
    "\n",
    "* **Backtracking:** Starting from the most likely ending state, follow the backpointers in the Viterbi trellis to trace back the most likely state sequence.\n",
    "\n",
    "The resulting state sequence obtained through the Viterbi algorithm represents the most probable sequence of hidden states that explains the observed sequence of symbols. The method `viterbi()` below implements the Viterbi algorithm as covered in the lecture. The implementation is annotated and should be straightforward enough to understand the code and map it to iterative steps of the Viterbi algorithm visualized on the lecture slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084f44e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(tokens, A, B, PI):\n",
    "    N, T = A.shape[0], len(tokens)         # N = number of states; T = lenght of sequence\n",
    "    M = np.zeros((N, T))                   # Reflecting probabilties of trellis\n",
    "    BT = np.zeros((N, T), dtype=np.int16)  # For the Backtracking pointers\n",
    "    \n",
    "    #####################################################################################################\n",
    "    ### Handle initial state = start probabilities multiplies by repespective emission probabilities\n",
    "    for s in range(N):\n",
    "        M[s,0] = PI[s] * B[s, word2index[tokens[0]]]\n",
    "        \n",
    "    #####################################################################################################\n",
    "    ### Handle all transitions\n",
    "    \n",
    "    # Loop over all time steps\n",
    "    for t in range(1, T):\n",
    "        # Loop over all states\n",
    "        for s in range(N):\n",
    "            # Compute the transition probabilities from ALL states from previous time step\n",
    "            trans_probs = M[:,t-1] * A[:,s] * B[s,word2index[tokens[t]]]\n",
    "            # Find the index that reflects the path the highest transition probability\n",
    "            max_idx = np.argmax(trans_probs)\n",
    "            # Update the trellis matrix with the hights probability the current state and time step\n",
    "            M[s,t] = trans_probs[max_idx]\n",
    "            # Remember the index reflecting the highest probability in the backtracking matrix\n",
    "            BT[s,t] = max_idx\n",
    "\n",
    "    #####################################################################################################\n",
    "    ### Use back pointers to follow the path that lead to the max prob\n",
    "    state = np.argmax(M[:,-1])\n",
    "    state_sequence = []\n",
    "    for i in reversed(range(T)):\n",
    "        state_sequence.append(state)\n",
    "        state = BT[:,i][state]\n",
    "        \n",
    "    # We also return matrix M, but only to print it\n",
    "    return [ index2tag[idx] for idx in reversed(state_sequence) ], M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f185da9",
   "metadata": {},
   "source": [
    "For some additional explanation of the `viterbi()` method above, have a look at the figure below; again, directly taken from the lecture slides. This figure visualizes the computation of `trans_probs` where \n",
    "\n",
    "* `M[:,t-1]` represents the left column (i.e., the probabilities of all paths up to time stamp $i-1$) -- in the figure: $v_{t-1}(1)$, $v_{t-1}(2)$, ..., $v_{t-1}(N)$\n",
    "\n",
    "* `A[:,s]` represents the transition probabilities from *all* states at time stamp $i-1$ to state `s` at time stamp $t$ -- in the figure: $a_{1,s}$, $a_{2,s}$, ... $a_{1,N}$\n",
    "\n",
    "* `B[s,word2index[tokens[t]]]` represents the emission probability of word `word2index[tokens[t]]` given state $s$ -- in the figure: $b_s(o_t)$\n",
    "\n",
    "<img src='data/images/hmm-viterbi-visualized.png' />\n",
    "\n",
    "\n",
    "The blue arrow indicates the path that yields the highest probability for reaching $v_{t}(s)$. This corresponds to the line `max_idx = np.argmax(trans_probs)` in the code of the `viterbi()` method.\n",
    "\n",
    "With our method, we can now decode any sequence of tokens, including the example sequence from the lecture slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027d17fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_sequence, M = viterbi(['the', 'fans', 'love', 'the', 'show'], A, B, PI)\n",
    "\n",
    "print(decoded_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c631c6-8ecd-4f9c-b1b1-4cd94bd786dd",
   "metadata": {},
   "source": [
    "The result looks as expected as those are arguably the correct POS tags for each of the words in the sentence/sequence. For a more detailed inspection, we can also have a look at the trellis matrix `M`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a503634-6d78-4649-bbf1-9c73e9618ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c825bb84-3cb7-4c72-99a4-3a81d91952ff",
   "metadata": {},
   "source": [
    "The values in matrix `M` natrually reflect the completed trellis from the lecture slides:\n",
    "\n",
    "<img src='data/images/hmm-toy-model-trellis-final.png' width='90%' />\n",
    "\n",
    "The implementation of the Viterbi algorithm in method `viterbi()` together with the toy example from the lecture slides should help with a better understand of the algorithm. If some individual steps are still unclear, feel free to edit the `viterbi()` method, e.g., by inserting additional `print` statements and see how intermediate results are reflected in the figures visualizing the Viterbi algorithm (see above)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ba4fab-2232-470e-89e7-dd661703f6bf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6680d884",
   "metadata": {},
   "source": [
    "## Real-World Example\n",
    "\n",
    "In the toy example above, we focused on the Viterbi algorithm for decoding a input sequence (e.g., an input sentence) given a trained HMM. In the toy example, the HMM was given to us in terms of the 3 components: transition matrix $A$, emission matrix $B$, and start probabilities $\\pi$. In this section, we will actually train an HMM based on an annotated dataset. For this, we combine 3 annotated dataset provided by NLTK:\n",
    "\n",
    "* **Treebank Dataset:** The treebank dataset refers to a collection of parsed sentences represented as syntactic tree structures. It is a corpus specifically designed for training and evaluating parsers and other NLP tools. The English treebank dataset in NLTK includes the Penn Treebank, which is one of the most widely used treebank resources. It contains parsed sentences from various sources, such as newspaper articles, and covers a range of genres and topics.\n",
    "\n",
    "* **Brown Dataset:** The Brown corpus is one of the first and most influential general-purpose corpora of English text. The Brown corpus consists of samples of written English from various sources, covering a wide range of genres and topics. It was compiled in the 1960s at Brown University and contains over one million words of text. The corpus is divided into categories such as news, fiction, government, religion, sports, and more, providing a diverse representation of English language usage. Each text sample in the Brown corpus is tokenized into words and annotated with POS tags.\n",
    "\n",
    "* **CoNLL2000 Dataset:** The CoNLL 2000 dataset is a collection of annotated English language data commonly used for training and evaluating information extraction systems, particularly those related to named entity recognition and chunking. The CoNLL 2000 dataset is based on the data used in the CoNLL-2000 shared task, which was organized as part of the Conference on Computational Natural Language Learning (CoNLL) in the year 2000. The dataset consists of news articles from the Wall Street Journal (WSJ) section of the Penn Treebank corpus. Each sentence in the CoNLL 2000 dataset is annotated with POS tags, chunk tags, and named entity tags.\n",
    "\n",
    "While each tokens/words in all three datasets are annotated with POS tags, these 3 dataset (by default) use different tag sets, technically prohibiting us to simply combine the datasets into one. However, all 3 dataset are additionally annotated using the **Universal Part-of-Speech (POS) tag set**. The Universal POS tag set is a standardized set of part-of-speech tags that aims to provide a cross-linguistic and language-independent representation of word categories or syntactic roles in natural language processing (NLP) tasks.\n",
    "\n",
    "The Universal POS tag set was introduced as part of the [Universal Dependencies (UD) project](https://universaldependencies.org/), which seeks to create consistent and multilingual syntactic treebanks across different languages. The goal of the Universal POS tag set is to facilitate cross-linguistic comparisons and enable the development of language-independent NLP models and tools. The Universal POS tag set consists of a small number of coarse-grained and high-level categories that are applicable to a wide range of languages. The tag set includes the following labels:\n",
    "\n",
    "* **NOUN:** Nouns (common and proper)\n",
    "* **VERB:** Verbs (main and auxiliary)\n",
    "* **ADJ:** Adjectives\n",
    "* **ADV:** Adverbs\n",
    "* **PRON:** Pronouns\n",
    "* **DET:** Determiners\n",
    "* **ADP:** Adpositions (prepositions and postpositions)\n",
    "* **NUM:** Numerals\n",
    "* **CONJ:** Conjunctions\n",
    "* **PRT:** Particles or other function words\n",
    "* **. :** Punctuation marks\n",
    "* **X:** Other or undetermined\n",
    "\n",
    "This tag set provides a simplified and consistent representation of word categories across different languages, making it easier to develop cross-linguistic NLP models and perform comparative analyses. The Universal POS tag set is widely used in various NLP tasks, including part-of-speech tagging, syntactic parsing, and machine translation.\n",
    "\n",
    "### Prepare Training Dataset\n",
    "\n",
    "We first download the 3 datasets; or at least check if the datasets are already available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ea303c-5461-4f87-b706-564d358cb610",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('treebank')\n",
    "nltk.download('brown')\n",
    "nltk.download('conll2000')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0866bbc-85e8-4258-9380-f264fbf27b78",
   "metadata": {},
   "source": [
    "We can now extract the sentences together with their universal POS tags; we need to explicitly specify this! For easier use in the following, we also concatenate all 3 datasets into a single list and treat this list as a single dataset for training our HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80abb95-1ae2-4fbd-8113-5877a2c96009",
   "metadata": {},
   "outputs": [],
   "source": [
    "treebank_corpus = treebank.tagged_sents(tagset='universal')\n",
    "brown_corpus = brown.tagged_sents(tagset='universal')\n",
    "conll_corpus = conll2000.tagged_sents(tagset='universal')\n",
    "\n",
    "# Combine all 3 corpora\n",
    "tagged_sentences = list(treebank_corpus + brown_corpus + conll_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ea1918-2127-41d4-a28a-634838e190b2",
   "metadata": {},
   "source": [
    "It's always helpful to first have a look at the raw data. The code cell below shows the information for the first sentence. As you can see, each sentence is represented by a list of 2-tuples, where each tuples contains the token/word at position 0 and the universal POS tag at position 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f000b5-69ec-4666-879f-27f78f38b225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of the total number of sentences; needed later to calculate the start probabilities\n",
    "num_sent = len(tagged_sentences)\n",
    "\n",
    "print('Total number of sentences: {}\\n'.format(num_sent))\n",
    "\n",
    "print('Example -- output for the first sentence')\n",
    "tagged_sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59273d1",
   "metadata": {},
   "source": [
    "### Compute all Required Counts\n",
    "\n",
    "We saw that we can compute $A$, $B$, and $\\pi$ using Maximum Likelihood Estimation (MLE). The figure below is taken from the lecture slides and shows how the values for $A$, $B$, and $\\pi$ are calculated. \n",
    "\n",
    "<img src='data/images/hmm-training-mle.png' width='90%' />\n",
    "\n",
    "Let's first define a series of dictionaries to keep track of all required counts to compute the probabilities later captured by $A$, $B$, and $\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be96ff6-2a11-45c8-9f25-aa03ec32e0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sets to keep track of vocabulary V and tag set S\n",
    "S, V = set(), set()\n",
    "\n",
    "initial_state_counts     = defaultdict(int)  # Count(<S>s_i): Number of times a state s_i was the first state in a training sequence\n",
    "state_counts             = defaultdict(int)  # Count(s_i): Number of times a state s_i occured\n",
    "state_transition_counts  = defaultdict(int)  # Count(s_i, s_j): Number of times the transition from a state s_i to state s_j occurred\n",
    "observation_counts       = defaultdict(int)  # Count(v_k, s_i): Number of times seeing a token v_k in a state s_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f3c76f-fa39-4b96-9f82-3721f9589604",
   "metadata": {},
   "source": [
    "With these dictionaries initialized we now only need to go through all annotated sentences to calculate these counts. This is done in the code cell below. Again, the code is annotated and should be self-explanatory. Note that the whole code essentially just goes through the dataset and increases the respective counts in the dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c97719-1e1d-4fee-927f-c469712ef294",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in tqdm(tagged_sentences):\n",
    "    \n",
    "    # Get all tokens and tags\n",
    "    tokens = [ t[0].lower() for t in sent ]\n",
    "    tags = [ t[1] for t in sent ]\n",
    "    \n",
    "    # Update the set of tokens (i.e., observed variables) and set of tags (i.e., states)\n",
    "    V.update(set(tokens))\n",
    "    S.update(set(tags))\n",
    "    \n",
    "    # Increase the count for inital state (tag)\n",
    "    initial_state_counts[tags[0]] += 1    \n",
    "    \n",
    "    # Iterate over all tokens\n",
    "    for pos in range(len(tags)-1):\n",
    "        pred, succ = tags[pos], tags[pos+1]\n",
    "        # Increase the counter for state \"pred\"\n",
    "        state_counts[pred] += 1\n",
    "        # Increase the counter for transition from state \"pred\" to state \"succ\"\n",
    "        state_transition_counts[(pred, succ)] += 1\n",
    "\n",
    "    # Iterate over all tags\n",
    "    for pos in range(len(tags)):\n",
    "        state, obs = tags[pos], tokens[pos]\n",
    "        # Increase counter for observation \"obs\" given state \"state\"\n",
    "        observation_counts[(state, obs)] += 1    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0963323",
   "metadata": {},
   "source": [
    "Like we saw for the toy example, we again need a few dictionaries to map between words, tags and their respective indices. Here, we compute these dictionaries automatically, of course:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d2c380",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2index, index2tag = {}, {}\n",
    "word2index, index2word = {}, {}\n",
    "\n",
    "for idx, tag in enumerate(S):\n",
    "    tag2index[tag] = idx\n",
    "    index2tag[idx] = tag\n",
    "    \n",
    "for idx, word in enumerate(V):\n",
    "    word2index[word] = idx\n",
    "    index2word[idx] = word "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366b397e",
   "metadata": {},
   "source": [
    "### Training the HMM\n",
    "\n",
    "With all required counts calculated, we can now train our HMM by computing the transition matrix $A$, emission matrix $B$, and the start probabilities $\\pi$.\n",
    "\n",
    "#### Auxiliary Methods\n",
    "\n",
    "Let's first define a series of auxiliary methods to compute:\n",
    "\n",
    "* the transition probability for two given states (i.e., tags)\n",
    "* the emission probability for a given state (i.e., tag) and observation (i.e., words)\n",
    "* the initial state probability for a given state (i.e., tag)\n",
    "\n",
    "According to the MLE, we simply need our counts to compute all probabilities. Note that we return 0 if anything fails (e.g., in case of unknown words). In practice, techniques such as smoothing can be applied.\n",
    "\n",
    "These auxiliary methods are not really needed, but they make the code below easier to read, which is the focus of this notebook; we do not care about the efficiency of the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd423fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_probability(s1, s2):\n",
    "    try:\n",
    "        return state_transition_counts[(s1, s2)] / state_counts[s1]\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def get_emission_probability(s, o):\n",
    "    try:\n",
    "        return observation_counts[(s, o)] / state_counts[s]\n",
    "    except:\n",
    "        return 0.0\n",
    "    \n",
    "def get_initial_probability(s):\n",
    "    try:\n",
    "        return initial_state_counts[s] / num_sent\n",
    "    except:\n",
    "        return 0.0\n",
    "    \n",
    "# Some example outputs\n",
    "print(get_transition_probability('DET', 'ADJ'))\n",
    "print(get_emission_probability('DET', 'the'))\n",
    "print(get_initial_probability('DET'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c09c5ac-3aa5-4f00-85e5-d9174a9b876c",
   "metadata": {},
   "source": [
    "We have to read the output of the code cell above as follows:\n",
    "\n",
    "* ~23.3% of the time, a determiner is followed by an adjective\n",
    "\n",
    "* ~52.2% of the time, the determiner is the word \"the\"\n",
    "\n",
    "* ~21.5% of the time, a sentence starts with a determiner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3963cfc2",
   "metadata": {},
   "source": [
    "#### Computing $A$, $B$,  $\\pi$\n",
    "\n",
    "With all the required counts and the auxiliary methods in place, the code cell below finally computes the transition matrix $A$, emission matrix $B$, and the start probabilities $\\pi$. To this end, the code simply loops over each entry of the  two matrices $A$ and $B$, as well as of vector $\\pi$, and calculates the corresponding values using the auxiliary methods (which in turn utilize the various count values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a31040",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Transition matrix\n",
    "A = np.zeros((len(S), len(S)))\n",
    "\n",
    "for si in range(len(S)):\n",
    "    for sj in range(len(S)):\n",
    "        A[si,sj] = get_transition_probability(index2tag[si], index2tag[sj])\n",
    "        \n",
    "        \n",
    "# Emission matrix\n",
    "B = np.zeros((len(S), len(V)))\n",
    "\n",
    "for s in range(len(S)):\n",
    "    for v in range(len(V)):\n",
    "        B[s,v] = get_emission_probability(index2tag[s], index2word[v])\n",
    "        \n",
    "        \n",
    "# Initial state matrix\n",
    "PI = np.zeros((len(S),))\n",
    "\n",
    "for s in range(len(S)):\n",
    "    PI[s] = get_initial_probability(index2tag[s])        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc92da95-a284-42f4-ab4b-accc12fae1c5",
   "metadata": {},
   "source": [
    "We have now trained our HMM. While the matrices $A$ and $B$ are too large to print, we can have a look at the start probabilities -- particularly since we used the reduced set of universal POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0fcf0f-d90c-4e16-ad08-7edb7a695940",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in range(len(S)):\n",
    "    print('Start probability of state {}: {}'.format(index2tag[s], PI[s]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66f6339-6bcf-4594-8840-b4981428abbc",
   "metadata": {},
   "source": [
    "These start probabilities tell us, the sentences are most likely to start with a determiner (DET: ~21.5), followed by a noun (NOUN: ~17.1%) and pronoun (PRON: ~14.1%). This seems intuitive since English is generally a subject-verb-object (SVO) language, with sentences typically starting with the subject, and the subject of a sentence is typically a noun or noun phrase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a15a7c",
   "metadata": {},
   "source": [
    "### Decoding with Viterbi Algorithm\n",
    "\n",
    "Now that we have trained our HMM, we can again use our `viterbi()` method to decode a couple of example sentences. We already implemented the Viterbi algorithm above, so let's use it to decode some examples. We use the ones included in the lecture slides, but feel free to come up and try your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0d29a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_sequence, _ = viterbi(['the', 'fans', 'love', 'the', 'show'], A, B, PI)\n",
    "print(decoded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cd49f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_sequence, _ = viterbi(['the', 'fans', 'like', 'the', 'show'], A, B, PI)\n",
    "print(decoded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ad2966",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_sequence, _ = viterbi(['funny', 'movies', 'are', 'the', 'best'], A, B, PI)\n",
    "print(decoded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fbf30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_sequence, _ = viterbi(['i', 'like', 'watching', 'comedies'], A, B, PI)\n",
    "print(decoded_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9369b9c",
   "metadata": {},
   "source": [
    "As you can see, our POS tagger based on the Viterbi algorithm is not perfect. In the second, example, \"like\" is labeled as ADP (adposition: preposition or postposition) where it should be labeled as verb (VERB)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3483f36-6a03-4834-978c-bd4734232600",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996afb71-d188-4a38-a45b-1eb992014ac3",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Hidden Markov Models (HMMs) have been widely used for part-of-speech (POS) tagging, which is the process of assigning the appropriate part-of-speech tags to each word in a sentence. HMMs provide a probabilistic framework for modeling the sequence of POS tags given the corresponding sequence of words in a sentence. In HMM-based POS tagging, the POS tags are considered as hidden states, and the observed words are treated as emissions from those hidden states. The underlying assumption is that the POS tags influence the choice of words in a sentence. By utilizing the probabilistic nature of HMMs, POS tagging can be formulated as a sequence labeling problem, where the goal is to find the most likely sequence of POS tags given the observed words.\n",
    "\n",
    "To train an HMM-based POS tagger, a labeled training corpus is used to estimate the probabilities of transitioning from one POS tag to another and the probabilities of emitting each word from a given POS tag. These probabilities are typically estimated using maximum likelihood estimation or other statistical techniques. During the tagging process, the Viterbi algorithm is commonly employed to find the most probable sequence of POS tags for a given sentence. The algorithm efficiently computes the most likely sequence by considering the transition probabilities between POS tags and the emission probabilities of words from the corresponding POS tags.\n",
    "\n",
    "HMM-based POS taggers have been successful in various NLP applications and research areas. They are particularly useful when the context and surrounding words play a significant role in determining the correct POS tags. However, HMM-based approaches may struggle with handling ambiguous words and capturing long-range dependencies. Over time, more advanced and neural network-based approaches, such as recurrent neural networks (RNNs) and transformer models, have gained popularity for POS tagging due to their ability to capture complex patterns and dependencies in language. Nonetheless, HMMs have provided a solid foundation for POS tagging and continue to serve as a reference point in the field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6161e391-4a35-4161-8cc8-5dfc533daef3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs5246",
   "language": "python",
   "name": "cs5246"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
