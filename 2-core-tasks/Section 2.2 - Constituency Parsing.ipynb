{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd2903b6-284e-4e05-a47f-3de6b3dcb79c",
   "metadata": {},
   "source": [
    "<img src='data/images/section-notebook-header.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123c0f60-0b97-4fe1-a949-8fbc9a1f25ea",
   "metadata": {},
   "source": [
    "# Constituency Parsing\n",
    "\n",
    "Constituency parsing, also known as syntactic parsing or phrase-structure parsing, is an NLP technique that analyzes the grammatical structure of a sentence. It aims to determine the syntactic relationships between words in a sentence and represents them in a hierarchical structure called a parse tree or a constituency tree.\n",
    "\n",
    "In constituency parsing, a sentence is divided into constituent phrases, which are groups of words that function together as a single unit within the sentence. The parse tree represents the hierarchical structure of these constituents, where the root of the tree represents the entire sentence, and the branches and leaves represent the constituents and words, respectively.\n",
    "\n",
    "The parse tree is typically generated using a formal grammar such as a context-free grammar (CFG) or a phrase-structure grammar. The parsing algorithm analyzes the sentence by applying grammar rules and assigning labels to the constituents based on their syntactic roles, such as noun phrases, verb phrases, prepositional phrases, and so on.\n",
    "\n",
    "Constituency parsing can provide valuable information about the sentence's structure, which can be useful in various NLP applications such as machine translation, information extraction, sentiment analysis, and question answering. It helps in understanding the relationships between words and provides a foundation for further semantic analysis and understanding of natural language text.\n",
    "\n",
    "In the notebook about the CYK algorithm, we focused on this important algorithm to perform constituency parsing. In this notebook, we look at how we can use off-the-shelf tools to perform constuency parsing, as well as consider a simple application that utilizes resulting parse trees. More specifically, use the information about the sentence structure to remove specific parts of the sentcence while maintaining it gramatical correctness.\n",
    "\n",
    "**Disclaimer:** Getting this notebook to run properly might by a bit tricky because of 2 issues:\n",
    "\n",
    "* spaCy does not support constituency parsing out of the box. You therefore need to install the aftermarket component [Berkeley Neural Parser](https://spacy.io/universe/project/self-attentive-parser); this should be straightforward but might first result in some errors that need to be fixed. While NLTK performs constituency parsing, it is good to have a working solution with spaCy as well, as it is not unlikely that you work more with spaCy in practice.\n",
    "\n",
    "* Since spaCy has no native in-built constituency parse it does not provide a suitable [visualizer](https://spacy.io/usage/visualizers). While this is not needed for subsequent processing steps, visualizing constituency parse trees can be useful when creating figures of a report or presentation slides. So a lot of the code below is actually only to draw the parse trees in a Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a81f21-f97d-4556-8896-524a3b503ee9",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Import Required Packages\n",
    "\n",
    "The code cell below was only needed for the drawing of the constituency parse trees, as this notebook was created on a remote server. Without this cell, trying to draw a parse tree (see below) will yield the error `TclError: no display name and no $DISPLAY environment variable` (see [here](https://stackoverflow.com/questions/37604289/tkinter-tclerror-no-display-name-and-no-display-environment-variable) or [here](https://github.com/jupyterlab/jupyterlab/issues/9660)).\n",
    "\n",
    "If you run this notebook on your local machine, this should not be necessary. But again, if you can't get this to work, no worries as this is purely for visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78ea63b7-cd98-4b40-ad01-b66356cbdd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.system('Xvfb :1 -screen 0 1600x1200x16  &')    # create virtual display with size 1600x1200 and 16 bit color. Color can be changed to 24 or 8\n",
    "os.environ['DISPLAY']=':1.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f66ef8b-f72d-4f5b-9f4b-c576758e12a1",
   "metadata": {},
   "source": [
    "Ignoring the visualization of parse tree, we only need `spacy` and `benepar` (as the component to allow spaCy to perform constituency parsing).\n",
    "\n",
    "Benepar for SpaCy is an integration of the Berkeley Neural Parser (Benepar) with the SpaCy library. Benepar is a state-of-the-art constituency parsing model developed by researchers at the University of California, Berkeley. It provides accurate and efficient parsing of sentences into syntactic parse trees.\n",
    "\n",
    "The integration of Benepar with SpaCy allows users to leverage the power of Benepar's parsing capabilities within the SpaCy NLP framework. SpaCy is a popular open-source library for natural language processing that provides various NLP functionalities, including tokenization, part-of-speech tagging, named entity recognition, and dependency parsing. With the Benepar for SpaCy integration, you can perform constituency parsing using Benepar's neural model seamlessly within the SpaCy pipeline. It allows you to generate parse trees that represent the hierarchical structure of sentences, capturing the syntactic relationships between words and phrases.\n",
    "\n",
    "By incorporating Benepar into SpaCy, you can benefit from the efficiency and ease of use provided by SpaCy while obtaining high-quality constituency parsing results from Benepar's state-of-the-art model. This integration allows for enhanced syntactic analysis and can be useful in a wide range of NLP applications, including text understanding, information extraction, and semantic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "516859ad-f0c9-4c6d-91fd-6638394587d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/home/vdw/env/anaconda3/envs/cs5246/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so: undefined symbol: ncclRedOpDestroy",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbenepar\u001b[39;00m\n",
      "File \u001b[0;32m~/env/anaconda3/envs/cs5246/lib/python3.9/site-packages/benepar/__init__.py:19\u001b[0m\n\u001b[1;32m     10\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInputSentence\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNonConstituentException\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m ]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnltk_plugin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Parser, InputSentence\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspacy_plugin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeneparComponent, NonConstituentException\n",
      "File \u001b[0;32m~/env/anaconda3/envs/cs5246/lib/python3.9/site-packages/benepar/integrations/nltk_plugin.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Optional, Tuple\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_trained_model\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseParser, BaseInputExample\n",
      "File \u001b[0;32m~/env/anaconda3/envs/cs5246/lib/python3.9/site-packages/torch/__init__.py:229\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[1;32m    228\u001b[0m         _load_global_deps()\n\u001b[0;32m--> 229\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# Appease the type checker; ordinarily this binding is inserted by the\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# torch._C module initialization code in C\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[0;31mImportError\u001b[0m: /home/vdw/env/anaconda3/envs/cs5246/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so: undefined symbol: ncclRedOpDestroy"
     ]
    }
   ],
   "source": [
    "import benepar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f89032fd-0544-457e-8f25-a14133ee6319",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/home/vdw/env/anaconda3/envs/cs5246/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so: undefined symbol: ncclRedOpDestroy",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbenepar\u001b[39;00m\n",
      "File \u001b[0;32m~/env/anaconda3/envs/cs5246/lib/python3.9/site-packages/benepar/__init__.py:19\u001b[0m\n\u001b[1;32m     10\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInputSentence\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNonConstituentException\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m ]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnltk_plugin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Parser, InputSentence\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspacy_plugin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeneparComponent, NonConstituentException\n",
      "File \u001b[0;32m~/env/anaconda3/envs/cs5246/lib/python3.9/site-packages/benepar/integrations/nltk_plugin.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Optional, Tuple\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_trained_model\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseParser, BaseInputExample\n",
      "File \u001b[0;32m~/env/anaconda3/envs/cs5246/lib/python3.9/site-packages/torch/__init__.py:229\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[1;32m    228\u001b[0m         _load_global_deps()\n\u001b[0;32m--> 229\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# Appease the type checker; ordinarily this binding is inserted by the\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# torch._C module initialization code in C\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[0;31mImportError\u001b[0m: /home/vdw/env/anaconda3/envs/cs5246/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so: undefined symbol: ncclRedOpDestroy"
     ]
    }
   ],
   "source": [
    "import spacy, benepar\n",
    "\n",
    "# Load language model\n",
    "#nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# Add constuency parser to pipeline (assume a recent version of spaCy)\n",
    "#nlp.add_pipe('benepar', config={'model': 'benepar_en3'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304d6cb7-0168-4341-a684-d61ab042c00f",
   "metadata": {},
   "source": [
    "While spaCy does the actual parsing, we need some components from NLTK but purely for visualizing the parse trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39015d49-69a2-439f-b3ea-b5db2dd0b57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tree import Tree\n",
    "from nltk.draw import TreeWidget\n",
    "from nltk.draw.util import CanvasFrame\n",
    "\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459b003f-5e9d-43c7-8bd3-2ba2208bf676",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3990b5-68f4-4d2b-bbbc-0ca2d700bbe3",
   "metadata": {},
   "source": [
    "## Perform Constituency Parsing\n",
    "\n",
    "With `benepar` constituency parser as a component of spaCy, getting a parse tree -- or multiple parse trees in case of multiple sentences -- is very easy. Simply speaking, we use spaCy the same way as usual, with the only difference that the analyzed document now also contains information about the constituency parse. In the code cell below, we provide a series of example sentences, but you are of course welcome to try your own sentences an inspect the resulting parse trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b2d3e0-8f91-4991-a9ba-d5186e4737ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Chris likes eating burger.\"\n",
    "#text = \"I saw the man with a telescope.\"\n",
    "#text = \"I stayed at the Kempinsky Hotel with Alice.\"\n",
    "#text = \"The castle in Camelot remained the residence of the king until 536 when he moved it to London\"\n",
    "#text = \"Basel was the birthplace of Leonhard Euler, but St. Petersburg was where he died\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Let's consider only the first sentence (in case there are more than 1 sentence)\n",
    "sentence = list(doc.sents)[0]\n",
    "\n",
    "# The parse string is some standardized way to \"verbalize\" parse trees using a nested notation\n",
    "print(sentence._.parse_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696ef25b-d2e6-42ef-88dc-96421b589959",
   "metadata": {},
   "source": [
    "## Visualization of Parse Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe44b24b-4e36-4048-adda-06ef2d05bc8d",
   "metadata": {},
   "source": [
    "We can now use this parse string to convert it into an internal tree representation of NLTK, which we only need for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387fbaf3-415b-4a60-af47-043a944f3376",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = Tree.fromstring(sentence._.parse_string)\n",
    "\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1353be-c6d7-476d-a4a1-d1fd83f43d36",
   "metadata": {},
   "source": [
    "The method below does all the magic to visualize a constituency parse tree within a Jupyter notebook. The required core components are `CanvasFrame` and `TreeWidget`, which indeed cause all the hassle to get this to work. Notice how the frame first gets exported to a `.ps` file, then converted into a `.png` file, which is finally displayed in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833d1c0a-a10d-484e-8039-83d85a6856d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jupyter_draw_nltk_tree(tree, height=40, width=60):\n",
    "    cf = CanvasFrame()\n",
    "    tc = TreeWidget(cf.canvas(), tree)\n",
    "    tc['node_font'] = 'arial 26 bold'\n",
    "    tc['leaf_font'] = 'arial 26'\n",
    "    tc['node_color'] = '#003D7C'\n",
    "    tc['leaf_color'] = '#CC0000'\n",
    "    tc['line_color'] = '#175252'\n",
    "    tc['xspace'] = width\n",
    "    tc['yspace'] = height\n",
    "    cf.add_widget(tc, 1, 1)\n",
    "    cf.print_to_file('data/tmp_tree_output.ps')\n",
    "    cf.destroy()\n",
    "    os.system('convert data/tmp_tree_output.ps data/tmp_tree_output.png')\n",
    "    display(Image(filename='data/tmp/tmp_tree_output.png'))\n",
    "    #os.system('rm tmp_tree_output.ps tmp_tree_output.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c370fbab-887d-4be4-8f83-864b0a04f130",
   "metadata": {},
   "source": [
    "Well, let's try drawing our constituency parse tree. As mentioned above, in practice you generally use the parse tree for further downstream task and not to only visualize it. So if this method causes any errors and does not work as intended, no big deal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ee6c64-8e1a-46e7-be90-e058043edee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyter_draw_nltk_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586c993f-4d57-4122-b1c0-c389f344f010",
   "metadata": {},
   "source": [
    "If you see a parse tree above and no errors, great :). If not, it doesn't really matter either.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984c8d6e-185a-4d88-bc48-d91dc867c8cb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a08373-7c8d-4048-8efb-0eec7f450a3b",
   "metadata": {},
   "source": [
    "## Application: Text Obfuscation\n",
    "\n",
    "Text obfuscation, also known as text anonymization or text de-identification, is the process of modifying or transforming text data to conceal or remove sensitive or personally identifiable information (PII). The purpose of text obfuscation is to protect the privacy and confidentiality of individuals mentioned in the text while preserving the utility and usefulness of the data for analysis or sharing.\n",
    "\n",
    "Text obfuscation techniques aim to replace sensitive information with non-sensitive or generic placeholders or to modify the text in a way that makes it difficult to identify individuals or extract sensitive information. Common types of sensitive information that may be obfuscated include names, addresses, phone numbers, email addresses, social security numbers, credit card numbers, and other personally identifiable details.\n",
    "\n",
    "There are several methods and strategies for text obfuscation, depending on the specific requirements and context. Some commonly used techniques include:\n",
    "\n",
    "* **Masking:** Replacing sensitive information with generic placeholders or masks. For example, replacing names with pseudonyms or using \"XXX\" to hide digits of a credit card number.\n",
    "\n",
    "* **Tokenization:** Replacing sensitive words or phrases with generic tokens or categories. For example, replacing names with \"PERSON\" or locations with \"LOCATION.\"\n",
    "\n",
    "* **Generalization:** Replacing specific values with more general or less precise ones. For example, replacing an exact age with an age range, such as \"30-40 years old.\"\n",
    "\n",
    "* **Perturbation:** Introducing controlled noise or randomization to modify the original data while preserving its statistical properties. This technique is commonly used in privacy-preserving data mining.\n",
    "\n",
    "* **Sentence-level obfuscation:** Rewriting or paraphrasing sentences to retain the meaning but remove specific details or sensitive information.\n",
    "\n",
    "Text obfuscation is crucial when handling sensitive data to ensure compliance with privacy regulations and protect the privacy of individuals involved. It allows organizations to share or analyze text data without the risk of exposing sensitive information, enabling data-driven insights while maintaining confidentiality.\n",
    "\n",
    "In the following, we look at sentence-level obfuscation. Let's assume the example task from the lecture where we want to remove some snippets from a sentence. The motivation was that some parts of a sentence might reveal sensitive information that we want to hide. We use the same example sentence from the lecture *\"I stayed at the Kempinski Hotel with Alice.\"* and we want to remove the information about the Kempinski Hotel. First, let's parse this sentence and have a look at the parse tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221fc4a6-76d5-4f29-a440-e6a31d846bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I stayed at the Kempinsky Hotel with Alice.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Let's consider only the first sentence (in case there are more than 1 sentence)\n",
    "sentence = list(doc.sents)[0]\n",
    "\n",
    "# The parse string is some standardized way to \"verbalize\" parse trees using a nested notation\n",
    "print(sentence._.parse_string)\n",
    "\n",
    "# Convert to NLTK representation\n",
    "tree = Tree.fromstring(sentence._.parse_string)\n",
    "\n",
    "# Draw parse tree\n",
    "jupyter_draw_nltk_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c98a9b-bb93-4ae9-8947-fe05c61e8d27",
   "metadata": {},
   "source": [
    "It is obvious that we cannot simply remove *\"Kempinski\"* or *\"Kempinski Hotel\"* since this would yield a sentence that is no longer well-formed. In the lecture, we made the argument that we need to remove complete constituents to ensure proper sentences. However, we cannot simply remove any constituent containing our target words. For example, removing the noun phrase (NP) *\"the Kempinski Hotel\"* would yield *\"I stayed at with Alive\"* which is obviously not a proper sentence. The reasons for this is that this NP *\"the Kempinski Hotel\"* is part of a prepositional phrase (PP) *\"at the Kempinski Hotel\"*.\n",
    "\n",
    "In short, in practice, it is not trivial which constituent to remove. There are 2 goals:\n",
    "* The constituent should be a small as possible, i.e., we only want to remove is little as possible\n",
    "* After the removal we need to guarantee a well-formed sentence (even if it will sound a bit odd, at least it will be grammatically correct).\n",
    "\n",
    "To keep it simple, let's assume that we can always safely remove prepositional phrases; this will be good enough for our example here.\n",
    "\n",
    "### Understanding the Parser Output\n",
    "\n",
    "If you check the [Benepar Docs](https://pypi.org/project/benepar/), you can see that we can loop over all constituents as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba40cd4-3c41-41a0-8b77-d113d98ac439",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word = \"Kempinski\"\n",
    "\n",
    "# Loop over all consituents\n",
    "for span in sentence._.constituents:\n",
    "    \n",
    "    # Extract the individual words of a consituent (for filtering)\n",
    "    words = [ t.text for t in span]\n",
    "    \n",
    "    # Let's focus only on the constituents that contain our target word \"Kempinsky\" \n",
    "    if target_word in words:\n",
    "        print(span, span._.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1140c76f-a94d-4557-b1a2-bb0ad9734545",
   "metadata": {},
   "source": [
    "This gives us all the constituents that contain our target word *\"Kempinski\"* at all levels of the parse tree. Given our assumption, we know that the can remove any prepositional phrase (PP) to ensure proper sentences. While this example contains only one PP with our target word, in general, there could be more. So let's fetch them all and select the shortest one. We only need to change the code cell above a bit to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb3e0d6-2d7d-46a7-baca-6dafc3e905ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word = \"Kempinsky\"\n",
    "\n",
    "candidates_spans = set()\n",
    "\n",
    "# Loop over all consituents\n",
    "for span in sentence._.constituents:\n",
    "    \n",
    "    # Extract the individual words of a consituent (for filtering)\n",
    "    words = [ t.text for t in span]\n",
    "    \n",
    "    # Let's focus only on the constituents that contain our target word \"Kempinsky\" \n",
    "    if target_word in words:\n",
    "        try:\n",
    "            if span._.labels[0] == 'PP':\n",
    "                candidates_spans.add(span)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "# In case of multiple constuents, pick the shortest one (Python makes such things very easy)\n",
    "shortest_candidate_span = min(candidates_spans, key=len)\n",
    "\n",
    "print(\"Shortest PP containg target word '{}': {}\".format(target_word, shortest_candidate_span))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203ebdf8-555e-42b9-8a7e-332699197eb2",
   "metadata": {},
   "source": [
    "### Removing the Selected Constituent\n",
    "\n",
    "Now that we know which constituents we want to remove, we can do so by identifying the indices of all tokens that are part of this constituent. Lastly, we can remove all the tokens associated with this indices to remove the prepositional phrase. Note that we first create the obfuscated sentence as a string and then analyze this string again using spaCy. In principle, we could directly modify the spaCy object for the original sentence, but a re-analysis of the new sentence is \"cleaner\" as it incorporates all possible changes in the analysis. At the very least, it ensures that all word/tokens have consecutive indices again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dab142-1481-4904-ab4a-abc85ec13311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract indices of the tokens that are part of the constituent we want to remove\n",
    "obsolete_indices = [ token.i for token in shortest_candidate_span ]\n",
    "\n",
    "# Create a new sentence from the original sentence but ignoring the obsolete indicies\n",
    "sentence_obfuscated = ' '.join([ token.text for token in doc if token.i not in obsolete_indices ])\n",
    "\n",
    "# Re-analyze the new sentence with spaCy\n",
    "doc_obfuscated = nlp(sentence_obfuscated)\n",
    "\n",
    "# Print the obfuscated sentence\n",
    "print(doc_obfuscated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2148fd20-2fac-4e3d-b6bb-3cfcf4dd482e",
   "metadata": {},
   "source": [
    "As you can see, the resulting sentences is still grammatically correct but no longer contains the part we considered as sensitive information. Of course, if we would furher consider *\"Alice\"* as senstive information as well, we could apply the same process again and end up with the sentence *\"I stayed\"*. While still grammatically correct, there naturally will come a time when removing more and more parts from a sentence will make it meaningless."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b08b182-881d-4453-8d04-76d882ef3fe2",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "The example of sentence-level obfuscation to remove snesitive parts from a sentence was of course very simplified. The focus was on the motivation and showcasing a simple example to get the basic idea of how to use constituency parsing for this task. For a practical application, there a several other important considerations:\n",
    "\n",
    "* We assumed that we can always safely remove prepositional phrases (PP) which might not be the case in all circumstances. In general, the question is what are the rules that state which constituents can be removed to ensure proper sentences.\n",
    "\n",
    "* Here, we only removed constituents. However, this can often lead to sentences that, although grammatically correct, may read a bit odd. This also means that it can be easier to spot that a sentence has been obfuscated. A more sophisticated approach would be to replace constituents with an alternative. From example, we could generate a sentence \"*I stayed in a hotel in Singapore with Alice*\" by utilizing some background knowledge that *\"Kempinski Hotel\"* in this sentence is referring to the hotel with that name in Singapore.\n",
    "\n",
    "* When it comes to replacing constituents, we can easily swap constituents of the same type and preserve the grammatical correctness. For example, we can replace a PP with any other PP and get a proper sentence. Of course, the meaning might be weird. So while *\"I stayed beyond the scope of the lecture with Alice\"* is a grammatically correct sentence, it arguably does not make sense.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bf5460-3373-43e4-9182-c5f5d3da109e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1988a9a7-9253-49ca-8388-41a3a2fa8c68",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Constituency parsing is a natural language processing technique that analyzes the grammatical structure of a sentence by determining the syntactic relationships between words. It involves dividing a sentence into constituent phrases and representing them in a hierarchical structure called a parse tree. The root of the tree represents the entire sentence, while the branches and leaves represent the constituents and words, respectively.\n",
    "\n",
    "Constituency parsing has several practical uses in various NLP applications. Here are some key applications:\n",
    "\n",
    "* **Syntax Analysis:** Constituency parsing helps in understanding the syntactic structure of a sentence by identifying noun phrases, verb phrases, prepositional phrases, and other constituents. This information is valuable for tasks such as grammar checking, syntactic analysis, and language understanding.\n",
    "\n",
    "* **Machine Translation:** Constituency parsing aids in improving the accuracy of machine translation systems. By understanding the syntactic structure of the source and target sentences, parsers can help identify corresponding phrases and improve alignment, resulting in more accurate translations.\n",
    "\n",
    "* **Information Extraction:** Constituency parsing can assist in extracting structured information from unstructured text. By identifying the syntactic constituents, such as subject-verb-object relationships or nested phrases, it becomes easier to extract relevant information, such as entities, relations, and events.\n",
    "\n",
    "* **Sentiment Analysis:** Analyzing the syntactic structure of sentences through constituency parsing can enhance sentiment analysis. By identifying the sentiment-bearing constituents, such as subject or verb phrases, parsers can provide finer-grained sentiment analysis and improve the overall accuracy of sentiment classification.\n",
    "\n",
    "* **Question Answering:** Constituency parsing helps in understanding the structure of questions and mapping them to corresponding answers. By analyzing the question constituents and their relationships, parsers can aid in identifying relevant information and generate accurate responses.\n",
    "\n",
    "* **Summarization and Text Generation:** Constituency parsing can be useful in text summarization and generation tasks. By understanding the sentence structure, parsers can identify key phrases or clauses, which can guide the selection of important information for summarization or assist in generating coherent and grammatically correct text.\n",
    "\n",
    "Constituency parsing is a fundamental technique in NLP that provides insights into the syntactic structure of sentences. Its practical applications span across various domains, enabling improved language understanding, information extraction, machine translation, sentiment analysis, question answering, and text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f869db2-7a90-43e4-b9de-c9a8fe232410",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs5246]",
   "language": "python",
   "name": "conda-env-cs5246-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
