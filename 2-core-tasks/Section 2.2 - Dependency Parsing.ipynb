{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e0d0ac5-4bc6-4bf2-b678-21fa1b90bc36",
   "metadata": {},
   "source": [
    "<img src='data/images/section-notebook-header.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb86bbf-81c8-46ba-ba18-3e673124d0d5",
   "metadata": {},
   "source": [
    "**Disclaimer:** The topic of dependency parsing is not part of the syllabus of the course \"Natural Language Processing: Core Task\". However, we provide this notebook as dependency parsing can be very useful in practice and, like syntactic parsing, analyzes the hierarchical structure of sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a702b1-2a8c-432f-8c38-3bd9d9019eb7",
   "metadata": {},
   "source": [
    "# Dependency Parsing\n",
    "\n",
    "Dependency parsing is a natural language processing (NLP) technique that analyzes the grammatical structure of a sentence by identifying the syntactic relationships between words. It aims to determine the dependency relationships between words, representing them as a directed graph called a dependency tree.\n",
    "\n",
    "In dependency parsing, each word in a sentence is considered as a node, and the relationships between the words are represented as directed edges. The root of the dependency tree typically represents the main verb or the main predicate of the sentence, while other words depend on this root word or on other words in the sentence. These dependencies capture the grammatical relationships such as subject-verb, verb-object, and modifier relationships.\n",
    "\n",
    "The most common representation used in dependency parsing is the Universal Dependencies (UD) framework, which provides a standardized set of dependency labels to describe the grammatical relationships. Examples of dependency labels include \"nsubj\" for a nominal subject, \"obj\" for a direct object, \"amod\" for an adjective modifier, and so on.\n",
    "\n",
    "Dependency parsing can be performed using different algorithms and models, such as rule-based approaches, statistical models, and neural network-based models. These models learn from annotated data to predict the dependency structure of a sentence. Once the dependency parsing is performed, the resulting dependency tree can be used for various downstream NLP tasks, such as information extraction, question answering, machine translation, and sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b7571c-2b2b-440a-be81-6ce1dbe2837b",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Import Required Packages\n",
    "\n",
    "We only need spaCy for this notebook as it provides dependency parsing out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a69ab4-95eb-4562-995c-e751cf54deb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2036547-d60e-4f17-b7c7-8102a685c640",
   "metadata": {},
   "source": [
    "Load a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dca91d-0025-46f3-9ac9-d40a47b40954",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca6fbcf-0ac8-4dc1-92e4-b6f0677fd493",
   "metadata": {},
   "source": [
    "### Example Sentences\n",
    "\n",
    "Let's create some example sentences. Feel free to modify the sentences or add your own and inspect the results from the code cells below. Note that some comments refer to the first example sentence *\"I downloaded the lecture slide decks from the website.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f70408-efbc-47cb-94a5-d1f5725f678f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I downloaded the lecture slide decks from the website.\" \n",
    "#sentence = \"The burgers were great, but the waiting time was too long\"\n",
    "#sentence = \"Leonhard Euler was born on 15 April 1707, in Basel, Switzerland.\"\n",
    "\n",
    "doc = nlp(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b7b967-118e-4756-b7d7-56cb5c9e5b37",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66260cf6-ddf3-41ae-8bd0-84bd3786c4e0",
   "metadata": {},
   "source": [
    "## Visualization of Dependency Trees\n",
    "\n",
    "`displacy` is a visualization module in the spaCy package. It is designed to provide an easy and intuitive way to visualize the dependency parse trees generated by spaCy's parser. It allows you to visualize dependency parse trees in a graphical format, making it easier to understand the syntactic structure of a sentence. It can be used to visualize the relationships between words and their dependencies, including the direction and type of the dependencies.\n",
    "\n",
    "Once you have a parsed document or sentence, you can pass it to the `displacy.render` function along with the appropriate options to generate a visual representation. The output can be displayed in a Jupyter notebook or saved as an image file. The visualization produced by `displacy` includes arrows connecting words to their dependencies, with different colors and labels indicating the types of dependencies. It also provides options for adjusting the visual style, such as changing the color scheme, word spacing, and background color. The complete list of options can be found on the [spaCy Visualizers](https://spacy.io/usage/visualizers) web page. Here we focus only on some basic options most useful in practice. Overall, `displacy` is a useful tool for exploring and understanding the syntactic structure of sentences and can aid in the analysis and debugging of NLP models that rely on dependency parsing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192b996c-d74b-45c6-8f7d-49d67b4642ed",
   "metadata": {},
   "source": [
    "### \"Full\" Dependency Tree\n",
    "\n",
    "The \"full\" dependency tree shows the relationships between all the tokens incl. punctuation marks. The latter are by default not explicitly shown (`\"collapse_punct\": True`). The example below, uses the default distance/spacing between tokens of `150`. Note that a dependency tree also shows the Part-of-Speech (POS) tag for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00d4113-ece4-42e2-9f25-936590dcb5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\"distance\": 120, \"collapse_phrases\": False, \"collapse_punct\": False}\n",
    "\n",
    "displacy.render(doc, style=\"dep\", options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2e9f4b-9b6e-4c6e-8709-d4a0c87c9908",
   "metadata": {},
   "source": [
    "### \"Full\" Dependency Tree with Reduced Distances\n",
    "\n",
    "To save some space to accommodate longer sentences, we can reduce the distance between two tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1609ff-314b-4f56-9bbb-2430f997a116",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\"distance\": 100, \"collapse_phrases\": False, \"collapse_punct\": False}\n",
    "\n",
    "displacy.render(doc, style=\"dep\", options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c4c81d-7b25-4c3a-9e22-162f2fa548b9",
   "metadata": {},
   "source": [
    "### Dependency Tree with Collapsed Punctuation Marks (default)\n",
    "\n",
    "Punctuation marks typically result in very long arcs that artificially blow up the dependency graph. Since punctuation marks are not interesting, we can plot the graph without their dependencies. Note that the punctuation marks are still shown by being attached to the token given by the input sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a520883e-80fa-4210-bd2b-0d845da68773",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\"distance\": 150, \"collapse_phrases\": False, \"collapse_punct\": True}\n",
    "\n",
    "displacy.render(doc, style=\"dep\", options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697cf8e0-1d26-48e9-b7d6-a0da37749cd1",
   "metadata": {},
   "source": [
    "### Dependency Tree with Collapsed Noun Phrases\n",
    "\n",
    "By default, spaCy performs **noun phrase chunking**. Noun chunks are phrases that consist of a noun and the words that directly modify or depend on it. They are a linguistic unit that groups together a noun and its associated words in a sentence. Noun chunks provide a way to identify and extract meaningful noun phrases from text.\n",
    "\n",
    "In English grammar, a noun chunk typically includes the noun itself and any determiners, adjectives, or other modifiers that directly precede or follow the noun. For example, in the sentence \"The big red apple fell from the tree,\" the noun chunk \"the big red apple\" includes the determiner \"the,\" the adjectives \"big\" and \"red,\" and the noun \"apple.\" Noun chunks are useful in natural language processing (NLP) tasks such as information extraction, text summarization, and named entity recognition. They can help identify and extract important noun phrases that carry significant semantic meaning in a sentence or document.\n",
    "\n",
    "In the spaCy library, noun chunks can be extracted using the noun_chunks attribute of a parsed document or sentence. It provides an iterator that yields noun chunks as Span objects, which can then be accessed and processed further in a program. This feature in spaCy makes it convenient to extract and work with noun phrases in NLP applications.\n",
    "\n",
    "Noun chunks can be explicitly listed as follows; see the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15329ad-3bbc-4055-8fa4-0b3d0b276180",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4c0674-4419-46c9-8c59-b367b2c14e15",
   "metadata": {},
   "source": [
    "We can utilize the information about noun chunks by treating them as a single leaf in the dependency tree. This is not done by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0fb0e9-e310-49fd-8af5-97ca5e1fa556",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\"distance\": 150, \"collapse_phrases\": True, \"collapse_punct\": True}\n",
    "\n",
    "displacy.render(doc, style=\"dep\", options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ff1453-fb0a-498d-9d96-6bca15635acf",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e23784-8c6c-4a6b-9504-f0995aa4ccb0",
   "metadata": {},
   "source": [
    "## Understanding Dependency Labels\n",
    "\n",
    "When looking at the dependency graphs above, we can identify the most important information: the edge/arc labels representing the dependency relationship between the words of a sentence. These labels are taken from a predefined set of dependency labels. To see the list of all available labels, together with a brief description, you can run the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b74071d-c7ae-4f74-99a2-0bcaae9b1706",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in nlp.get_pipe(\"parser\").labels:\n",
    "    print(label, \" -- \", spacy.explain(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8461bf46-8d86-47cd-8f4d-4c3df5fff06e",
   "metadata": {},
   "source": [
    "In principle, there is no single agreed upon set of dependency labels, and this set may also differ across languages. However, there is of course the effort to unify and standardize these labels as much as possible. [Universal Dependencies](https://universaldependencies.org/) (UD) is a framework for consistent annotation of grammar (parts of speech, morphological features, and syntactic dependencies) across different human languages. UD is an open community effort with over 300 contributors producing nearly 200 treebanks in over 100 languages. If you’re new to UD, you should start by reading the first part of the Short Introduction and then browsing the annotation guidelines.\n",
    "\n",
    "The currently defined dependency labels can be found [here](https://universaldependencies.org/u/dep/index.html). Note that the label set of spaCy does not fully match the list of universal dependencies, although the most important ones are of course available. Still, this is something to keep in mind in practice since different dependency parsers may yield different dependency trees if they use different labels set. Of course, even when using the same label set, dependency parsers may return different trees since parsers may rely on different methods/algorithms and/or training of different datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abf0c33-1721-4739-9ed9-e1bbf7646be4",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef501c81-60f6-44fe-8ca3-3c582a0e3660",
   "metadata": {},
   "source": [
    "## Navigating Dependency Tree\n",
    "\n",
    "To navigate the dependencies of a spaCy document, you can use the `Token` objects and their properties provided by the spaCy library. Each token represents a word in the document and contains information about its linguistic features, including its dependency relationships.\n",
    "\n",
    "By following the dependency relations and the head tokens of each token, you can navigate the entire dependency tree of the document. Additionally, you can use the token.children attribute to access the immediate children of a token in the dependency tree. This provides a way to explore the subtree rooted at a particular token. By utilizing these properties and attributes, you can effectively navigate and explore the dependencies within a spaCy document.\n",
    "\n",
    "The code cell below prints the children (i.e., the dependent tokens) of each token of our sentence. Tokens without dependent tokens will natrually yield an empty list of children."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3ee0d4-eb6b-4c41-b7c6-06878fcf5681",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(\"{} >>> children {}\".format(token.text, [child.text for child in token.children]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db509b1-2c22-42b0-8f54-d76929635298",
   "metadata": {},
   "source": [
    "To illustrate the use of dependencies between tokens, the method `get_compound()` below implements the extraction of compound words, typically nouns. Compound nouns are nouns that are formed by combining two or more words together. These words can be either other nouns, adjectives, verbs, or prepositions. When combined, the individual words create a new noun with a specific meaning that may not be easily deducible from the meanings of the individual words.\n",
    "\n",
    "Compound nouns can be written in different forms, such as separate words (\"ice cream\"), hyphenated words (\"well-being\"), or single words (\"notebook\"). The form used often depends on language conventions and style guides. Compound nouns can be categorized into different types based on the relationship between the constituent words:\n",
    "\n",
    "* **Noun-Noun compounds:** These compounds are formed by combining two nouns together, such as \"treehouse,\" \"rainstorm,\" or \"bookshelf.\"\n",
    "\n",
    "* **Adjective-Noun compounds:** These compounds involve an adjective and a noun, like \"blackboard,\" \"hotdog,\" or \"greenhouse.\"\n",
    "\n",
    "* **Verb-Noun compounds:** These compounds combine a verb and a noun, such as \"breakfast,\" \"pickup,\" or \"output.\"\n",
    "\n",
    "* **Preposition-Noun compounds:** These compounds include a preposition and a noun, like \"overcoat,\" \"underground,\" or \"afterthought.\"\n",
    "\n",
    "It's worth noting that compound nouns can have their own grammar rules and usage patterns. Understanding and recognizing compound nouns is important for accurate understanding, interpretation, and generation of language in various NLP tasks.\n",
    "\n",
    "Of course, compound nouns that are written/represented by a single word are trivial to extract. We therefore only need to focus on compound nouns that are written using separate words. The relevant dependency label is aptly named `compound` and we utilize this label in the method `get_compound()` to find all compound nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166d5022-e5d8-423f-b5f9-bb8ab60c6051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compound(token, compound_parts=[]):\n",
    "\n",
    "    # Loop over all children of the token\n",
    "    for child in token.children:\n",
    "        # We are only interested in the \"compound\" relationship\n",
    "        if child.dep_ == \"compound\":\n",
    "            # Call method recursively on the child token\n",
    "            get_compound(child, compound_parts=compound_parts)\n",
    "    \n",
    "    # Add the token itself to the list\n",
    "    compound_parts.append(token)\n",
    "    \n",
    "    #return compound_parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81d96aa-e390-4198-9711-1538310d3397",
   "metadata": {},
   "source": [
    "Let's execute our method for a head noun (*\"deck\"*) assuming our example sentence *\"I downloaded the lecture slide decks from the website.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88231d9d-d9b3-41cf-afb2-a0ee09238e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_parts = []\n",
    "\n",
    "get_compound(doc[5], compound_parts=compound_parts) # \"decks\" in sentence \"I downloaded the lecture slide deck from Canvas.\"\n",
    "\n",
    "print(' '.join([t.text for t in compound_parts]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9688777-951c-4d74-ac8a-6cbb6b524a94",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a65568-ffb2-4910-ab4a-61862d8f05c8",
   "metadata": {},
   "source": [
    "## Example Application: Relation Extraction\n",
    "\n",
    "Recall that text is considered unstructured data. However, many tasks benefit from some more structured representation (e.g., document search). A very popular method to represent information is a **knowledge graph**. A knowledge graph is a structured representation of knowledge that captures information about entities, their attributes, and the relationships between them. It is designed to organize and represent knowledge in a machine-readable format, enabling efficient retrieval, reasoning, and analysis of information. Any object, place, or person can be a node. An edge defines the relationship between the nodes. For example, we can have the nodes *Singapore* and *Asia* with an directed edge from *Singapore* to *Asia* with the label *locate_in*. Simply speaking, a knowledge graph is a large collection of **triples** where each triple contains a subject (e.g., *Singapore*), a predicate (e.g., *located_in*), and an object (e.g., *Asia*).\n",
    "\n",
    "In a knowledge graph, entities are represented as nodes, and the relationships between entities are represented as edges or links. Each entity and relationship typically has additional attributes or properties associated with them, providing more detailed information about the entities and their connections. Knowledge graphs are used in a wide range of applications, including semantic search, question answering, recommendation systems, information retrieval, and knowledge representation. By representing knowledge in a graph structure, it becomes possible to perform complex queries, infer new information, discover patterns, and derive insights from the interconnected data.\n",
    "\n",
    "Knowledge graphs can be built from various sources, including structured databases, **textual data**, web pages, and other structured or semi-structured resources. Since creating such knowledge graphs manually is very resource-intensive, automatic approaches are highly sought-after. Given that such much information is available in text form, trying to convert this data into a knowledge graph is an obvious important goal. There are many challenges involved and many different approaches conceivable -- and here we can only look at some basic ideas and methods.\n",
    "\n",
    "\n",
    "### Most Basic Triple Extraction\n",
    "\n",
    "Here, we have a very basic look into how we can use dependency graph extract triples from a sentence. If you look at the examples above, you might already have some intuition about which kind of dependencies make meaningful triples. In the following, we look at the most straightforward case where a (subject, predicate, object) triple is derived from a verb and the dependent subject (`nsubj`) and dependent direct object (`dobj`). The method below tries to accomplish this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3539ae-9c0a-40b6-ba01-f00d052375c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_svo_triples(doc):\n",
    "\n",
    "    # Collect all triples; a sentence my contain more such dependencies\n",
    "    triples = []\n",
    "    \n",
    "    for token in doc:\n",
    "        \n",
    "        # The relationships we are interested require a verb as the predicate of the triple\n",
    "        # So wie ignore all other tokens\n",
    "        if token.pos_ != \"VERB\":\n",
    "            continue\n",
    "\n",
    "        # Initialize the subject/predicate/object of the potential triple\n",
    "        # The predicate is naturally the current verb\n",
    "        subj, pred, obj = [], token.text.lower(), []\n",
    "\n",
    "        # We now check all dependent tokens of the current verb\n",
    "        for child in token.children:\n",
    "            # If we see an \"nsubj\" label, we set the SUBJECT of the triple to this child\n",
    "            if (child.dep_ == \"nsubj\"):\n",
    "                get_compound(child, compound_parts=subj)\n",
    "                subj = ' '.join([t.text.lower() for t in subj])\n",
    "            # If we see an \"dobj\" label, we set the OBJECT of the triple to this child\n",
    "            elif (child.dep_ == \"dobj\"):\n",
    "                get_compound(child, compound_parts=obj)\n",
    "                obj = ' '.join([t.text.lower() for t in obj])\n",
    "                \n",
    "        # Only if we have a subject and object, we assume we have a valid triple\n",
    "        if subj is not None and obj is not None:\n",
    "            triples.append((subj, pred, obj))\n",
    "            \n",
    "    # Return all the found triples\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f2ec7f-199e-44bc-aee8-57b2a0db1bb6",
   "metadata": {},
   "source": [
    "Let's run this method over our example sentence *\"I downloaded the lecture slide decks from Canvas.\"*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d25d1e2-293c-4169-8747-ff4e3a620bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I downloaded the lecture slide decks from Canvas.\" \n",
    "#sentence = \"Alice bought a book, and Bob bought a pizza.\"\n",
    "\n",
    "doc = nlp(sentence)\n",
    "\n",
    "triples = extract_svo_triples(doc)\n",
    "\n",
    "for t in triples:\n",
    "    print(\"{} ===> {} ===> {}\".format(t[0], t[1], t[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8732498d-42db-4268-972c-e091274e6026",
   "metadata": {},
   "source": [
    "### Limitations\n",
    "\n",
    "The method `extract_svo_triples()` is not only very simplified but also only extracts the most basic type of triplet from a sentence. Consider for example *\"Ice cream is so delicious.\"*. One can argue that we want to extract the tripled (*ice cream*, *is*, *delicious*). However, if we run our method over this sentence we get an empty result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828f422b-f8f2-4b11-a222-4813a4da427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Ice cream is so delicious.\"\n",
    "\n",
    "doc = nlp(sentence)\n",
    "\n",
    "triples = extract_svo_triples(doc)\n",
    "\n",
    "for t in triples:\n",
    "    print(\"{} ===> {} ===> {}\".format(t[0], t[1], t[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2b7e4d-3942-4f39-9dc5-840fb0ae169d",
   "metadata": {},
   "source": [
    "Let's have a look at the dependency graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7191a54f-3de2-495e-b1a5-9e6ead45852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d29a1fd-f859-4cd3-aee8-6481957b0236",
   "metadata": {},
   "source": [
    "Of course, since *\"delicious\"* is not a direct object, we fail to capture it as an object for a triple. In short, this calls for identifying which other dependency relationships may yield valid/interesting/useful/etc. triples -- which may depend on the application. After that, naturally, these identification steps need to be implemented similar to method `extract_svo_triples()`.\n",
    "\n",
    "### Discussion\n",
    "\n",
    "Apart from just extracting more triples there are also many other challenges that need to be addressed to expect a meaningful knowledge graph. Just to mention a couple:\n",
    "\n",
    "* In our triple `(I, downloaded, lecture slide decks)`, the subject and object are just words/phrases that are nor linked to any real-world entity or uniquely identified concept. For example, if we later might extract another triple such as `(I, ate, cake)` from some other sentence, it's arbitrarily unlikely that the `I` will refer to the same person.\n",
    "\n",
    "* Let's assume, we extract some triple `(I, saved, lecture slide decks)` we treat label *\"downloaded\"* and *\"saved\"* is completely different predicates. In practice, we would like to agree on some kind of vocabulary of labels. This brings us to the concept of **ontologies**.\n",
    "\n",
    "There is a long way to convert arbitrary unstructured text reliably into a meaningful knowledge graph. However, extracting dependency relations between words is a very important step towards this goal. Summing up, dependency parsing plays a crucial role in creating knowledge graphs by extracting structured information about entities and their relationships from text. Here's a summary of its use:\n",
    "\n",
    "* **Entity Extraction:** Dependency parsing helps identify entities in a sentence by recognizing noun phrases and their modifiers. By extracting entities and their associated attributes, such as names, locations, or dates, dependency parsing contributes to populating the nodes of a knowledge graph.\n",
    "\n",
    "* **Relationship Extraction:** Dependency parsing reveals the syntactic relationships between words in a sentence. By analyzing the dependencies between verbs, nouns, and other parts of speech, it becomes possible to identify and extract relationships between entities. For example, it can help identify subject-verb-object relationships or modifiers of a particular entity.\n",
    "\n",
    "* **Graph Structure:** The dependencies identified through parsing provide the foundation for defining the structure of a knowledge graph. The head-dependent relationships between words form the edges of the graph, while the words themselves serve as nodes. This structure facilitates the organization and representation of information in a graph format.\n",
    "\n",
    "* **Semantic Enrichment:** Dependency parsing can enrich the knowledge graph by providing additional semantic information. By analyzing the grammatical roles and dependencies, it becomes possible to assign more precise labels and attributes to the entities and relationships, enhancing the depth and accuracy of the knowledge graph.\n",
    "\n",
    "* **Querying and Reasoning:** Once the knowledge graph is constructed using dependency parsing, it enables efficient querying and reasoning. The structured representation allows for complex queries, pattern matching, and inference, supporting tasks such as semantic search, question answering, and information retrieval.\n",
    "\n",
    "Overall, dependency parsing serves as a fundamental step in creating knowledge graphs, facilitating the extraction of entities, relationships, and the overall structure of the graph. It enhances the organization, understanding, and utilization of textual information for building intelligent systems and enabling various downstream applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c14220-b014-4c8b-82bd-e828eee058f4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e72c21-afbe-439b-bd2e-96c76a017508",
   "metadata": {},
   "source": [
    "## Example Application: Text Simplification\n",
    "\n",
    "Recall that natural language is very expressive, i.e., there are virtually an infinite number of ways to express the same message. This includes that often a sentence may contain parts that are arguably not fundamentally important to convey that core message. We can utilize this observation for the task of **text simplification**. Text simplification involves transforming complex or difficult-to-understand text into simpler and more accessible language while retaining its original meaning. It aims to make information more comprehensible for various user groups, including individuals with cognitive or language impairments, non-native speakers, or people with limited literacy skills.\n",
    "\n",
    "Text simplification typically involves several subtasks, including:\n",
    "\n",
    "* **Lexical Simplification:** Replacing complex or uncommon words with simpler synonyms or paraphrases. This helps to reduce the vocabulary difficulty and enhance readability.\n",
    "\n",
    "* **Sentence Simplification:** Restructuring or rephrasing complex sentences into shorter and simpler forms. This can involve splitting long sentences, removing or reordering clauses, and using simpler grammatical structures.\n",
    "\n",
    "* **Discourse Simplification:** Simplifying the overall structure and coherence of the text. This includes clarifying pronoun references, simplifying coreference relationships, and ensuring the flow of information is more linear and straightforward.\n",
    "\n",
    "Text simplification can benefit a wide range of applications, such as educational resources, machine translation, information retrieval, and assistive technologies. It enables better understanding and accessibility of information, making it easier for diverse users to grasp complex concepts and effectively engage with text-based content.\n",
    "\n",
    "In the following, we look at a basic approach towards **sentence simplification**. More specifically, we aim to remove appositives. In grammar, an apposition is a construction where two noun phrases (or noun-like phrases) are placed side by side, with one phrase providing additional information or clarification about the other. The phrase that provides the additional information is called the appositive, and it is typically set off by commas, dashes, or parentheses.\n",
    "\n",
    "An appositive can further describe or identify the noun it modifies, providing additional details, explanations, or specifications. It adds non-essential information that helps to provide more context or description to the noun. Here are a few examples to illustrate the use of apposition:\n",
    "\n",
    "* *\"My friend, a talented musician, is coming over tonight.\"* In this sentence, the appositive \"a talented musician\" provides additional information about the noun \"my friend.\"\n",
    "\n",
    "* *\"The city of Paris, known as the City of Lights, is famous for its romantic ambiance.\"* Here, the appositive \"known as the City of Lights\" provides an additional name or identifier for the noun \"the city of Paris.\"\n",
    "\n",
    "* *\"My dog, a golden retriever, loves to play fetch.\"* In this example, the appositive \"a golden retriever\" gives more information about the noun \"my dog.\"\n",
    "\n",
    "Apposition helps to provide more details, specifications, or clarifications about a noun, enhancing the overall understanding and description of the sentence. It allows for the insertion of additional information without disrupting the basic grammatical structure of the sentence. However, one can equally argue that the appositive is not important to capture the core meaning of a sentence. We can therefore simplify sentences by removing appositives. Given this examples above, this would yield the sentences *\"My friend is coming over tonight.\"*, *\"The city of Paris is famous for its romantic ambiance.\"*, and *\"My dog loves to play fetch.\"*\n",
    "\n",
    "We consider the following example for our tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bf6574-6e35-49e5-9d17-d0dc4fdbee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Musk, the CEO of Tesla, bought Twitter.\"\n",
    "\n",
    "doc = nlp(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24395049-7618-4d13-a22a-f5a67329bec5",
   "metadata": {},
   "source": [
    "Let's first have a look at the dependency tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91cb3f0-5fb5-4ec1-ba89-de54299f95fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0decdb-24db-45ac-86a0-3e1ef2a8553b",
   "metadata": {},
   "source": [
    "While *\"[...], the CEO of Tesla, [...]\"* provides some additional information, it is not required to convey the message that *\"Musk bought Twitter.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be775506-cee4-4930-9100-60c09695a821",
   "metadata": {},
   "source": [
    "### Utility Method\n",
    "\n",
    "From the dependency parse tree above, we can see that the appositional modifier of *\"Musk\"* is *\"CEO\"*. However, we need to remove the whole phrase *\"the CEO of Twitter\"*, which is represented by all the tokens that are descendants of *\"CEO\"*. Since finding all descendants for a token is a generic and useful taks, we can implement this as method `get_descendants()` below.\n",
    "\n",
    "`get_descendants()` returns all the token indices starting with the original token, and then recursively checks all the children. Note that\n",
    "* The method returns a set of indices; so if a sorted list is required, this can be done trivially afterwards.\n",
    "* The returned indices may not be consecutive; although it generally should be for appositional modifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df993419-1db3-45a5-8d42-158744e97ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_descendants(token, indices=set()):\n",
    "    indices.add(token.i)\n",
    "    \n",
    "    for child in token.children:\n",
    "        indices.add(child.i)\n",
    "        get_descendants(child, indices=indices)\n",
    "    \n",
    "    return indices\n",
    "    \n",
    "# In this example, we know that the 4 token (with index 3) is \"CEO\"\n",
    "appos_indices = get_descendants(doc[3])\n",
    "\n",
    "print(appos_indices)\n",
    "\n",
    "for i in appos_indices:\n",
    "    print(doc[i].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c75bd4-89bc-461d-8baa-e9543ad4a7df",
   "metadata": {},
   "source": [
    "### Rule to Remove Appositives\n",
    "\n",
    "While we consider only appositives here, there are other rules to simplify a text (e.g., remove (some) relative clause, replace complex words with simpler synonyms, etc.). So let's set up our implementation in such a way that it could easily be extended. For that, we implement each rules as method that returns:\n",
    "\n",
    "* A boolean that signals if the sentence was indeed modified/simplified in any way\n",
    "* The modified sentence as a new spaCy document\n",
    "\n",
    "Let's implement such a rule for removing appositives. We basically just require the method `get_descendants()` for this. The only extension is to check if the appositive is preceded and/or succeeded by a comma. If so, they of course need also be removed to yield a properly formed sentence. The method `remove_appos()` accomplishes this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d693383a-fbc3-4215-b05f-92cc186bc98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_appos(doc, token):\n",
    "    # Since we remove at least on token, the doc will always be modified\n",
    "    # (there might other rules where does may not necessarily be the case)\n",
    "    was_modified = True\n",
    "    \n",
    "    # Get indices of all descendats of token with appos dependency\n",
    "    obsolete_indices = get_descendants(token)\n",
    "    \n",
    "    # Check if appositive is preceeded and/or succeeded by a comma that also needs to be removed\n",
    "    # We use the try-except block to handle case where the appositive is at the start or end of sentence\n",
    "    try:\n",
    "        if doc[min(obsolete_indices)-1].text == \",\":\n",
    "            obsolete_indices.add(min(obsolete_indices)-1)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        if doc[max(obsolete_indices)+1].text == \",\":\n",
    "            obsolete_indices.add(max(obsolete_indices)+1)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Form a new sentence from the original text but ignoring the obsolete tokens\n",
    "    sentence = ' '.join([ t.text for t in doc if t.i not in obsolete_indices ])\n",
    "    \n",
    "    # Return a newly analyzed version of the sentence; probably not the most efficient\n",
    "    # approach but it's \"safer\" as any edit might change other dependencies\n",
    "    return was_modified, nlp(sentence)\n",
    "\n",
    "\n",
    "was_modified, doc_modified = remove_appos(doc, doc[3])\n",
    "\n",
    "print(\"Has the original document beend modified: {}\".format(was_modified))\n",
    "print(\"New sentence: {}\".format(doc_modified))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a95954e-fe33-4107-af7d-63e90a875bd4",
   "metadata": {},
   "source": [
    "### Simplify Document\n",
    "\n",
    "Now we only need to wrap our rule into a method that, in principle, checks all implemented rules. At the very least, since we only have one rule, we need to check if a sentence may contain more than one appositive. The method `simplify()` below tries to simplify a sentence by repeatedly applying simplification rules until no further change is possible. In other words, at some point the variable `was_modified` will be `False` and the method exits. For our set of rules thi will be when all appositives have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456fae1c-5486-44a9-8d0e-6341fb2b1036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify(doc):\n",
    "    # Initialize local variables\n",
    "    was_modified, current_doc = True, doc\n",
    "    \n",
    "    # Perform all simplifications rules until no further changes was fone\n",
    "    while was_modified: \n",
    "        # By default, assume that doc has not been edited\n",
    "        was_modified = False\n",
    "\n",
    "        ############################################################\n",
    "        # Below follow all the check for simplifiaction rules.\n",
    "        # Here, we have just one, bu we could include more\n",
    "        ############################################################\n",
    "        \n",
    "        for token in current_doc:\n",
    "            \n",
    "            # Look for \"appos\" depencency label, ignore everything else\n",
    "            if token.dep_ == \"appos\":\n",
    "                \n",
    "                # Simplify sentence by removing appostion\n",
    "                was_modified, current_doc = remove_appos(doc, token)\n",
    "                \n",
    "                # If the document was modified, stop the loop and check the new doc from scratch again\n",
    "                # (probably not efficient but ensures that any previous edits don't \"mess up\" later edits)\n",
    "                if was_modified:\n",
    "                    break\n",
    "                    \n",
    "        ############################################################    \n",
    "                \n",
    "    return current_doc    \n",
    "\n",
    "\n",
    "doc_simplified = simplify(doc)\n",
    "\n",
    "print(\"Simplified sentence: {}\".format(doc_simplified))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a788ef-67f4-4d09-aaaf-8dcb58cbd804",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "In practice, text simplification is a very challenging task as it must preserve the core meaning of a text as well as preserve the correctness and flow of the language. Here we only looked at a basic approach towards sentence simplification by removing parts of sentences that are arguably not that important. In general, removing parts of a sentence is often a bit easier than rewriting or rephrasing a sentence as it is easier to ensure grammatical correctness.\n",
    "\n",
    "Keep in mind that we also made the assumption here that it is always harmless to remove appositives. However, this assumption might not always hold. For example, consider the sentence from above *\"My friend, a talented musician, is coming over tonight.\"* The information the friend is a talented musician can be a very crucial bit of information depending on the context (e.g., when organizing a party and to provide some form of entertainment). While the simplified sentence *\"My friend is coming over tonight.\"* is perfectly correct, the friend can now be any friend with no indication that he or she can help with the entertainment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378f3ebd-543a-488d-b940-aa7fc1b201e0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f515ddbc-b832-49c3-8943-e0adaa92c857",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Dependency parsing is a natural language processing technique that analyzes the grammatical structure of sentences. It identifies the relationships between words, represented as a directed graph called a dependency tree. Each word is considered a node, and the relationships between words are depicted as directed edges. Dependency parsing helps understand the syntactic dependencies, such as subject-verb and verb-object relationships, providing insights into the sentence's structure and meaning. It is used in various NLP tasks, such as information extraction, question answering, and machine translation.\n",
    "\n",
    "By utilizing dependency parsing, we can extract meaningful insights from text and represent them in a structured format. This allows for efficient information retrieval and analysis. Dependency parsing helps uncover the relationships between words, empowering us to navigate through the syntactic structure of sentences and identify dependencies accurately. It serves as a foundation for building intelligent systems that can understand and process human language, contributing to tasks such as semantic search, sentiment analysis, and text summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1fe0ef-4fe7-4ea8-a94d-44ebebad750b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs5246]",
   "language": "python",
   "name": "conda-env-cs5246-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
